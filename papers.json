[
  {
    "title": "World Models",
    "authors": [
      "David Ha",
      "Jürgen Schmidhuber"
    ],
    "abstract": "We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment.",
    "publication_date": "2018-03-27",
    "url": "https://arxiv.org/abs/1803.10122",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225498050,
    "created_at": "2026-02-04T17:18:18.050Z"
  },
  {
    "title": "DreamerV3: Mastering Diverse Domains through World Models",
    "authors": [
      "Danijar Hafner",
      "Jurgis Pasukonis",
      "Jimmy Ba",
      "Timothy Lillicrap"
    ],
    "abstract": "General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this out by specializing to the specific domain. We present DreamerV3, a general and scalable algorithm based on world models.",
    "publication_date": "2023-01-10",
    "url": "https://arxiv.org/abs/2301.04104",
    "tags": [
      "Model-Based RL",
      "World Models",
      "General Intelligence"
    ],
    "id": 1770225498050,
    "created_at": "2026-02-04T17:18:18.050Z"
  },
  {
    "title": "Mastering Atari with Discrete World Models",
    "authors": [
      "Danijar Hafner",
      "Timothy Lillicrap",
      "Mohammad Norouzi",
      "Jimmy Ba"
    ],
    "abstract": "Intelligent agents need to generalize from past experience to unseen situations. We introduce DreamerV2, a reinforcement learning agent that learns a world model with discrete latent variables.",
    "publication_date": "2020-10-01",
    "url": "https://arxiv.org/abs/2010.02193",
    "tags": [
      "Model-Based RL",
      "Discrete Latents",
      "Atari"
    ],
    "id": 1770225498050,
    "created_at": "2026-02-04T17:18:18.050Z"
  },
  {
    "title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks",
    "authors": [
      "Yixiang Chen",
      "Peiyan Li",
      "Jiabing Yang",
      "Keji He",
      "Xiangnan Wu",
      "Yuan Xu",
      "Kai Wang",
      "Jing Liu",
      "Nianfeng Liu",
      "Yan Huang",
      "Liang Wang"
    ],
    "abstract": "Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03793",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500348,
    "created_at": "2026-02-04T17:18:20.348Z"
  },
  {
    "title": "LIVE: Long-horizon Interactive Video World Modeling",
    "authors": [
      "Junchao Huang",
      "Ziyang Ye",
      "Xinting Hu",
      "Tianyu He",
      "Guiyu Zhang",
      "Shaoshuai Shi",
      "Jiang Bian",
      "Li Jiang"
    ],
    "abstract": "Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03747",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Generative Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500348,
    "created_at": "2026-02-04T17:18:20.348Z"
  },
  {
    "title": "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures",
    "authors": [
      "Basile Terver",
      "Randall Balestriero",
      "Megi Dervishi",
      "David Fan",
      "Quentin Garrido",
      "Tushar Nagarajan",
      "Koustuv Sinha",
      "Wancong Zhang",
      "Mike Rabbat",
      "Yann LeCun",
      "Amir Bar"
    ],
    "abstract": "We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03604",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500348,
    "created_at": "2026-02-04T17:18:20.348Z"
  },
  {
    "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories",
    "authors": [
      "Linjie Mu",
      "Zhongzhen Huang",
      "Yannian Gu",
      "Shengqian Qin",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ],
    "abstract": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03569",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500348,
    "created_at": "2026-02-04T17:18:20.348Z"
  },
  {
    "title": "Reparameterization Flow Policy Optimization",
    "authors": [
      "Hai Zhong",
      "Zhuoran Li",
      "Xun Wang",
      "Longbo Huang"
    ],
    "abstract": "Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\\times$ the reward of the state-of-the-art baseline.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03501",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1770225500349,
    "created_at": "2026-02-04T17:18:20.349Z"
  },
  {
    "title": "A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model",
    "authors": [
      "Linda Ariel Ventura",
      "Victoria Bosch",
      "Tim C Kietzmann",
      "Sushrut Thorat"
    ],
    "abstract": "Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such \"world models\", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03490",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500349,
    "created_at": "2026-02-04T17:18:20.349Z"
  },
  {
    "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
    "authors": [
      "Zhuoran Yang",
      "Xi Guo",
      "Chenjing Ding",
      "Chiyu Wang",
      "Wei Wu",
      "Yanyong Zhang"
    ],
    "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03242",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500349,
    "created_at": "2026-02-04T17:18:20.349Z"
  },
  {
    "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
    "authors": [
      "Zhuoran Yang",
      "Yanyong Zhang"
    ],
    "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03213",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500350,
    "created_at": "2026-02-04T17:18:20.350Z"
  },
  {
    "title": "From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning",
    "authors": [
      "Yao-Hui Li",
      "Zeyu Wang",
      "Xin Li",
      "Wei Pang",
      "Yingfang Yuan",
      "Zhengkun Chen",
      "Boya Zhang",
      "Riashat Islam",
      "Alex Lamb",
      "Yonggang Zhang"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03201",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225500350,
    "created_at": "2026-02-04T17:18:20.350Z"
  },
  {
    "title": "General Agents Contain World Models, even under Partial Observability and Stochasticity",
    "authors": [
      "Santiago Cifuentes"
    ],
    "abstract": "Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.\n  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.03146",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500350,
    "created_at": "2026-02-04T17:18:20.350Z"
  },
  {
    "title": "Latent Perspective-Taking via a Schrödinger Bridge in Influence-Augmented Local Models",
    "authors": [
      "Kevin Alcedo",
      "Pedro U. Lima",
      "Rachid Alami"
    ],
    "abstract": "Operating in environments alongside humans requires robots to make decisions under uncertainty. In addition to exogenous dynamics, they must reason over others' hidden mental-models and mental-states. While Interactive POMDPs and Bayesian Theory of Mind formulations are principled, exact nested-belief inference is intractable, and hand-specified models are brittle in open-world settings. We address both by learning structured mental-models and an estimator of others' mental-states. Building on the Influence-Based Abstraction, we instantiate an Influence-Augmented Local Model to decompose socially-aware robot tasks into local dynamics, social influences, and exogenous factors. We propose (a) a neuro-symbolic world model instantiating a factored, discrete Dynamic Bayesian Network, and (b) a perspective-shift operator modeled as an amortized Schrödinger Bridge over the learned local dynamics that transports factored egocentric beliefs into other-centric beliefs. We show that this architecture enables agents to synthesize socially-aware policies in model-based reinforcement learning, via decision-time mental-state planning (a Schrödinger Bridge in belief space), with preliminary results in a MiniGrid social navigation task.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02857",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225500351,
    "created_at": "2026-02-04T17:18:20.351Z"
  },
  {
    "title": "Joint Learning of Hierarchical Neural Options and Abstract World Model",
    "authors": [
      "Wasu Top Piriyakulkij",
      "Wolfgang Lehrach",
      "Kevin Ellis",
      "Kevin Murphy"
    ],
    "abstract": "Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02799",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500351,
    "created_at": "2026-02-04T17:18:20.351Z"
  },
  {
    "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
    "authors": [
      "Ansh Kumar Sharma",
      "Yixiang Sun",
      "Ninghao Lu",
      "Yunzhe Zhang",
      "Jiarao Liu",
      "Sherry Yang"
    ],
    "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02454",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500352,
    "created_at": "2026-02-04T17:18:20.352Z"
  },
  {
    "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
    "authors": [
      "Ruiqi Wu",
      "Xuanhua He",
      "Meng Cheng",
      "Tianyu Yang",
      "Yong Zhang",
      "Zhuoliang Kang",
      "Xunliang Cai",
      "Xiaoming Wei",
      "Chunle Guo",
      "Chongyi Li",
      "Ming-Ming Cheng"
    ],
    "abstract": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02393",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500352,
    "created_at": "2026-02-04T17:18:20.352Z"
  },
  {
    "title": "Self-Supervised Learning from Structural Invariance",
    "authors": [
      "Yipeng Zhang",
      "Hafez Ghaemi",
      "Jungyoon Lee",
      "Shahab Bakhtiari",
      "Eilif B. Muller",
      "Laurent Charlin"
    ],
    "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02381",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500353,
    "created_at": "2026-02-04T17:18:20.353Z"
  },
  {
    "title": "Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management",
    "authors": [
      "Owen Shen",
      "Patrick Jaillet"
    ],
    "abstract": "We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \\emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02283",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500353,
    "created_at": "2026-02-04T17:18:20.353Z"
  },
  {
    "title": "An Empirical Study of World Model Quantization",
    "authors": [
      "Zhongqian Fu",
      "Tianyi Zhao",
      "Kai Han",
      "Hang Zhou",
      "Xinghao Chen",
      "Yunhe Wang"
    ],
    "abstract": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02110",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Planning",
      "Representation Learning",
      "Transformers",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500353,
    "created_at": "2026-02-04T17:18:20.353Z"
  },
  {
    "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
    "authors": [
      "Guosheng Zhao",
      "Yaozeng Wang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Tingdong Yu",
      "Guan Huang",
      "Yongchen Zai",
      "Ji Jiao",
      "Changliang Xue",
      "Xiaole Wang",
      "Zhen Yang",
      "Futang Zhu",
      "Xingang Wang"
    ],
    "abstract": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.02002",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Transformers"
    ],
    "id": 1770225500354,
    "created_at": "2026-02-04T17:18:20.354Z"
  },
  {
    "title": "Grounding Generated Videos in Feasible Plans via World Models",
    "authors": [
      "Christos Ziakas",
      "Amir Bar",
      "Alessandra Russo"
    ],
    "abstract": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01960",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500355,
    "created_at": "2026-02-04T17:18:20.355Z"
  },
  {
    "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
    "authors": [
      "Dvir Samuel",
      "Issar Tzachor",
      "Matan Levy",
      "Micahel Green",
      "Gal Chechik",
      "Rami Ben-Ari"
    ],
    "abstract": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01801",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Generative Models",
      "Transformers",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500355,
    "created_at": "2026-02-04T17:18:20.355Z"
  },
  {
    "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
    "authors": [
      "Shicheng Yin",
      "Kaixuan Yin",
      "Weixing Chen",
      "Yang Liu",
      "Guanbin Li",
      "Liang Lin"
    ],
    "abstract": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLab-SYSU/DDP-WM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01780",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500356,
    "created_at": "2026-02-04T17:18:20.356Z"
  },
  {
    "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
    "authors": [
      "Yurun Chen",
      "Zeyi Liao",
      "Ping Yin",
      "Taotao Xie",
      "Keting Yin",
      "Shengyu Zhang"
    ],
    "abstract": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01725",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Planning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500356,
    "created_at": "2026-02-04T17:18:20.356Z"
  },
  {
    "title": "From Perception to Action: Spatial AI Agents and World Models",
    "authors": [
      "Gloria Felicia",
      "Nolan Bryant",
      "Handi Putra",
      "Ayaan Gazali",
      "Eliel Lobo",
      "Esteban Rojas"
    ],
    "abstract": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01644",
    "publication_date": "2026-02-02",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Transformers",
      "State Space Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500357,
    "created_at": "2026-02-04T17:18:20.357Z"
  },
  {
    "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
    "authors": [
      "Bohan Zeng",
      "Kaixin Zhu",
      "Daili Hua",
      "Bozhou Li",
      "Chengzhuo Tong",
      "Yuran Wang",
      "Xinyi Huang",
      "Yifan Dai",
      "Zixiang Zhang",
      "Yifan Yang",
      "Zhou Liu",
      "Hao Liang",
      "Xiaochen Ma",
      "Ruichuan An",
      "Tianyi Bai",
      "Hongcheng Gao",
      "Junbo Niu",
      "Yang Shi",
      "Xinlong Chen",
      "Yue Ding",
      "Minglei Shi",
      "Kai Zeng",
      "Yiwen Tang",
      "Yuanxing Zhang",
      "Pengfei Wan",
      "et al. (2 additional authors not shown)"
    ],
    "abstract": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01630",
    "publication_date": "2026-02-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500357,
    "created_at": "2026-02-04T17:18:20.357Z"
  },
  {
    "title": "Generative Visual Code Mobile World Models",
    "authors": [
      "Woosung Koh",
      "Sungjun Han",
      "Segyu Lee",
      "Se-Young Yun",
      "Jamin Shin"
    ],
    "abstract": "Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01576",
    "publication_date": "2026-02-01",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500358,
    "created_at": "2026-02-04T17:18:20.358Z"
  },
  {
    "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning",
    "authors": [
      "Shuai Liu",
      "Siheng Ren",
      "Xiaoyao Zhu",
      "Quanmin Liang",
      "Zefeng Li",
      "Qiang Li",
      "Xin Hu",
      "Kai Huang"
    ],
    "abstract": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01536",
    "publication_date": "2026-02-01",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500359,
    "created_at": "2026-02-04T17:18:20.359Z"
  },
  {
    "title": "Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics",
    "authors": [
      "Boxuan Zhang",
      "Weipu Zhang",
      "Zhaohan Feng",
      "Wei Xiao",
      "Jian Sun",
      "Jie Chen",
      "Gang Wang"
    ],
    "abstract": "A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.01270",
    "publication_date": "2026-02-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Transformers",
      "Generative Models"
    ],
    "id": 1770225500360,
    "created_at": "2026-02-04T17:18:20.360Z"
  },
  {
    "title": "World Models as an Intermediary between Agents and the Real World",
    "authors": [
      "Sherry Yang"
    ],
    "abstract": "Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00785",
    "publication_date": "2026-01-31",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500361,
    "created_at": "2026-02-04T17:18:20.361Z"
  },
  {
    "title": "Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design",
    "authors": [
      "Wei Zeng",
      "Xuchen Li",
      "Ruili Feng",
      "Zhen Liu",
      "Fengwei An",
      "Jian Zhao"
    ],
    "abstract": "Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \\times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \\textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \\times 480$ resolution -- a $50\\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00608",
    "publication_date": "2026-01-31",
    "tags": [
      "World Models",
      "Generative Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500361,
    "created_at": "2026-02-04T17:18:20.361Z"
  },
  {
    "title": "NetWorld: Communication-Based Diffusion World Model for Multi-Agent Reinforcement Learning in Wireless Networks",
    "authors": [
      "Kechen Meng",
      "Rongpeng Li",
      "Yansha Deng",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "abstract": "As wireless communication networks grow in scale and complexity, diverse resource allocation tasks become increasingly critical. Multi-Agent Reinforcement Learning (MARL) provides a promising solution for distributed control, yet it often requires costly real-world interactions and lacks generalization across diverse tasks. Meanwhile, recent advances in Diffusion Models (DMs) have demonstrated strong capabilities in modeling complex dynamics and supporting high-fidelity simulation. Motivated by these challenges and opportunities, we propose a Communication-based Diffusion World Model (NetWorld) to enable few-shot generalization across heterogeneous MARL tasks in wireless networks. To improve applicability to large-scale distributed networks, NetWorld adopts the Distributed Training with Decentralized Execution (DTDE) paradigm and is organized into a two-stage framework: (i) pre-training a classifier-guided conditional diffusion world model on multi-task offline datasets, and (ii) performing trajectory planning entirely within this world model to avoid additional online interaction. Cross-task heterogeneity is handled via shared latent processing for observations, two-hot discretization for task-specific actions and rewards, and an inverse dynamics model for action recovery. We further introduce a lightweight Mean Field (MF) communication mechanism to reduce non-stationarity and promote coordinated behaviors with low overhead. Experiments on three representative tasks demonstrate improved performance and sample efficiency over MARL baselines, indicating strong scalability and practical potential for wireless network optimization.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00558",
    "publication_date": "2026-01-31",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Diffusion Models",
      "Model-Based RL"
    ],
    "id": 1770225500362,
    "created_at": "2026-02-04T17:18:20.362Z"
  },
  {
    "title": "Parallel Stochastic Gradient-Based Planning for World Models",
    "authors": [
      "Michael Psenka",
      "Michael Rabbat",
      "Aditi Krishnapriyan",
      "Yann LeCun",
      "Amir Bar"
    ],
    "abstract": "World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables (\"virtual states\") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00475",
    "publication_date": "2026-01-30",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500363,
    "created_at": "2026-02-04T17:18:20.363Z"
  },
  {
    "title": "DISK: Dynamic Inference SKipping for World Models",
    "authors": [
      "Anugunj Naman",
      "Gaibo Zhang",
      "Ayushman Singh",
      "Yaguang Zhang"
    ],
    "abstract": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00440",
    "publication_date": "2026-01-30",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500363,
    "created_at": "2026-02-04T17:18:20.363Z"
  },
  {
    "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
    "authors": [
      "Shiqian Li",
      "Ruihong Shen",
      "Junfeng Ni",
      "Chang Pan",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00148",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Model-Based RL"
    ],
    "id": 1770225500364,
    "created_at": "2026-02-04T17:18:20.364Z"
  },
  {
    "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video",
    "authors": [
      "Zhengqing Gao",
      "Ziwen Li",
      "Xin Wang",
      "Jiaxin Huang",
      "Zhenyang Ren",
      "Mingkai Shao",
      "Hanlue Zhang",
      "Tianyu Huang",
      "Yongkang Cheng",
      "Yandong Guo",
      "Runqi Lin",
      "Yuanyuan Wang",
      "Tongliang Liu",
      "Kun Zhang",
      "Mingming Gong"
    ],
    "abstract": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.\n        △ Less",
    "url": "https://arxiv.org/pdf/2602.00096",
    "publication_date": "2026-01-24",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500365,
    "created_at": "2026-02-04T17:18:20.365Z"
  },
  {
    "title": "Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments",
    "authors": [
      "Jinwoo Jang",
      "Minjong Yoo",
      "Sihyung Yoon",
      "Honguk Woo"
    ],
    "abstract": "Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22647",
    "publication_date": "2026-01-30",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500366,
    "created_at": "2026-02-04T17:18:20.366Z"
  },
  {
    "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
    "authors": [
      "Hang Ding",
      "Peidong Liu",
      "Junqiao Wang",
      "Ziwei Ji",
      "Meng Cao",
      "Rongzhao Zhang",
      "Lynn Ai",
      "Eric Yang",
      "Tianyu Shi",
      "Lei Yu"
    ],
    "abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22149",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers",
      "Generative Models"
    ],
    "id": 1770225500367,
    "created_at": "2026-02-04T17:18:20.367Z"
  },
  {
    "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
    "authors": [
      "Lakshya Gupta",
      "Litao Li",
      "Yizhe Liu",
      "Sriram Ganapathi Subramanian",
      "Kaheer Suleman",
      "Zichen Zhang",
      "Haoye Lu",
      "Sumit Pasupalak"
    ],
    "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22130",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Video Prediction",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500367,
    "created_at": "2026-02-04T17:18:20.367Z"
  },
  {
    "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
    "authors": [
      "Irsyad Adam",
      "Zekai Chen",
      "David Laprade",
      "Shaun Porwal",
      "David Laub",
      "Erik Reinertsen",
      "Arda Pekis",
      "Kevin Brown"
    ],
    "abstract": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22128",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Representation Learning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500368,
    "created_at": "2026-02-04T17:18:20.368Z"
  },
  {
    "title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
    "authors": [
      "Onur T. Doganay",
      "Alexander Klawonn",
      "Martin Eigel",
      "Hanno Gottschalk"
    ],
    "abstract": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22086",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL"
    ],
    "id": 1770225500369,
    "created_at": "2026-02-04T17:18:20.369Z"
  },
  {
    "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
    "authors": [
      "Linhan Wang",
      "Zichong Yang",
      "Chen Bai",
      "Guoxiang Zhang",
      "Xiaotong Liu",
      "Xiaoyin Zheng",
      "Xiao-Xiao Long",
      "Chang-Tien Lu",
      "Cheng Lu"
    ],
    "abstract": "End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22032",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500370,
    "created_at": "2026-02-04T17:18:20.370Z"
  },
  {
    "title": "Causal World Modeling for Robot Control",
    "authors": [
      "Lin Li",
      "Qihang Zhang",
      "Yiming Luo",
      "Shuai Yang",
      "Ruilin Wang",
      "Fei Han",
      "Mingrui Yu",
      "Zelin Gao",
      "Nan Xue",
      "Xing Zhu",
      "Yujun Shen",
      "Yinghao Xu"
    ],
    "abstract": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21998",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Representation Learning",
      "Transformers",
      "Model-Based RL"
    ],
    "id": 1770225500371,
    "created_at": "2026-02-04T17:18:20.371Z"
  },
  {
    "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
    "authors": [
      "Weidong Huang",
      "Zhehan Li",
      "Hangxin Liu",
      "Biao Hou",
      "Yao Su",
      "Jingwen Zhang"
    ],
    "abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21363",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Generative Models"
    ],
    "id": 1770225500371,
    "created_at": "2026-02-04T17:18:20.371Z"
  },
  {
    "title": "The Surprising Difficulty of Search in Model-Based Reinforcement Learning",
    "authors": [
      "Wei-Di Chang",
      "Mikael Henaff",
      "Brandon Amos",
      "Gregory Dudek",
      "Scott Fujimoto"
    ],
    "abstract": "This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21306",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225500374,
    "created_at": "2026-02-04T17:18:20.374Z"
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "authors": [
      "Rishi Upadhyay",
      "Howard Zhang",
      "Jim Solomon",
      "Ayush Agrawal",
      "Pranay Boreddy",
      "Shruti Satya Narayana",
      "Yunhao Ba",
      "Alex Wong",
      "Celso M de Melo",
      "Achuta Kadambi"
    ],
    "abstract": "Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21282",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225500375,
    "created_at": "2026-02-04T17:18:20.375Z"
  },
  {
    "title": "Advancing Open-source World Models",
    "authors": [
      "Robbyant Team",
      "Zelin Gao",
      "Qiuyu Wang",
      "Yanhong Zeng",
      "Jiapeng Zhu",
      "Ka Leong Cheng",
      "Yixuan Li",
      "Hanlin Wang",
      "Yinghao Xu",
      "Shuailei Ma",
      "Yihang Chen",
      "Jie Liu",
      "Yansong Cheng",
      "Yao Yao",
      "Jiayi Zhu",
      "Yihao Meng",
      "Kecheng Zheng",
      "Qingyan Bai",
      "Jingye Chen",
      "Zehong Shen",
      "Yue Yu",
      "Xing Zhu",
      "Yujun Shen",
      "Hao Ouyang"
    ],
    "abstract": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20540",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Model-Based RL"
    ],
    "id": 1770225500376,
    "created_at": "2026-02-04T17:18:20.376Z"
  },
  {
    "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs",
    "authors": [
      "Oguzhan Gungordu",
      "Siheng Xiong",
      "Faramarz Fekri"
    ],
    "abstract": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20539",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500377,
    "created_at": "2026-02-04T17:18:20.377Z"
  },
  {
    "title": "Distributional value gradients for stochastic environments",
    "authors": [
      "Baptiste Debes",
      "Tinne Tuytelaars"
    ],
    "abstract": "Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20071",
    "publication_date": "2026-01-30",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Model-Based RL"
    ],
    "id": 1770225500381,
    "created_at": "2026-02-04T17:18:20.382Z"
  },
  {
    "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
    "authors": [
      "Jeanne Malécot",
      "Hamed Rahimi",
      "Jeanne Cattoni",
      "Marie Samson",
      "Mouad Abrini",
      "Mahdi Khoramshahi",
      "Maribel Pino",
      "Mohamed Chetouani"
    ],
    "abstract": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19839",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225500383,
    "created_at": "2026-02-04T17:18:20.383Z"
  },
  {
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "authors": [
      "Jialong Wu",
      "Xiaoying Zhang",
      "Hongyi Yuan",
      "Xiangcheng Zhang",
      "Tianhao Huang",
      "Changjing He",
      "Chaoyi Deng",
      "Renrui Zhang",
      "Youbin Wu",
      "Mingsheng Long"
    ],
    "abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19834",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225500384,
    "created_at": "2026-02-04T17:18:20.384Z"
  },
  {
    "title": "A Latent Space Framework for Modeling Transient Engine Emissions Using Joint Embedding Predictive Architectures",
    "authors": [
      "Ganesh Sundaram",
      "Tobias Gehra",
      "Jonas Ulmen",
      "Mirjan Heubaum",
      "Daniel Görges",
      "Michael Günthner"
    ],
    "abstract": "Accurately modeling and controlling vehicle exhaust emissions during transient events, such as rapid acceleration, is critical for meeting environmental regulations and optimizing powertrains. Conventional data-driven methods, such as Multilayer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks, improve upon phenomenological models but often struggle with the complex nonlinear dynamics of emission formation. These monolithic architectures are sensitive to dataset variability and typically require deep, computationally expensive structures to perform well, limiting their practical utility. This paper introduces a novel approach that overcomes these limitations by modeling emission dynamics within a structured latent space. Leveraging a Joint Embedding Predictive Architecture (JEPA), the proposed framework learns from a rich dataset that combines real-world Portable Emission Measurement System (PEMS) data with high-frequency hardware-in-the-loop measurements. The model abstracts away irrelevant noise, encoding only the key factors governing emission behavior into a compact, robust representation. This results in superior data efficiency and predictive accuracy across diverse transient regimes, significantly outperforming high-performing LSTM baselines in generalization. To ensure suitability for real-world deployment, the JEPA framework is structured to support pruning and post-training quantization. This strategy drastically reduces the computational footprint, minimizing inference time and memory demand with negligible accuracy loss. The result is a highly efficient model ideal for on-board implementation of advanced strategies, such as model predictive control or model-based reinforcement learning, in conventional and hybrid powertrains. These findings offer a clear pathway toward more robust emission control systems for next-generation vehicles.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19822",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Representation Learning",
      "Generative Models"
    ],
    "id": 1770225500385,
    "created_at": "2026-02-04T17:18:20.385Z"
  },
  {
    "title": "Comment on \"Multidimensional arrow of time\" (arXiv:2601.14134)",
    "authors": [
      "Andrei Galiautdinov"
    ],
    "abstract": "In a recent preprint [arXiv:2601.14134v1], Rubin argues that the arrow of time originates from the monotonic growth of the volume of extra dimensions. While the identification of a geometric origin for time's arrow is compelling in the case of brane-world models, we point out a possible tension between the proposed volume growth and the observational stability of the effective four-dimensional Newton's gravitational constant, G, that may arise in Kaluza-Klein (KK) theory. In standard KK approaches, such volume growth induces a time-variation of G that exceeds Big Bang Nucleosynthesis (BBN) and Lunar Laser Ranging (LLR) bounds by many orders of magnitude. To resolve this tension while preserving the author's key insight in the Kaluza-Klein case, we propose an extension: the \"shape-dynamic arrow of time\". By utilizing the scale-invariant monotonicity of Perelman's nu-entropy under normalized Ricci flow, we demonstrate how an arrow of time can emerge from the geometric smoothing of extra dimensions at fixed volume, thereby satisfying observational constraints on fundamental constants.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19819",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Generative Models",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502694,
    "created_at": "2026-02-04T17:18:22.694Z"
  },
  {
    "title": "Agentic Design Patterns: A System-Theoretic Framework",
    "authors": [
      "Minh-Dung Dao",
      "Quy Minh Le",
      "Hoang Thanh Lam",
      "Duc-Trong Le",
      "Quoc-Viet Pham",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "abstract": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19752",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502695,
    "created_at": "2026-02-04T17:18:22.695Z"
  },
  {
    "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
    "authors": [
      "Yin Wang",
      "Zhiying Leng",
      "Haitian Liu",
      "Frederick W. B. Li",
      "Mu Li",
      "Xiaohui Liang"
    ],
    "abstract": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19484",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502696,
    "created_at": "2026-02-04T17:18:22.696Z"
  },
  {
    "title": "From Observations to Events: Event-Aware World Model for Reinforcement Learning",
    "authors": [
      "Zhao-Han Peng",
      "Shaohui Li",
      "Zhi Li",
      "Shulan Ruan",
      "Yu Liu",
      "You He"
    ],
    "abstract": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19336",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Generative Models"
    ],
    "id": 1770225502697,
    "created_at": "2026-02-04T17:18:22.697Z"
  },
  {
    "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction",
    "authors": [
      "Oren Sultan",
      "Jordi Armengol-Estape",
      "Pascal Kesseli",
      "Julien Vanegue",
      "Dafna Shahaf",
      "Yossi Adi",
      "Peter O'Hearn"
    ],
    "abstract": "Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18987",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502698,
    "created_at": "2026-02-04T17:18:22.698Z"
  },
  {
    "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling",
    "authors": [
      "Panagiotis Lymperopoulos",
      "Abhiramon Rajasekharan",
      "Ian Berlot-Attwell",
      "Stéphane Aroca-Ouellette",
      "Kaheer Suleman"
    ],
    "abstract": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18620",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502699,
    "created_at": "2026-02-04T17:18:22.699Z"
  },
  {
    "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
    "authors": [
      "Weishi Mi",
      "Yong Bao",
      "Xiaowei Chi",
      "Xiaozhu Ju",
      "Zhiyuan Qin",
      "Kuangzhi Ge",
      "Kai Tang",
      "Peidong Jia",
      "Shanghang Zhang",
      "Jian Tang"
    ],
    "abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18323",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL"
    ],
    "id": 1770225502700,
    "created_at": "2026-02-04T17:18:22.700Z"
  },
  {
    "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions",
    "authors": [
      "Pedram Agand",
      "Mo Chen"
    ],
    "abstract": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18107",
    "publication_date": "2026-01-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Transformers"
    ],
    "id": 1770225502703,
    "created_at": "2026-02-04T17:18:22.703Z"
  },
  {
    "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
    "authors": [
      "Haoyuan Pan",
      "Sizhao Chen",
      "Zhaorui Wang",
      "Tse-Tin Chan"
    ],
    "abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18069",
    "publication_date": "2026-01-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1770225502704,
    "created_at": "2026-02-04T17:18:22.704Z"
  },
  {
    "title": "AI and World Models",
    "authors": [
      "Robert Worden"
    ],
    "abstract": "While large neural nets perform impressively on specific tasks, they are unreliable and unsafe, as is shown by the persistent hallucinations of large language models. This paper shows that large neural nets are intrinsically unreliable, because it is not possible to make or validate a tractable theory of how a neural net works. There is no reliable way to extrapolate its performance from a limited number of test cases to an unlimited set of use cases. To have confidence in the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. World models have been proposed as a way to do this. This paper discusses the scope and architecture required of world models. World models are often conceived as models of the physical and natural world, using established theories of natural science, or learned regularities, to predict the physical consequences of AI actions. However, unforeseen consequences of AI actions impact the human social world as much as the physical world. To predict and control the consequences of AI, a world model needs to include a model of the human social world. I explore the challenges that this entails. Human language is based on a Common Ground of mutual understanding of the world, shared by the people conversing. The common ground is an overlapping subset of each persons world model, including their models of the physical, social and mental worlds. LLMs have no stable representation of a common ground. To be reliable, AI systems will need to represent a common ground with their users, including physical, mental and social domains.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17796",
    "publication_date": "2026-01-25",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502705,
    "created_at": "2026-02-04T17:18:22.705Z"
  },
  {
    "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
    "authors": [
      "Yutong Shen",
      "Hangxu Liu",
      "Kailin Pei",
      "Ruizhe Xia",
      "Tongtong Feng"
    ],
    "abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17507",
    "publication_date": "2026-01-24",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225502706,
    "created_at": "2026-02-04T17:18:22.706Z"
  },
  {
    "title": "SkyReels-V3 Technique Report",
    "authors": [
      "Debang Li",
      "Zhengcong Fei",
      "Tuanhui Li",
      "Yikun Dou",
      "Zheng Chen",
      "Jiangping Yang",
      "Mingyuan Fan",
      "Jingtao Xu",
      "Jiahua Wang",
      "Baoxuan Gu",
      "Mingshan Chang",
      "Wenjing Cai",
      "Yuqiang Xie",
      "Binjie Mao",
      "Youqiang Zhang",
      "Nuo Pang",
      "Hao Zhang",
      "Yuzhe Jin",
      "Zhiheng Xu",
      "Dixuan Lin",
      "Guibin Chen",
      "Yahui Zhou"
    ],
    "abstract": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17323",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Transformers",
      "Model-Based RL"
    ],
    "id": 1770225502707,
    "created_at": "2026-02-04T17:18:22.707Z"
  },
  {
    "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation",
    "authors": [
      "Junichiro Niimi"
    ],
    "abstract": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17094",
    "publication_date": "2026-01-23",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Representation Learning",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225502708,
    "created_at": "2026-02-04T17:18:22.708Z"
  },
  {
    "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
    "authors": [
      "Luozhou Wang",
      "Zhifei Chen",
      "Yihua Du",
      "Dongyu Yan",
      "Wenhang Ge",
      "Guibao Shen",
      "Xinli Xu",
      "Leyi Wu",
      "Man Chen",
      "Tianshuo Xu",
      "Peiran Ren",
      "Xin Tao",
      "Pengfei Wan",
      "Ying-Cong Chen"
    ],
    "abstract": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17067",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502709,
    "created_at": "2026-02-04T17:18:22.709Z"
  },
  {
    "title": "Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics",
    "authors": [
      "Pierrick Lorang"
    ],
    "abstract": "Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16985",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225502710,
    "created_at": "2026-02-04T17:18:22.710Z"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "authors": [
      "Moo Jin Kim",
      "Yihuai Gao",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Yunhao Ge",
      "Grace Lam",
      "Percy Liang",
      "Shuran Song",
      "Ming-Yu Liu",
      "Chelsea Finn",
      "Jinwei Gu"
    ],
    "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16163",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Diffusion Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502711,
    "created_at": "2026-02-04T17:18:22.711Z"
  },
  {
    "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
    "authors": [
      "Chak-Wing Mak",
      "Guanyu Zhu",
      "Boyi Zhang",
      "Hongji Li",
      "Xiaowei Chi",
      "Kevin Zhang",
      "Yichen Wu",
      "Yangfan He",
      "Chun-Kai Fan",
      "Wentao Lu",
      "Kuangzhi Ge",
      "Xinyu Fang",
      "Hongyang He",
      "Kuan Lu",
      "Tianxiang Xu",
      "Li Zhang",
      "Yongxin Ni",
      "Youhua Li",
      "Shanghang Zhang"
    ],
    "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16007",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502713,
    "created_at": "2026-02-04T17:18:22.713Z"
  },
  {
    "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models",
    "authors": [
      "Zhikang Chen",
      "Tingting Zhu"
    ],
    "abstract": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15533",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning",
      "Model-Based RL"
    ],
    "id": 1770225502714,
    "created_at": "2026-02-04T17:18:22.714Z"
  },
  {
    "title": "Non-Stationary Functional Bilevel Optimization",
    "authors": [
      "Jason Bohne",
      "Ieva Petrulionyte",
      "Michael Arbel",
      "Julien Mairal",
      "Paweł Polak"
    ],
    "abstract": "Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15363",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225502715,
    "created_at": "2026-02-04T17:18:22.715Z"
  },
  {
    "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
    "authors": [
      "Anurag Bagchi",
      "Zhipeng Bao",
      "Homanga Bharadhwaj",
      "Yu-Xiong Wang",
      "Pavel Tokmakov",
      "Martial Hebert"
    ],
    "abstract": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15284",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502716,
    "created_at": "2026-02-04T17:18:22.716Z"
  },
  {
    "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation",
    "authors": [
      "Ying Yang",
      "Zhengyao Lv",
      "Tianlin Pan",
      "Haofan Wang",
      "Binxin Yang",
      "Hubery Yin",
      "Chen Li",
      "Ziwei Liu",
      "Chenyang Si"
    ],
    "abstract": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15281",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502717,
    "created_at": "2026-02-04T17:18:22.717Z"
  },
  {
    "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
    "authors": [
      "Tony Chen",
      "Sam Cheyette",
      "Kelsey Allen",
      "Joshua Tenenbaum",
      "Kevin Smith"
    ],
    "abstract": "Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.14514",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Planning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225502718,
    "created_at": "2026-02-04T17:18:22.718Z"
  },
  {
    "title": "VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models",
    "authors": [
      "Yongchao Huang"
    ],
    "abstract": "Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \\textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \\emph{Variational JEPA (VJEPA)}, a \\textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \\emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.14354",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502719,
    "created_at": "2026-02-04T17:18:22.719Z"
  },
  {
    "title": "CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning",
    "authors": [
      "Wenxin Ma",
      "Chenlong Wang",
      "Ruisheng Yuan",
      "Hao Chen",
      "Nanru Dai",
      "S. Kevin Zhou",
      "Yijun Yang",
      "Alan Yuille",
      "Jieneng Chen"
    ],
    "abstract": "Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer \"what-if\" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.13304",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502720,
    "created_at": "2026-02-04T17:18:22.720Z"
  },
  {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "authors": [
      "Baochang Ren",
      "Yunzhi Yao",
      "Rui Sun",
      "Shuofei Qiao",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "abstract": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.13247",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502721,
    "created_at": "2026-02-04T17:18:22.721Z"
  },
  {
    "title": "Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design",
    "authors": [
      "Kaleem Arshid",
      "Ali Krayani",
      "Lucio Marcenaro",
      "David Martin Gomez",
      "Carlo Regazzoni"
    ],
    "abstract": "This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12939",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225502722,
    "created_at": "2026-02-04T17:18:22.722Z"
  },
  {
    "title": "Agentic Reasoning for Large Language Models",
    "authors": [
      "Tianxin Wei",
      "Ting-Wei Li",
      "Zhining Liu",
      "Xuying Ning",
      "Ze Yang",
      "Jiaru Zou",
      "Zhichen Zeng",
      "Ruizhong Qiu",
      "Xiao Lin",
      "Dongqi Fu",
      "Zihao Li",
      "Mengting Ai",
      "Duo Zhou",
      "Wenxuan Bao",
      "Yunzhe Li",
      "Gaotang Li",
      "Cheng Qian",
      "Yu Wang",
      "Xiangru Tang",
      "Yin Xiao",
      "Liri Fang",
      "Hui Liu",
      "Xianfeng Tang",
      "Yuji Zhang",
      "Chi Wang",
      "et al. (4 additional authors not shown)"
    ],
    "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12538",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL"
    ],
    "id": 1770225502723,
    "created_at": "2026-02-04T17:18:22.723Z"
  },
  {
    "title": "ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models",
    "authors": [
      "Baorui Peng",
      "Wenyao Zhang",
      "Liang Xu",
      "Zekun Qi",
      "Jiazhao Zhang",
      "Hongsi Liu",
      "Wenjun Zeng",
      "Xin Jin"
    ],
    "abstract": "Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12428",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225502724,
    "created_at": "2026-02-04T17:18:22.724Z"
  },
  {
    "title": "An Efficient and Multi-Modal Navigation System with One-Step World Model",
    "authors": [
      "Wangtian Shen",
      "Ziyang Meng",
      "Jinming Ma",
      "Mingliang Zhou",
      "Diyun Xiang"
    ],
    "abstract": "Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12277",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Transformers",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502725,
    "created_at": "2026-02-04T17:18:22.725Z"
  },
  {
    "title": "Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning",
    "authors": [
      "Rajat Ghosh",
      "Debojyoti Dutta"
    ],
    "abstract": "Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10905",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225502726,
    "created_at": "2026-02-04T17:18:22.726Z"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "authors": [
      "Delong Chen",
      "Tejaswi Kasarla",
      "Yejin Bang",
      "Mustafa Shukor",
      "Willy Chung",
      "Jade Yu",
      "Allen Bolourchi",
      "Theo Moutakanni",
      "Pascale Fung"
    ],
    "abstract": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10592",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Generative Models",
      "Representation Learning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502727,
    "created_at": "2026-02-04T17:18:22.727Z"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "authors": [
      "Jianhao Yuan",
      "Xiaofeng Zhang",
      "Felix Friedrich",
      "Nicolas Beltran-Velez",
      "Melissa Hall",
      "Reyhane Askari-Hemmat",
      "Xiaochuang Han",
      "Nicolas Ballas",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "abstract": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10553",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Representation Learning",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502728,
    "created_at": "2026-02-04T17:18:22.728Z"
  },
  {
    "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
    "authors": [
      "Ahmad Rahimi",
      "Valentin Gerard",
      "Eloi Zablocki",
      "Matthieu Cord",
      "Alexandre Alahi"
    ],
    "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.09452",
    "publication_date": "2026-01-14",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Transformers",
      "Diffusion Models",
      "Model-Based RL"
    ],
    "id": 1770225502729,
    "created_at": "2026-02-04T17:18:22.729Z"
  },
  {
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "authors": [
      "Youwei Liu",
      "Jian Wang",
      "Hanlin Wang",
      "Beichen Guo",
      "Wenjie Li"
    ],
    "abstract": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.08955",
    "publication_date": "2026-01-13",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Planning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225502731,
    "created_at": "2026-02-04T17:18:22.731Z"
  },
  {
    "title": "Creativity in AI as Emergence from Domain-Limited Generative Models",
    "authors": [
      "Corina Chutaux"
    ],
    "abstract": "Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.08388",
    "publication_date": "2026-01-13",
    "tags": [
      "World Models",
      "Generative Models",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502732,
    "created_at": "2026-02-04T17:18:22.732Z"
  },
  {
    "title": "Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling",
    "authors": [
      "Alexander Boldachev"
    ],
    "abstract": "This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07964",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502733,
    "created_at": "2026-02-04T17:18:22.733Z"
  },
  {
    "title": "Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions",
    "authors": [
      "Zhiting Mei",
      "Tenny Yin",
      "Ola Shorinwa",
      "Apurva Badithela",
      "Zhonghe Zheng",
      "Joseph Bruno",
      "Madison Bland",
      "Lihan Zha",
      "Asher Hancock",
      "Jaime Fernández Fisac",
      "Philip Dames",
      "Anirudha Majumdar"
    ],
    "abstract": "Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07823",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL"
    ],
    "id": 1770225502734,
    "created_at": "2026-02-04T17:18:22.734Z"
  },
  {
    "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
    "authors": [
      "Huanyu Li",
      "Kun Lei",
      "Sheng Zang",
      "Kaizhe Hu",
      "Yongyuan Liang",
      "Bo An",
      "Xiaoli Li",
      "Huazhe Xu"
    ],
    "abstract": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07821",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Generative Models"
    ],
    "id": 1770225502736,
    "created_at": "2026-02-04T17:18:22.736Z"
  },
  {
    "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning",
    "authors": [
      "Sijia li",
      "Xinran Li",
      "Shibo Chen",
      "Jun Zhang"
    ],
    "abstract": "Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07463",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225502737,
    "created_at": "2026-02-04T17:18:22.737Z"
  },
  {
    "title": "Object-Centric World Models Meet Monte Carlo Tree Search",
    "authors": [
      "Rodion Vakhitov",
      "Leonid Ugadiarov",
      "Aleksandr Panov"
    ],
    "abstract": "In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.06604",
    "publication_date": "2026-01-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225502738,
    "created_at": "2026-02-04T17:18:22.738Z"
  },
  {
    "title": "Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur",
    "authors": [
      "Yani Meziani"
    ],
    "abstract": "We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (<50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.06212",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Representation Learning",
      "Transformers",
      "Diffusion Models",
      "State Space Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502739,
    "created_at": "2026-02-04T17:18:22.739Z"
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
      "Jingsheng Zheng",
      "Jintian Zhang",
      "Yujie Luo",
      "Yuren Mao",
      "Yunjun Gao",
      "Lun Du",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05930",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502740,
    "created_at": "2026-02-04T17:18:22.740Z"
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "authors": [
      "Nate Gillman",
      "Yinghua Zhou",
      "Zitian Tang",
      "Evan Luo",
      "Arjan Chakravarthy",
      "Daksh Aggarwal",
      "Michael Freeman",
      "Charles Herrmann",
      "Chen Sun"
    ],
    "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05848",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502741,
    "created_at": "2026-02-04T17:18:22.741Z"
  },
  {
    "title": "EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium",
    "authors": [
      "Phu-Hoa Pham",
      "Chi-Nguyen Tran",
      "Duy-Minh Dao-Sy",
      "Phu-Quy Nguyen-Lam",
      "Trung-Kiet Huynh"
    ],
    "abstract": "Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative world model with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05653",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502742,
    "created_at": "2026-02-04T17:18:22.742Z"
  },
  {
    "title": "Learning Latent Action World Models In The Wild",
    "authors": [
      "Quentin Garrido",
      "Tushar Nagarajan",
      "Basile Terver",
      "Nicolas Ballas",
      "Yann LeCun",
      "Michael Rabbat"
    ],
    "abstract": "Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05230",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225502744,
    "created_at": "2026-02-04T17:18:22.744Z"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
      "Sixiao Zheng",
      "Minghao Yin",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Ying Shan",
      "Yanwei Fu"
    ],
    "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05138",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502745,
    "created_at": "2026-02-04T17:18:22.745Z"
  },
  {
    "title": "Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking",
    "authors": [
      "Sofiene Lassoued",
      "Laxmikant Shrikant Bahetic",
      "Nathalie Weiß-Borkowskib",
      "Stefan Lierc",
      "Andreas Schwunga"
    ],
    "abstract": "Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04887",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225502746,
    "created_at": "2026-02-04T17:18:22.746Z"
  },
  {
    "title": "Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning",
    "authors": [
      "Enze Pan"
    ],
    "abstract": "We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what \"uncertainty reduction\" objectives can and cannot guarantee under rule shifts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04695",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225502748,
    "created_at": "2026-02-04T17:18:22.748Z"
  },
  {
    "title": "Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead",
    "authors": [
      "Oluwatosin Oseni",
      "Shengjie Wang",
      "Jun Zhu",
      "Micah Corah"
    ],
    "abstract": "Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04686",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225502749,
    "created_at": "2026-02-04T17:18:22.749Z"
  },
  {
    "title": "UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving",
    "authors": [
      "Zhexiao Xiong",
      "Xin Ye",
      "Burhan Yaman",
      "Sheng Cheng",
      "Yiren Lu",
      "Jingru Luo",
      "Nathan Jacobs",
      "Liu Ren"
    ],
    "abstract": "World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04453",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225502750,
    "created_at": "2026-02-04T17:18:22.750Z"
  },
  {
    "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
    "authors": [
      "Chun-Kai Fan",
      "Xiaowei Chi",
      "Xiaozhu Ju",
      "Hao Li",
      "Yong Bao",
      "Yu-Kai Wang",
      "Lizhang Chen",
      "Zhiyuan Jiang",
      "Kuangzhi Ge",
      "Ying Li",
      "Weishi Mi",
      "Qingpo Wuwu",
      "Peidong Jia",
      "Yulin Luo",
      "Kevin Zhang",
      "Zhiyuan Qin",
      "Yong Dai",
      "Sirui Han",
      "Yike Guo",
      "Shanghang Zhang",
      "Jian Tang"
    ],
    "abstract": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04137",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504731,
    "created_at": "2026-02-04T17:18:24.731Z"
  },
  {
    "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
    "authors": [
      "Yilin Cao",
      "Yufeng Zhong",
      "Zhixiong Zeng",
      "Liming Zheng",
      "Jing Huang",
      "Haibo Qiu",
      "Peng Shi",
      "Wenji Mao",
      "Wan Guanglu"
    ],
    "abstract": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04035",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225504733,
    "created_at": "2026-02-04T17:18:24.733Z"
  },
  {
    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
    "authors": [
      "Cheng Qian",
      "Emre Can Acikgoz",
      "Bingxuan Li",
      "Xiusi Chen",
      "Yuji Zhang",
      "Bingxiang He",
      "Qinyu Luo",
      "Dilek Hakkani-Tür",
      "Gokhan Tur",
      "Yunzhu Li",
      "Heng Ji"
    ],
    "abstract": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03905",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504734,
    "created_at": "2026-02-04T17:18:24.734Z"
  },
  {
    "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
    "authors": [
      "Wenlong Huang",
      "Yu-Wei Chao",
      "Arsalan Mousavian",
      "Ming-Yu Liu",
      "Dieter Fox",
      "Kaichun Mo",
      "Li Fei-Fei"
    ],
    "abstract": "Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03782",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504735,
    "created_at": "2026-02-04T17:18:24.735Z"
  },
  {
    "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
    "authors": [
      "Zhongbin Guo",
      "Zhen Yang",
      "Yushan Li",
      "Xinyue Zhang",
      "Wenyu Gao",
      "Jiacheng Wang",
      "Chengzhi Li",
      "Xiangrui Liu",
      "Ping Jian"
    ],
    "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03590",
    "publication_date": "2026-01-07",
    "tags": [
      "Robotics",
      "Transformers"
    ],
    "id": 1770225504736,
    "created_at": "2026-02-04T17:18:24.736Z"
  },
  {
    "title": "Semantic Belief-State World Model for 3D Human Motion Prediction",
    "authors": [
      "Sarim Chaudhry"
    ],
    "abstract": "Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03517",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "State Space Models",
      "Generative Models"
    ],
    "id": 1770225504737,
    "created_at": "2026-02-04T17:18:24.737Z"
  },
  {
    "title": "Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring",
    "authors": [
      "Zeyu Dong",
      "Yuke Tian",
      "Yu Sun"
    ],
    "abstract": "The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03486",
    "publication_date": "2026-01-13",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225504739,
    "created_at": "2026-02-04T17:18:24.739Z"
  },
  {
    "title": "Time-Scaling Is What Agents Need Now",
    "authors": [
      "Zhi Liu",
      "Guangzhi Wang"
    ],
    "abstract": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities.\n  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.\n  This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02714",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504740,
    "created_at": "2026-02-04T17:18:24.740Z"
  },
  {
    "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
    "authors": [
      "Junhao Cai",
      "Zetao Cai",
      "Jiafei Cao",
      "Yilun Chen",
      "Zeyu He",
      "Lei Jiang",
      "Hang Li",
      "Hengjie Li",
      "Yang Li",
      "Yufei Liu",
      "Yanan Lu",
      "Qi Lv",
      "Haoxiang Ma",
      "Jiangmiao Pang",
      "Yu Qiao",
      "Zherui Qiu",
      "Yanqing Shen",
      "Xu Shi",
      "Yang Tian",
      "Bolun Wang",
      "Hanqing Wang",
      "Jiaheng Wang",
      "Tai Wang",
      "Xueyuan Wei",
      "Chao Wu",
      "et al. (17 additional authors not shown)"
    ],
    "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02456",
    "publication_date": "2026-01-05",
    "tags": [
      "World Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504742,
    "created_at": "2026-02-04T17:18:24.742Z"
  },
  {
    "title": "Modeling the Mental World for Embodied AI: A Comprehensive Review",
    "authors": [
      "Biyuan Liu",
      "Daigang Xu",
      "Lei Jiang",
      "Wenjun Guo",
      "Ping Chen"
    ],
    "abstract": "As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice.\n  To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02378",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504743,
    "created_at": "2026-02-04T17:18:24.743Z"
  },
  {
    "title": "AI Agent Systems: Architectures, Applications, and Evaluation",
    "authors": [
      "Bin Xu"
    ],
    "abstract": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01743",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504745,
    "created_at": "2026-02-04T17:18:24.745Z"
  },
  {
    "title": "Explicit World Models for Reliable Human-Robot Collaboration",
    "authors": [
      "Kenneth Kwok",
      "Basura Fernando",
      "Qianli Xu",
      "Vigneshwaran Subbaraju",
      "Dongkyu Choi",
      "Boon Kiat Quek"
    ],
    "abstract": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01705",
    "publication_date": "2026-01-11",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504746,
    "created_at": "2026-02-04T17:18:24.746Z"
  },
  {
    "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller",
    "authors": [
      "Tran Tien Dat",
      "Nguyen Hai An",
      "Nguyen Khanh Viet Dung",
      "Nguyen Duy Duc"
    ],
    "abstract": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01577",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504747,
    "created_at": "2026-02-04T17:18:24.747Z"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "authors": [
      "Yang Zhou",
      "Hao Shao",
      "Letian Wang",
      "Zhuofan Zong",
      "Hongsheng Li",
      "Steven L. Waslander"
    ],
    "abstract": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01528",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504749,
    "created_at": "2026-02-04T17:18:24.749Z"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "authors": [
      "Rong Zhou",
      "Dongping Chen",
      "Zihan Jia",
      "Yao Su",
      "Yixin Liu",
      "Yiwen Lu",
      "Dongwei Shi",
      "Yue Huang",
      "Tianyang Xu",
      "Yi Pan",
      "Xinliang Li",
      "Yohannes Abate",
      "Qingyu Chen",
      "Zhengzhong Tu",
      "Yu Yang",
      "Yu Zhang",
      "Qingsong Wen",
      "Gengchen Mai",
      "Sunyang Fu",
      "Jiachen Li",
      "Xuyu Wang",
      "Ziran Wang",
      "Jing Huang",
      "Tianming Liu",
      "Yong Chen",
      "et al. (2 additional authors not shown)"
    ],
    "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01321",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504750,
    "created_at": "2026-02-04T17:18:24.750Z"
  },
  {
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "authors": [
      "Hansen Jin Lillemark",
      "Benhao Huang",
      "Fangneng Zhan",
      "Yilun Du",
      "Thomas Anderson Keller"
    ],
    "abstract": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01075",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504751,
    "created_at": "2026-02-04T17:18:24.751Z"
  },
  {
    "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
    "authors": [
      "Nicolas Bougie",
      "Gian Maria Marconi",
      "Tony Yip",
      "Narimasa Watanabe"
    ],
    "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00930",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504753,
    "created_at": "2026-02-04T17:18:24.753Z"
  },
  {
    "title": "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "authors": [
      "Joyjit Roy",
      "Samaresh Kumar Singh"
    ],
    "abstract": "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00911",
    "publication_date": "2026-01-23",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504754,
    "created_at": "2026-02-04T17:18:24.754Z"
  },
  {
    "title": "Value-guided action planning with JEPA world models",
    "authors": [
      "Matthieu Destrade",
      "Oumayma Bounou",
      "Quentin Le Lidec",
      "Jean Ponce",
      "Yann LeCun"
    ],
    "abstract": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00844",
    "publication_date": "2025-12-28",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504756,
    "created_at": "2026-02-04T17:18:24.756Z"
  },
  {
    "title": "A formal theory on problem space as a semantic world model in systems engineering",
    "authors": [
      "Mayuranath SureshKumar",
      "Hanumanthrao Kannan"
    ],
    "abstract": "Classic problem-space theory models problem solving as a navigation through a structured space of states, operators, goals, and constraints. Systems Engineering (SE) employs analogous constructs (functional analysis, operational analysis, scenarios, trade studies), yet still lacks a rigorous systems-theoretic representation of the problem space itself. In current practice, reasoning often proceeds directly from stakeholder goals to prescriptive artifacts. This makes foundational assumptions about the operational environment, admissible interactions, and contextual conditions implicit or prematurely embedded in architectures or requirements. This paper addresses that gap by formalizing the problem space as an explicit semantic world model containing theoretical constructs that are defined prior to requirements and solution commitments. These constructs along with the developed axioms, theorems and corollary establish a rigorous criterion for unambiguous boundary semantics, context-dependent interaction traceability to successful stakeholder goal satisfaction, and sufficiency of problem-space specification over which disciplined reasoning can occur independent of solution design. It offers a clear distinction between what is true of the problem domain and what is chosen as a solution. The paper concludes by discussing the significance of the theory on practitioners and provides a dialogue-based hypothetical case study between a stakeholder and an engineer, demonstrating how the theory guides problem framing before designing any prescriptive artifacts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00755",
    "publication_date": "2026-01-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504757,
    "created_at": "2026-02-04T17:18:24.757Z"
  },
  {
    "title": "Safe Adaptive Feedback Control via Barrier States",
    "authors": [
      "Trivikram Satharasi",
      "Tochukwu E. Ogri",
      "Muzaffar Qureshi",
      "Kyle Volle",
      "Rushikesh Kamalapurkar"
    ],
    "abstract": "This paper presents a safe feedback control framework for nonlinear control-affine systems with parametric uncertainty by leveraging adaptive dynamic programming (ADP) with barrier-state augmentation. The developed ADP-based controller enforces control invariance by optimizing a value function that explicitly penalizes the barrier state, thereby embedding safety directly into the Bellman structure. The near-optimal control policy computed using model-based reinforcement learning is combined with a concurrent learning estimator to identify the unknown parameters and guarantee uniform convergence without requiring persistency of excitation. Using a barrier-state Lyapunov function, we establish boundedness of the barrier dynamics and prove closed-loop stability and safety. Numerical simulations on an optimal obstacle-avoidance problem validate the effectiveness of the developed approach.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00476",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Representation Learning",
      "Generative Models"
    ],
    "id": 1770225504760,
    "created_at": "2026-02-04T17:18:24.760Z"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "authors": [
      "Yuxue Yang",
      "Lue Fan",
      "Ziqi Shi",
      "Junran Peng",
      "Feng Wang",
      "Zhaoxiang Zhang"
    ],
    "abstract": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00393",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504761,
    "created_at": "2026-02-04T17:18:24.761Z"
  },
  {
    "title": "TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model",
    "authors": [
      "Yabo Chen",
      "Yuanzhi Liang",
      "Jiepeng Wang",
      "Tingxi Chen",
      "Junfei Cheng",
      "Zixiao Gu",
      "Yuyang Huang",
      "Zicheng Jiang",
      "Wei Li",
      "Tian Li",
      "Weichen Li",
      "Zuoxin Li",
      "Guangce Liu",
      "Jialun Liu",
      "Junqi Liu",
      "Haoyuan Wang",
      "Qizhen Weng",
      "Xuan'er Wu",
      "Xunzhi Xiang",
      "Xiaoyan Yang",
      "Xin Zhang",
      "Shiwen Zhang",
      "Junyu Zhou",
      "Chengcheng Zhou",
      "Haibin Huang",
      "et al. (2 additional authors not shown)"
    ],
    "abstract": "World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00051",
    "publication_date": "2025-12-31",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning",
      "Model-Based RL"
    ],
    "id": 1770225504762,
    "created_at": "2026-02-04T17:18:24.762Z"
  },
  {
    "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving",
    "authors": [
      "Qian Cheng",
      "Weitao Zhou",
      "Cheng Jing",
      "Nanshan Deng",
      "Junze Wen",
      "Zhaoyang Liu",
      "Kun Jiang",
      "Diange Yang"
    ],
    "abstract": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment. This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24712",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Representation Learning",
      "State Space Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504764,
    "created_at": "2026-02-04T17:18:24.764Z"
  },
  {
    "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
    "authors": [
      "Basile Terver",
      "Tsung-Yen Yang",
      "Jean Ponce",
      "Adrien Bardes",
      "Yann LeCun"
    ],
    "abstract": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24497",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504766,
    "created_at": "2026-02-04T17:18:24.766Z"
  },
  {
    "title": "World model inspired sarcasm reasoning with large language model agents",
    "authors": [
      "Keito Inoshita",
      "Shinnosuke Mizuno"
    ],
    "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24329",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504767,
    "created_at": "2026-02-04T17:18:24.767Z"
  },
  {
    "title": "Large Emotional World Model",
    "authors": [
      "Changhao Song",
      "Yazhou Zhang",
      "Hui Gao",
      "Chang Yang",
      "Peng Zhang"
    ],
    "abstract": "World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24149",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504769,
    "created_at": "2026-02-04T17:18:24.769Z"
  },
  {
    "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
    "authors": [
      "Tianxin Xie",
      "Wentao Lei",
      "Guanjie Huang",
      "Pengfei Zhang",
      "Kai Jiang",
      "Chunhui Zhang",
      "Fengji Ma",
      "Haoyu He",
      "Han Zhang",
      "Jiangshan He",
      "Jinting Wang",
      "Linghan Fang",
      "Lufei Gao",
      "Orkesh Ablet",
      "Peihua Zhang",
      "Ruolin Hu",
      "Shengyu Li",
      "Weilin Lin",
      "Xiaoyang Feng",
      "Xinyue Yang",
      "Yan Rong",
      "Yanyun Wang",
      "Zihang Shao",
      "Zelin Zhao",
      "Chenxing Li",
      "et al. (5 additional authors not shown)"
    ],
    "abstract": "Text-to-audio-video (T2AV) generation underpins a wide range of applications demanding realistic audio-visual content, including virtual reality, world modeling, gaming, and filmmaking. However, existing T2AV models remain incapable of generating physically plausible sounds, primarily due to their limited understanding of physical principles. To situate current research progress, we present PhyAVBench, a challenging audio physics-sensitivity benchmark designed to systematically evaluate the audio physics grounding capabilities of existing T2AV models. PhyAVBench comprises 1,000 groups of paired text prompts with controlled physical variables that implicitly induce sound variations, enabling a fine-grained assessment of models' sensitivity to changes in underlying acoustic conditions. We term this evaluation paradigm the Audio-Physics Sensitivity Test (APST). Unlike prior benchmarks that primarily focus on audio-video synchronization, PhyAVBench explicitly evaluates models' understanding of the physical mechanisms underlying sound generation, covering 6 major audio physics dimensions, 4 daily scenarios (music, sound effects, speech, and their mix), and 50 fine-grained test points, ranging from fundamental aspects such as sound diffraction to more complex phenomena, e.g., Helmholtz resonance. Each test point consists of multiple groups of paired prompts, where each prompt is grounded by at least 20 newly recorded or collected real-world videos, thereby minimizing the risk of data leakage during model pre-training. Both prompts and videos are iteratively refined through rigorous human-involved error correction and quality control to ensure high quality. We argue that only models with a genuine grasp of audio-related physical principles can generate physically consistent audio-visual content. We hope PhyAVBench will stimulate future progress in this critical yet largely unexplored domain.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23994",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "State Space Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504770,
    "created_at": "2026-02-04T17:18:24.770Z"
  },
  {
    "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
    "authors": [
      "Guo Ye",
      "Zexi Zhang",
      "Xu Zhao",
      "Shang Wu",
      "Haoran Lu",
      "Shihan Lu",
      "Han Liu"
    ],
    "abstract": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23864",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504772,
    "created_at": "2026-02-04T17:18:24.772Z"
  },
  {
    "title": "Emergent World Beliefs: Exploring Transformers in Stochastic Games",
    "authors": [
      "Adam Kamel",
      "Tanish Rastogi",
      "Michael Ma",
      "Kailash Ranganathan",
      "Kevin Zhu"
    ],
    "abstract": "Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23722",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504774,
    "created_at": "2026-02-04T17:18:24.774Z"
  },
  {
    "title": "Web World Models",
    "authors": [
      "Jichen Feng",
      "Yifan Zhang",
      "Chenggong Zhang",
      "Yifu Lu",
      "Shilong Liu",
      "Mengdi Wang"
    ],
    "abstract": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23676",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504775,
    "created_at": "2026-02-04T17:18:24.775Z"
  },
  {
    "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
    "authors": [
      "Pengfei Zhou",
      "Liliang Chen",
      "Shengcong Chen",
      "Di Chen",
      "Wenzhi Zhao",
      "Rongjun Jin",
      "Guanghui Ren",
      "Jianlan Luo"
    ],
    "abstract": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23541",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504777,
    "created_at": "2026-02-04T17:18:24.777Z"
  },
  {
    "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
    "authors": [
      "Tianze Xia",
      "Yongkang Li",
      "Lijun Zhou",
      "Jingfeng Yao",
      "Kaixin Xiong",
      "Haiyang Sun",
      "Bing Wang",
      "Kun Ma",
      "Guang Chen",
      "Hangjun Ye",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23421",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504779,
    "created_at": "2026-02-04T17:18:24.779Z"
  },
  {
    "title": "The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis",
    "authors": [
      "Alex Lewandowski",
      "Adtiya A. Ramesh",
      "Edan Meyer",
      "Dale Schuurmans",
      "Marlos C. Machado"
    ],
    "abstract": "Continual learning is often motivated by the idea, known as the big world hypothesis, that \"the world is bigger\" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23419",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers",
      "Generative Models"
    ],
    "id": 1770225504781,
    "created_at": "2026-02-04T17:18:24.781Z"
  },
  {
    "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
    "authors": [
      "Tianchen Deng",
      "Xuefeng Chen",
      "Yi Chen",
      "Qu Chen",
      "Yuyao Xu",
      "Lijin Yang",
      "Le Xu",
      "Yu Zhang",
      "Bo Zhang",
      "Wuxiong Huang",
      "Hesheng Wang"
    ],
    "abstract": "Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23180",
    "publication_date": "2026-01-11",
    "tags": [
      "World Models",
      "Generative Models",
      "Representation Learning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504783,
    "created_at": "2026-02-04T17:18:24.783Z"
  },
  {
    "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "authors": [
      "Yufan He",
      "Pengfei Guo",
      "Mengya Xu",
      "Zhaoshuo Li",
      "Andriy Myronenko",
      "Dillan Imans",
      "Bingjie Liu",
      "Dongren Yang",
      "Mingxue Gu",
      "Yongnan Ji",
      "Yueming Jin",
      "Ren Zhao",
      "Baiyong Shen",
      "Daguang Xu"
    ],
    "abstract": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23162",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504788,
    "created_at": "2026-02-04T17:18:24.788Z"
  },
  {
    "title": "YOLO-IOD: Towards Real Time Incremental Object Detection",
    "authors": [
      "Shizhou Zhang",
      "Xueqiang Lv",
      "Yinghui Xing",
      "Qirui Wu",
      "Di Xu",
      "Chen Zhao",
      "Yanning Zhang"
    ],
    "abstract": "Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22973",
    "publication_date": "2025-12-31",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504790,
    "created_at": "2026-02-04T17:18:24.790Z"
  },
  {
    "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback",
    "authors": [
      "Mengkang Hu",
      "Bowei Xia",
      "Yuran Wu",
      "Ailing Yu",
      "Yude Zou",
      "Qiguang Chen",
      "Shijian Wang",
      "Jiarui Jin",
      "Kexin Li",
      "Wenxiang Jiao",
      "Yuan Lu",
      "Ping Luo"
    ],
    "abstract": "Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22336",
    "publication_date": "2025-12-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Planning",
      "Reinforcement Learning"
    ],
    "id": 1770225504792,
    "created_at": "2026-02-04T17:18:24.792Z"
  },
  {
    "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling",
    "authors": [
      "Conor Wallace",
      "Umer Siddique",
      "Yongcan Cao"
    ],
    "abstract": "Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \\Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \\ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \\Collab effectively distinguishes teammate types, while \\ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22129",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504794,
    "created_at": "2026-02-04T17:18:24.794Z"
  },
  {
    "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space",
    "authors": [
      "Weichen Zhang",
      "Peizhi Tang",
      "Xin Zeng",
      "Fanhang Man",
      "Shiquan Yu",
      "Zichao Dai",
      "Baining Zhao",
      "Hongjin Chen",
      "Yu Shang",
      "Wei Wu",
      "Chen Gao",
      "Xinlei Chen",
      "Xin Wang",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "abstract": "Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21887",
    "publication_date": "2026-01-02",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504796,
    "created_at": "2026-02-04T17:18:24.796Z"
  },
  {
    "title": "AstraNav-World: World Model for Foresight Control and Consistency",
    "authors": [
      "Junjun Hu",
      "Jintao Chen",
      "Haochen Bai",
      "Minghua Luo",
      "Shichao Xie",
      "Ziyi Chen",
      "Fei Liu",
      "Zedong Chu",
      "Xinda Xue",
      "Botao Ren",
      "Xiaolong Wu",
      "Mu Xu",
      "Shanghang Zhang"
    ],
    "abstract": "Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled \"envision-then-plan\" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21714",
    "publication_date": "2025-12-25",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504798,
    "created_at": "2026-02-04T17:18:24.798Z"
  },
  {
    "title": "A Unified Definition of Hallucination: It's The World Model, Stupid!",
    "authors": [
      "Emmy Liu",
      "Varun Gangal",
      "Chelsea Zou",
      "Michael Yu",
      "Xiaoqi Huang",
      "Alex Chang",
      "Zhuofu Tao",
      "Karan Singh",
      "Sachin Kumar",
      "Steven Y. Feng"
    ],
    "abstract": "Despite numerous attempts at mitigation since the inception of language models, hallucinations remain a persistent problem even in today's frontier LLMs. Why is this? We review existing definitions of hallucination and fold them into a single, unified definition wherein prior definitions are subsumed. We argue that hallucination can be unified by defining it as simply inaccurate (internal) world modeling, in a form where it is observable to the user. For example, stating a fact which contradicts a knowledge base OR producing a summary which contradicts the source. By varying the reference world model and conflict policy, our framework unifies prior definitions. We argue that this unified view is useful because it forces evaluations to clarify their assumed reference \"world\", distinguishes true hallucinations from planning or reward errors, and provides a common language for comparison across benchmarks and discussion of mitigation strategies. Building on this definition, we outline plans for a family of benchmarks using synthetic, fully specified reference world models to stress-test and improve world modeling components.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21577",
    "publication_date": "2026-02-03",
    "tags": [
      "World Models",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504800,
    "created_at": "2026-02-04T17:18:24.800Z"
  },
  {
    "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
    "authors": [
      "Yu He",
      "Da Huang",
      "Zhenyang Liu",
      "Zixiao Gu",
      "Qiang Sun",
      "Guangnan Ye",
      "Yanwei Fu"
    ],
    "abstract": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21201",
    "publication_date": "2025-12-24",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504801,
    "created_at": "2026-02-04T17:18:24.801Z"
  },
  {
    "title": "Active inference and artificial reasoning",
    "authors": [
      "Karl Friston",
      "Lancelot Da Costa",
      "Alexander Tschantz",
      "Conor Heins",
      "Christopher Buckley",
      "Tim Verbelen",
      "Thomas Parr"
    ],
    "abstract": "This technical note considers the sampling of outcomes that provide the greatest amount of information about the structure of underlying world models. This generalisation furnishes a principled approach to structure learning under a plausible set of generative models or hypotheses. In active inference, policies - i.e., combinations of actions - are selected based on their expected free energy, which comprises expected information gain and value. Information gain corresponds to the KL divergence between predictive posteriors with, and without, the consequences of action. Posteriors over models can be evaluated quickly and efficiently using Bayesian Model Reduction, based upon accumulated posterior beliefs about model parameters. The ensuing information gain can then be used to select actions that disambiguate among alternative models, in the spirit of optimal experimental design. We illustrate this kind of active selection or reasoning using partially observed discrete models; namely, a 'three-ball' paradigm used previously to describe artificial insight and 'aha moments' via (synthetic) introspection or sleep. We focus on the sample efficiency afforded by seeking outcomes that resolve the greatest uncertainty about the world model, under which outcomes are generated.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21129",
    "publication_date": "2025-12-24",
    "tags": [
      "World Models",
      "Generative Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225504803,
    "created_at": "2026-02-04T17:18:24.803Z"
  },
  {
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "authors": [
      "Saeed Mohammadzadeh",
      "Erfan Hamdi",
      "Joel Shor",
      "Emma Lejeune"
    ],
    "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20732",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504805,
    "created_at": "2026-02-04T17:18:24.805Z"
  },
  {
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "authors": [
      "Xuanhua He",
      "Tianyu Yang",
      "Ke Cao",
      "Ruiqi Wu",
      "Cheng Meng",
      "Yong Zhang",
      "Zhuoliang Kang",
      "Xiaoming Wei",
      "Qifeng Chen"
    ],
    "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20615",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504807,
    "created_at": "2026-02-04T17:18:24.807Z"
  },
  {
    "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
    "authors": [
      "Zepeng Xin",
      "Kaiyu Li",
      "Luodi Chen",
      "Wanchen Li",
      "Yuchen Xiao",
      "Hui Qiao",
      "Weizhan Zhang",
      "Deyu Meng",
      "Xiangyong Cao"
    ],
    "abstract": "Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20013",
    "publication_date": "2025-12-22",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225504809,
    "created_at": "2026-02-04T17:18:24.809Z"
  },
  {
    "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
    "authors": [
      "Pengxuan Yang",
      "Ben Lu",
      "Zhongpu Xia",
      "Chao Han",
      "Yinfeng Gao",
      "Teng Zhang",
      "Kun Zhan",
      "XianPeng Lang",
      "Yupeng Zheng",
      "Qichao Zhang"
    ],
    "abstract": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.19133",
    "publication_date": "2025-12-22",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225504811,
    "created_at": "2026-02-04T17:18:24.811Z"
  },
  {
    "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
    "authors": [
      "Utae Jeong",
      "Sumin In",
      "Hyunju Ryu",
      "Jaewan Choi",
      "Feng Yang",
      "Jongheon Jeong",
      "Seungryong Kim",
      "Sangpil Kim"
    ],
    "abstract": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.19048",
    "publication_date": "2025-12-22",
    "tags": [
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1770225504813,
    "created_at": "2026-02-04T17:18:24.813Z"
  },
  {
    "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement",
    "authors": [
      "Feeza Khan Khanzada",
      "Jaerock Kwon"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18850",
    "publication_date": "2025-12-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225504815,
    "created_at": "2026-02-04T17:18:24.815Z"
  },
  {
    "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "authors": [
      "Yixia Li",
      "Hongru Wang",
      "Jiahao Qiu",
      "Zhenfei Yin",
      "Dongdong Zhang",
      "Cheng Qian",
      "Zeping Li",
      "Pony Ma",
      "Guanhua Chen",
      "Heng Ji",
      "Mengdi Wang"
    ],
    "abstract": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18832",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506669,
    "created_at": "2026-02-04T17:18:26.669Z"
  },
  {
    "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
    "authors": [
      "Tianrui Zhu",
      "Shiyi Zhang",
      "Zhirui Sun",
      "Jingqi Tian",
      "Yansong Tang"
    ],
    "abstract": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18741",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506671,
    "created_at": "2026-02-04T17:18:26.671Z"
  },
  {
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "authors": [
      "Pierre Colombo",
      "Malik Boudiaf",
      "Allyn Sweet",
      "Michael Desa",
      "Hongxi Wang",
      "Kevin Candra",
      "Syméon del Marmol"
    ],
    "abstract": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18658",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506674,
    "created_at": "2026-02-04T17:18:26.674Z"
  },
  {
    "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
    "authors": [
      "Zhenhao Zhou",
      "Dan Negrut"
    ],
    "abstract": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18619",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Transformers",
      "Generative Models"
    ],
    "id": 1770225506676,
    "created_at": "2026-02-04T17:18:26.676Z"
  },
  {
    "title": "Large Language Models as Discounted Bayesian Filters",
    "authors": [
      "Jensen Zhang",
      "Jing Yang",
      "Keze Wang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18489",
    "publication_date": "2025-12-20",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506678,
    "created_at": "2026-02-04T17:18:26.678Z"
  },
  {
    "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
    "authors": [
      "Wenjun Lin",
      "Jensen Zhang",
      "Kaitong Cai",
      "Keze Wang"
    ],
    "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18477",
    "publication_date": "2025-12-20",
    "tags": [
      "World Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506680,
    "created_at": "2026-02-04T17:18:26.680Z"
  },
  {
    "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
    "authors": [
      "Yulu Wu",
      "Jiujun Cheng",
      "Haowen Wang",
      "Dengyang Suo",
      "Pei Ren",
      "Qichao Mao",
      "Shangce Gao",
      "Yakun Huang"
    ],
    "abstract": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18396",
    "publication_date": "2025-12-20",
    "tags": [
      "Robotics"
    ],
    "id": 1770225506682,
    "created_at": "2026-02-04T17:18:26.682Z"
  },
  {
    "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
    "authors": [
      "Qianwei Wang",
      "Bowen Li",
      "Zhanpeng Luo",
      "Yifan Xu",
      "Alexander Gray",
      "Tom Silver",
      "Sebastian Scherer",
      "Katia Sycara",
      "Yaqi Xie"
    ],
    "abstract": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17992",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506685,
    "created_at": "2026-02-04T17:18:26.685Z"
  },
  {
    "title": "Dexterous World Models",
    "authors": [
      "Byungjun Kim",
      "Taeksoo Kim",
      "Junyoung Lee",
      "Hanbyul Joo"
    ],
    "abstract": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17907",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506686,
    "created_at": "2026-02-04T17:18:26.686Z"
  },
  {
    "title": "Animate Any Character in Any World",
    "authors": [
      "Yitong Wang",
      "Fangyun Wei",
      "Hongyang Zhang",
      "Bo Dai",
      "Yan Lu"
    ],
    "abstract": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17796",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL"
    ],
    "id": 1770225506689,
    "created_at": "2026-02-04T17:18:26.689Z"
  },
  {
    "title": "Investigating methods to solve large windfarm optimization problems with a minimum number of qubits using circuit-based quantum computers",
    "authors": [
      "James Hancock",
      "Matthew Craven",
      "Craig McNeile"
    ],
    "abstract": "This study investigates quantum computing approaches for solving the windfarm layout optimization (WFLO) problems formulated as a quadratic unconstrained binary optimization (QUBO) problem. We investigate two encoding methods that require fewer than one qubit per grid point: the previously developed Pauli correlation encoding (PCE) and a novel single-qubit operator encoding (SQOE). These methods are tested on three windfarm configurations - two from prior WFLO scaling studies and a new real-world model based on an existing windfarm in Wales. The improved encoding methods allow us to solve WFLO problems on $9\\times 9$ grids using up to 20 qubits on a quantum computer simulator. The results show that both encoding methods perform competitively and demonstrate favorable scaling characteristics across the tested systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17582",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506691,
    "created_at": "2026-02-04T17:18:26.691Z"
  },
  {
    "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction",
    "authors": [
      "Ziyang Lin",
      "Zixuan Sun",
      "Sanhorn Chen",
      "Xiaoyang Chen",
      "Roy Zhao"
    ],
    "abstract": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17250",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Robotics",
      "Planning",
      "Transformers",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506694,
    "created_at": "2026-02-04T17:18:26.694Z"
  },
  {
    "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
    "authors": [
      "Nan Zhou",
      "Huandong Wang",
      "Jiahao Li",
      "Yang Li",
      "Xiao-Ping Zhang",
      "Yong Li",
      "Xinlei Chen"
    ],
    "abstract": "Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17152",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506697,
    "created_at": "2026-02-04T17:18:26.697Z"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": [
      "Hanlin Wang",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Yue Yu",
      "Yihao Meng",
      "Wen Wang",
      "Ka Leong Cheng",
      "Shuailei Ma",
      "Qingyan Bai",
      "Yixuan Li",
      "Cheng Chen",
      "Yanhong Zeng",
      "Xing Zhu",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.16924",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506699,
    "created_at": "2026-02-04T17:18:26.699Z"
  },
  {
    "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
    "authors": [
      "Tin Stribor Sohn",
      "Maximilian Dillitzer",
      "Jason J. Corso",
      "Eric Sax"
    ],
    "abstract": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.16461",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506701,
    "created_at": "2026-02-04T17:18:26.701Z"
  },
  {
    "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
    "authors": [
      "Dimitrios Danopoulos",
      "Enrico Lupi",
      "Chang Sun",
      "Sebastian Dittmeier",
      "Michael Kagan",
      "Vladimir Loncar",
      "Maurizio Pierini"
    ],
    "abstract": "Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15946",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Planning",
      "State Space Models",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506704,
    "created_at": "2026-02-04T17:18:26.704Z"
  },
  {
    "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
    "authors": [
      "Tin Stribor Sohn",
      "Maximilian Dillitzer",
      "Jason J. Corso",
      "Eric Sax"
    ],
    "abstract": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15940",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506707,
    "created_at": "2026-02-04T17:18:26.707Z"
  },
  {
    "title": "OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence",
    "authors": [
      "Yu Zheng",
      "Jie Hu",
      "Kailun Yang",
      "Jiaming Zhang"
    ],
    "abstract": "Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: \"what would happen given a specific future action\". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15621",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506710,
    "created_at": "2026-02-04T17:18:26.710Z"
  },
  {
    "title": "Soft Geometric Inductive Bias for Object Centric Dynamics",
    "authors": [
      "Hampus Linander",
      "Conor Heins",
      "Alexander Tschantz",
      "Marco Perin",
      "Christopher Buckley"
    ],
    "abstract": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15493",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506712,
    "created_at": "2026-02-04T17:18:26.712Z"
  },
  {
    "title": "Double Horizon Model-Based Policy Optimization",
    "authors": [
      "Akihiro Kubo",
      "Paavo Parmas",
      "Shin Ishii"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long \"distribution rollout\" (DR) and a short \"training rollout\" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15439",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Generative Models"
    ],
    "id": 1770225506715,
    "created_at": "2026-02-04T17:18:26.715Z"
  },
  {
    "title": "FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments",
    "authors": [
      "Quanxi Zhou",
      "Wencan Mao",
      "Manabu Tsukada",
      "John C. S. Lui",
      "Yusheng Ji"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15430",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Generative Models"
    ],
    "id": 1770225506717,
    "created_at": "2026-02-04T17:18:26.717Z"
  },
  {
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": [
      "Zefan Cai",
      "Haoyi Qiu",
      "Tianyi Ma",
      "Haozhe Zhao",
      "Gengze Zhou",
      "Kung-Hsiang Huang",
      "Parisa Kordjamshidi",
      "Minjia Zhang",
      "Wen Xiao",
      "Jiuxiang Gu",
      "Nanyun Peng",
      "Junjie Hu"
    ],
    "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14691",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Generative Models",
      "Planning",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506720,
    "created_at": "2026-02-04T17:18:26.720Z"
  },
  {
    "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes",
    "authors": [
      "Alessandro Trapasso",
      "Luca Iocchi",
      "Fabio Patrizi"
    ],
    "abstract": "Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14617",
    "publication_date": "2025-12-16",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225506723,
    "created_at": "2026-02-04T17:18:26.723Z"
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "authors": [
      "Wenqiang Sun",
      "Haiyu Zhang",
      "Haoyuan Wang",
      "Junta Wu",
      "Zehan Wang",
      "Zhenwei Wang",
      "Yunhong Wang",
      "Jun Zhang",
      "Tengfei Wang",
      "Chunchao Guo"
    ],
    "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14614",
    "publication_date": "2025-12-16",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506726,
    "created_at": "2026-02-04T17:18:26.726Z"
  },
  {
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ],
    "abstract": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14014",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506728,
    "created_at": "2026-02-04T17:18:26.728Z"
  },
  {
    "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces",
    "authors": [
      "Subramanyam Sahoo",
      "Jared Junkin"
    ],
    "abstract": "Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13821",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506734,
    "created_at": "2026-02-04T17:18:26.734Z"
  },
  {
    "title": "Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation",
    "authors": [
      "Daoyuan Qian",
      "Qiyao Liang",
      "Ila Fiete"
    ],
    "abstract": "Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13707",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506737,
    "created_at": "2026-02-04T17:18:26.737Z"
  },
  {
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "authors": [
      "Raktim Gautam Goswami",
      "Amir Bar",
      "David Fan",
      "Tsung-Yen Yang",
      "Gaoyue Zhou",
      "Prashanth Krishnamurthy",
      "Michael Rabbat",
      "Farshad Khorrami",
      "Yann LeCun"
    ],
    "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13644",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506739,
    "created_at": "2026-02-04T17:18:26.739Z"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "authors": [
      "Jianxiong Gao",
      "Zhaoxi Chen",
      "Xian Liu",
      "Junhao Zhuang",
      "Chengming Xu",
      "Jianfeng Feng",
      "Yu Qiao",
      "Yanwei Fu",
      "Chenyang Si",
      "Ziwei Liu"
    ],
    "abstract": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13604",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Model-Based RL"
    ],
    "id": 1770225506741,
    "created_at": "2026-02-04T17:18:26.741Z"
  },
  {
    "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
    "authors": [
      "Raymond Khazoum",
      "Daniela Fernandes",
      "Aleksandr Krylov",
      "Qin Li",
      "Stephane Deny"
    ],
    "abstract": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13517",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506744,
    "created_at": "2026-02-04T17:18:26.744Z"
  },
  {
    "title": "Motus: A Unified Latent Action World Model",
    "authors": [
      "Hongzhe Bi",
      "Hengkai Tan",
      "Shenghao Xie",
      "Zeyuan Wang",
      "Shuhe Huang",
      "Haitian Liu",
      "Ruowen Zhao",
      "Yao Feng",
      "Chendong Xiang",
      "Yinze Rong",
      "Hongyan Zhao",
      "Hanyu Liu",
      "Zhizhong Su",
      "Lei Ma",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13030",
    "publication_date": "2025-12-25",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506746,
    "created_at": "2026-02-04T17:18:26.746Z"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "authors": [
      "Zhenya Yang",
      "Zhe Liu",
      "Yuxiang Lu",
      "Liping Hou",
      "Chenxuan Miao",
      "Siyi Peng",
      "Bailan Feng",
      "Xiang Bai",
      "Hengshuang Zhao"
    ],
    "abstract": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12751",
    "publication_date": "2025-12-14",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506748,
    "created_at": "2026-02-04T17:18:26.748Z"
  },
  {
    "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents",
    "authors": [
      "Yesid Fonseca",
      "Manuel S. Ríos",
      "Nicanor Quijano",
      "Luis F. Giraldo"
    ],
    "abstract": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12548",
    "publication_date": "2025-12-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225506750,
    "created_at": "2026-02-04T17:18:26.750Z"
  },
  {
    "title": "From Human Intention to Action Prediction: Intention-Driven End-to-End Autonomous Driving",
    "authors": [
      "Huan Zheng",
      "Yucheng Zhou",
      "Tianyi Yan",
      "Jiayi Su",
      "Hongjun Chen",
      "Dubing Chen",
      "Xingtai Gui",
      "Wencheng Han",
      "Runzhou Tao",
      "Zhongying Qiu",
      "Jianfei Yang",
      "Jianbing Shen"
    ],
    "abstract": "While end-to-end autonomous driving has achieved remarkable progress in geometric control, current systems remain constrained by a command-following paradigm that relies on simple navigational instructions. Transitioning to genuinely intelligent agents requires the capability to interpret and fulfill high-level, abstract human intentions. However, this advancement is hindered by the lack of dedicated benchmarks and semantic-aware evaluation metrics. In this paper, we formally define the task of Intention-Driven End-to-End Autonomous Driving and present Intention-Drive, a comprehensive benchmark designed to bridge this gap. We construct a large-scale dataset featuring complex natural language intentions paired with high-fidelity sensor data. To overcome the limitations of conventional trajectory-based metrics, we introduce the Imagined Future Alignment (IFA), a novel evaluation protocol leveraging generative world models to assess the semantic fulfillment of human goals beyond mere geometric accuracy. Furthermore, we explore the solution space by proposing two distinct paradigms: an end-to-end vision-language planner and a hierarchical agent-based framework. The experiments reveal a critical dichotomy where existing models exhibit satisfactory driving stability but struggle significantly with intention fulfillment. Notably, the proposed frameworks demonstrate superior alignment with human intentions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12302",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Robotics",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506753,
    "created_at": "2026-02-04T17:18:26.753Z"
  },
  {
    "title": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes",
    "authors": [
      "Mohammad Pivezhandi",
      "Mahdi Banisharif",
      "Saeed Bakhshan",
      "Abusayeed Saifullah",
      "Ali Jannesari"
    ],
    "abstract": "Autonomous AI agents on embedded platforms require real-time, risk-aware scheduling under resource and thermal constraints. Classical heuristics struggle with workload irregularity, tabular regressors discard structural information, and model-free reinforcement learning (RL) risks overheating. We introduce GraphPerf-RT, a graph neural network surrogate achieving deep learning accuracy at heuristic speeds (2-7ms). GraphPerf-RT is, to our knowledge, the first to unify task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph with typed edges encoding precedence, placement, and contention. Evidential regression with Normal-Inverse-Gamma priors provides calibrated uncertainty; we validate on makespan prediction for risk-aware scheduling. Experiments on three ARM platforms (Jetson TX2, Orin NX, RUBIK Pi) achieve R^2 = 0.81 on log-transformed makespan with Spearman rho = 0.95 and conservative uncertainty calibration (PICP = 99.9% at 95% confidence). Integration with four RL methods demonstrates that multi-agent model-based RL with GraphPerf-RT as the world model achieves 66% makespan reduction and 82% energy reduction versus model-free baselines, with zero thermal violations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12091",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1770225506756,
    "created_at": "2026-02-04T17:18:26.756Z"
  },
  {
    "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models",
    "authors": [
      "Ryan Po",
      "Eric Ryan Chan",
      "Changan Chen",
      "Gordon Wetzstein"
    ],
    "abstract": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12080",
    "publication_date": "2025-12-12",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Transformers",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506758,
    "created_at": "2026-02-04T17:18:26.758Z"
  },
  {
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "authors": [
      "Junjie Ye",
      "Rong Xue",
      "Basile Van Hoorick",
      "Pavel Tokmakov",
      "Muhammad Zubair Irshad",
      "Yue Wang",
      "Vitor Guizilini"
    ],
    "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11797",
    "publication_date": "2025-12-12",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Diffusion Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506761,
    "created_at": "2026-02-04T17:18:26.761Z"
  },
  {
    "title": "FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model",
    "authors": [
      "Hongbin Lin",
      "Yiming Yang",
      "Yifan Zhang",
      "Chaoda Zheng",
      "Jie Feng",
      "Sheng Wang",
      "Zhennan Wang",
      "Shijia Chen",
      "Boyang Wang",
      "Yu Zhang",
      "Xianming Liu",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11226",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506763,
    "created_at": "2026-02-04T17:18:26.763Z"
  },
  {
    "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
    "authors": [
      "Gabrijel Boduljak",
      "Yushi Lan",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11225",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Generative Models",
      "Video Prediction",
      "Representation Learning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506766,
    "created_at": "2026-02-04T17:18:26.766Z"
  },
  {
    "title": "VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation",
    "authors": [
      "Felix O'Mahony",
      "Roberto Cipolla",
      "Ayush Tewari"
    ],
    "abstract": "Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11061",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Transformers",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506768,
    "created_at": "2026-02-04T17:18:26.768Z"
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "authors": [
      "Ao Liang",
      "Lingdong Kong",
      "Tianyi Yan",
      "Hongsi Liu",
      "Wesley Yang",
      "Ziqi Huang",
      "Wei Yin",
      "Jialong Zuo",
      "Yixuan Hu",
      "Dekai Zhu",
      "Dongyue Lu",
      "Youquan Liu",
      "Guangfeng Jiang",
      "Linfeng Li",
      "Xiangtai Li",
      "Long Zhuo",
      "Lai Xing Ng",
      "Benoit R. Cottereau",
      "Changxin Gao",
      "Liang Pan",
      "Wei Tsang Ooi",
      "Ziwei Liu"
    ],
    "abstract": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10958",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506770,
    "created_at": "2026-02-04T17:18:26.770Z"
  },
  {
    "title": "Generalized Spherical Neural Operators: Green's Function Formulation",
    "authors": [
      "Hao Tang",
      "Hao Chen",
      "Chao Li"
    ],
    "abstract": "Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a generalized operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to non-equivariant systems while retaining spectral efficiency and grid invariance. To exploit GSNO, we develop SHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and SHNet consistently outperform state-of-the-art methods. The theoretical and experimental results position GSNO as a principled and generalized framework for spherical operator design and learning, bridging rigorous theory with real-world complexity.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10723",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Reinforcement Learning"
    ],
    "id": 1770225506773,
    "created_at": "2026-02-04T17:18:26.773Z"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "authors": [
      "Gemini Robotics Team",
      "Krzysztof Choromanski",
      "Coline Devin",
      "Yilun Du",
      "Debidatta Dwibedi",
      "Ruiqi Gao",
      "Abhishek Jindal",
      "Thomas Kipf",
      "Sean Kirmani",
      "Isabel Leal",
      "Fangchen Liu",
      "Anirudha Majumdar",
      "Andrew Marmon",
      "Carolina Parada",
      "Yulia Rubanova",
      "Dhruv Shah",
      "Vikas Sindhwani",
      "Jie Tan",
      "Fei Xia",
      "Ted Xiao",
      "Sherry Yang",
      "Wenhao Yu",
      "Allan Zhou"
    ],
    "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10675",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Model-Based RL"
    ],
    "id": 1770225506775,
    "created_at": "2026-02-04T17:18:26.775Z"
  },
  {
    "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
    "authors": [
      "Shuhan Tan",
      "Kashyap Chitta",
      "Yuxiao Chen",
      "Ran Tian",
      "Yurong You",
      "Yan Wang",
      "Wenjie Luo",
      "Yulong Cao",
      "Philipp Krahenbuhl",
      "Marco Pavone",
      "Boris Ivanovic"
    ],
    "abstract": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10226",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Robotics",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506778,
    "created_at": "2026-02-04T17:18:26.778Z"
  },
  {
    "title": "Latent Action World Models for Control with Unlabeled Trajectories",
    "authors": [
      "Marvin Alles",
      "Xingyuan Zhang",
      "Patrick van der Smagt",
      "Philip Becker-Ehmck"
    ],
    "abstract": "Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10016",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Representation Learning",
      "Model-Based RL",
      "Generative Models"
    ],
    "id": 1770225506780,
    "created_at": "2026-02-04T17:18:26.780Z"
  },
  {
    "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
    "authors": [
      "Arjun Parthasarathy",
      "Nimit Kalra",
      "Rohun Agrawal",
      "Yann LeCun",
      "Oumayma Bounou",
      "Pavel Izmailov",
      "Micah Goldblum"
    ],
    "abstract": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.09929",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Planning",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506783,
    "created_at": "2026-02-04T17:18:26.783Z"
  },
  {
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "authors": [
      "Hao Lu",
      "Ziyang Liu",
      "Guangfeng Jiang",
      "Yuanfei Luo",
      "Sheng Chen",
      "Yangang Zhang",
      "Ying-Cong Chen"
    ],
    "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.09864",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Transformers",
      "Reinforcement Learning"
    ],
    "id": 1770225506785,
    "created_at": "2026-02-04T17:18:26.785Z"
  },
  {
    "title": "Deterministic World Models for Verification of Closed-loop Vision-based Systems",
    "authors": [
      "Yuang Geng",
      "Zhuoyang Zhou",
      "Zhongzheng Zhang",
      "Siyuan Pan",
      "Hoang-Dung Tran",
      "Ivan Ruchkin"
    ],
    "abstract": "Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08991",
    "publication_date": "2025-12-07",
    "tags": [
      "World Models",
      "Generative Models",
      "Robotics",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1770225506791,
    "created_at": "2026-02-04T17:18:26.791Z"
  },
  {
    "title": "Astra: General Interactive World Model with Autoregressive Denoising",
    "authors": [
      "Yixuan Zhu",
      "Jiaqi Feng",
      "Wenzhao Zheng",
      "Yuan Gao",
      "Xin Tao",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08931",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Diffusion Models",
      "Model-Based RL"
    ],
    "id": 1770225506794,
    "created_at": "2026-02-04T17:18:26.794Z"
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "authors": [
      "Yuning Gong",
      "Yifei Liu",
      "Yifan Zhan",
      "Muyao Niu",
      "Xueying Li",
      "Yuanjun Liao",
      "Jiaming Chen",
      "Yuanyuan Gao",
      "Jiaqi Chen",
      "Minming Chen",
      "Li Zhou",
      "Yuning Zhang",
      "Wei Wang",
      "Xiaoqing Hou",
      "Huaxi Huang",
      "Shixiang Tang",
      "Le Ma",
      "Dingwen Zhang",
      "Xue Yang",
      "Junchi Yan",
      "Yanchi Zhang",
      "Yinqiang Zheng",
      "Xiao Sun",
      "Zhihang Zhong"
    ],
    "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08478",
    "publication_date": "2025-12-09",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Generative Models",
      "Model-Based RL"
    ],
    "id": 1770225506796,
    "created_at": "2026-02-04T17:18:26.796Z"
  }
]