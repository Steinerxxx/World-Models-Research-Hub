export const MOCK_PAPERS = [
  {
    id: 1,
    title: "World Models",
    authors: ["David Ha", "JÃ¼rgen Schmidhuber"],
    abstract: "We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy to solve the required task. We can even train our agent entirely inside of its own dreamed environment generated by its world model, and transfer this policy back into the actual environment.",
    publication_date: "2018-03-27",
    url: "https://arxiv.org/abs/1803.10122",
    tags: ["World Models", "Reinforcement Learning", "Generative Models"],
    summary: "A pioneering paper that introduces the concept of training agents inside a learned world model.",
    contribution: "Demonstrated that agents can be trained entirely within a latent space world model.",
    limitations: "The VAE representation might miss small but critical details in the environment."
  },
  {
    id: 2,
    title: "DreamerV3: Mastering Diverse Domains through World Models",
    authors: ["Danijar Hafner", "Jurgis Pasukonis", "Jimmy Ba", "Timothy Lillicrap"],
    abstract: "General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this out by specializing to the specific domain. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, data budgets, reward frequencies, and reward scales. We observe that DreamerV3 scales favorably with model size.",
    publication_date: "2023-01-10",
    url: "https://arxiv.org/abs/2301.04104",
    tags: ["Model-Based RL", "World Models", "General Intelligence"],
    summary: "DreamerV3 achieves state-of-the-art performance across diverse domains with fixed hyperparameters.",
    contribution: "A robust and scalable world model algorithm that works without domain-specific tuning.",
    limitations: "Requires significant computational resources for training large models."
  },
  {
    id: 3,
    title: "Mastering Atari with Discrete World Models",
    authors: ["Danijar Hafner", "Timothy Lillicrap", "Mohammad Norouzi", "Jimmy Ba"],
    abstract: "Intelligent agents need to generalize from past experience to unseen situations. World models learn a representation of the environment that enables planning and generalization. However, learning accurate world models in complex visual environments remains a challenge. We introduce DreamerV2, a reinforcement learning agent that learns a world model with discrete latent variables. We show that discrete representations are crucial for accurate long-term predictions in complex environments.",
    publication_date: "2020-10-01",
    url: "https://arxiv.org/abs/2010.02193",
    tags: ["Model-Based RL", "Discrete Latents", "Atari"],
    summary: "DreamerV2 introduces discrete latent variables to improve long-term prediction accuracy.",
    contribution: "Showed that discrete world models can outperform continuous ones in complex visual tasks.",
    limitations: "Discrete representations may struggle with continuous state spaces if not carefully designed."
  },
  {
    id: 4,
    title: "Planning with Diffusion for Flexible Behavior Synthesis",
    authors: ["Michael Janner", "Yilun Du", "Joshua B. Tenenbaum", "Sergey Levine"],
    abstract: "We propose a trajectory optimization algorithm that uses a diffusion model as a learned prior over trajectories. This allows us to perform planning by sampling from the diffusion model, conditioned on the initial state and goal. We show that this approach, which we call Diffuser, can solve long-horizon tasks that are difficult for standard planners.",
    publication_date: "2022-05-18",
    url: "https://arxiv.org/abs/2205.09991",
    tags: ["Diffusion Models", "Planning", "Trajectory Optimization"],
    summary: "Diffuser treats planning as a generative modeling problem using diffusion models.",
    contribution: "Introduced a novel way to use diffusion models for trajectory planning.",
    limitations: "Sampling speed can be slow compared to traditional planners."
  },
  {
    id: 5,
    title: "Decision Transformer: Reinforcement Learning via Sequence Modeling",
    authors: ["Lili Chen", "Kevin Lu", "Aravind Rajeswaran", "Kimin Lee", "Aditya Grover", "Michael Laskin", "Pieter Abbeel", "Aravind Srinivas", "Igor Mordatch"],
    abstract: "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to use the simplicity and scalability of the Transformer architecture. We present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling.",
    publication_date: "2021-06-02",
    url: "https://arxiv.org/abs/2106.01345",
    tags: ["Transformers", "Offline RL", "Sequence Modeling"],
    summary: "Casts RL as a sequence modeling problem solvable by Transformers.",
    contribution: "Demonstrated that Transformers can effectively solve RL tasks via offline training.",
    limitations: "Performance depends heavily on the quality of the offline dataset."
  }
];
