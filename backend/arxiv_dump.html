<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,203 results for all: <span class="mathjax">&#34;World Models&#34;</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="&quot;World Models&quot;">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=%22World+Models%22&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="&quot;World Models&quot;">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.20540">arXiv:2601.20540</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.20540">pdf</a>, <a href="https://arxiv.org/ps/2601.20540">ps</a>, <a href="https://arxiv.org/format/2601.20540">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advancing Open-source <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Robbyant+Team"> Robbyant Team</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zelin Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qiuyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Y">Yanhong Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jiapeng Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+K+L">Ka Leong Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yixuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hanlin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yinghao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Shuailei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yihang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jie Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yansong Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yao Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jiayi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Y">Yihao Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+K">Kecheng Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Q">Qingyan Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jingye Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Z">Zehong Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Y">Yue Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yujun Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ouyang%2C+H">Hao Ouyang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.20540v1-abstract-short" style="display: inline;">
        We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20540v1-abstract-full').style.display = 'inline'; document.getElementById('2601.20540v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.20540v1-abstract-full" style="display: none;">
        We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as &#34;long-term memory&#34;. (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20540v1-abstract-full').style.display = 'none'; document.getElementById('2601.20540v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://technology.robbyant.com/lingbot-world; Code: https://github.com/robbyant/lingbot-world</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.20539">arXiv:2601.20539</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.20539">pdf</a>, <a href="https://arxiv.org/ps/2601.20539">ps</a>, <a href="https://arxiv.org/format/2601.20539">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PathWise: Planning through <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> for Automated Heuristic Design via Self-Evolving LLMs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gungordu%2C+O">Oguzhan Gungordu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+S">Siheng Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fekri%2C+F">Faramarz Fekri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.20539v1-abstract-short" style="display: inline;">
        &hellip;redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through <span class="search-hit mathjax">World</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20539v1-abstract-full').style.display = 'inline'; document.getElementById('2601.20539v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.20539v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks&#39; reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20539v1-abstract-full').style.display = 'none'; document.getElementById('2601.20539v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.20071">arXiv:2601.20071</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.20071">pdf</a>, <a href="https://arxiv.org/ps/2601.20071">ps</a>, <a href="https://arxiv.org/format/2601.20071">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributional value gradients for stochastic environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Debes%2C+B">Baptiste Debes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tuytelaars%2C+T">Tinne Tuytelaars</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.20071v1-abstract-short" style="display: inline;">
        &hellip;but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-base&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20071v1-abstract-full').style.display = 'inline'; document.getElementById('2601.20071v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.20071v1-abstract-full" style="display: none;">
        Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.20071v1-abstract-full').style.display = 'none'; document.getElementById('2601.20071v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19839">arXiv:2601.19839</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19839">pdf</a>, <a href="https://arxiv.org/ps/2601.19839">ps</a>, <a href="https://arxiv.org/format/2601.19839">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mal%C3%A9cot%2C+J">Jeanne Mal√©cot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rahimi%2C+H">Hamed Rahimi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cattoni%2C+J">Jeanne Cattoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Samson%2C+M">Marie Samson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abrini%2C+M">Mouad Abrini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khoramshahi%2C+M">Mahdi Khoramshahi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pino%2C+M">Maribel Pino</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chetouani%2C+M">Mohamed Chetouani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19839v1-abstract-short" style="display: inline;">
        &hellip;long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span> module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that upd&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19839v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19839v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19839v1-abstract-full" style="display: none;">
        Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span> module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19839v1-abstract-full').style.display = 'none'; document.getElementById('2601.19839v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19834">arXiv:2601.19834</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19834">pdf</a>, <a href="https://arxiv.org/ps/2601.19834">ps</a>, <a href="https://arxiv.org/format/2601.19834">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Generation Unlocks Human-Like Reasoning through Multimodal <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Jialong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaoying Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+H">Hongyi Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiangcheng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+T">Tianhao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+C">Changjing He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+C">Chaoyi Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Renrui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Youbin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+M">Mingsheng Long</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19834v1-abstract-short" style="display: inline;">
        Humans construct internal <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19834v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19834v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19834v1-abstract-full" style="display: none;">
        Humans construct internal <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a <span class="search-hit mathjax">world</span>-<span class="search-hit mathjax">model</span> perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, whereas purely verbal <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span> as a core component of CoT reasoning and analyze distinctions among different forms of <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span> for more powerful, human-like multimodal AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19834v1-abstract-full').style.display = 'none'; document.getElementById('2601.19834v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://thuml.github.io/Reasoning-Visual-World</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19819">arXiv:2601.19819</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19819">pdf</a>, <a href="https://arxiv.org/ps/2601.19819">ps</a>, <a href="https://arxiv.org/format/2601.19819">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="General Relativity and Quantum Cosmology">gr-qc</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comment on &#34;Multidimensional arrow of time&#34; (arXiv:2601.14134)
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Galiautdinov%2C+A">Andrei Galiautdinov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19819v1-abstract-short" style="display: inline;">
        &hellip;originates from the monotonic growth of the volume of extra dimensions. While the identification of a geometric origin for time&#39;s arrow is compelling in the case of brane-<span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, we point out a possible tension between the proposed volume growth and the observational stability of the effective four-dimensiona&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19819v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19819v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19819v1-abstract-full" style="display: none;">
        In a recent preprint [arXiv:2601.14134v1], Rubin argues that the arrow of time originates from the monotonic growth of the volume of extra dimensions. While the identification of a geometric origin for time&#39;s arrow is compelling in the case of brane-<span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, we point out a possible tension between the proposed volume growth and the observational stability of the effective four-dimensional Newton&#39;s gravitational constant, G, that may arise in Kaluza-Klein (KK) theory. In standard KK approaches, such volume growth induces a time-variation of G that exceeds Big Bang Nucleosynthesis (BBN) and Lunar Laser Ranging (LLR) bounds by many orders of magnitude. To resolve this tension while preserving the author&#39;s key insight in the Kaluza-Klein case, we propose an extension: the &#34;shape-dynamic arrow of time&#34;. By utilizing the scale-invariant monotonicity of Perelman&#39;s nu-entropy under normalized Ricci flow, we demonstrate how an arrow of time can emerge from the geometric smoothing of extra dimensions at fixed volume, thereby satisfying observational constraints on fundamental constants.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19819v1-abstract-full').style.display = 'none'; document.getElementById('2601.19819v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Comment on arXiv:2601.14134; 6 pages, no figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19752">arXiv:2601.19752</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19752">pdf</a>, <a href="https://arxiv.org/ps/2601.19752">ps</a>, <a href="https://arxiv.org/format/2601.19752">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Agentic Design Patterns: A System-Theoretic Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dao%2C+M">Minh-Dung Dao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+Q+M">Quy Minh Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lam%2C+H+T">Hoang Thanh Lam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+D">Duc-Trong Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+Q">Quoc-Viet Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Sullivan%2C+B">Barry O&#39;Sullivan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+H+D">Hoang D. Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19752v1-abstract-short" style="display: inline;">
        &hellip;two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning &amp; <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span>, Perception &amp; Grounding, Action Execution, Learning &amp; Adaptation, and Inter-Agent Communication. Second, derived from this ar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19752v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19752v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19752v1-abstract-full" style="display: none;">
        With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning &amp; <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span>, Perception &amp; Grounding, Action Execution, Learning &amp; Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive &amp; Decisional, Execution &amp; Interaction, and Adaptive &amp; Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19752v1-abstract-full').style.display = 'none'; document.getElementById('2601.19752v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19484">arXiv:2601.19484</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19484">pdf</a>, <a href="https://arxiv.org/ps/2601.19484">ps</a>, <a href="https://arxiv.org/format/2601.19484">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+Z">Zhiying Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Haitian Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+F+W+B">Frederick W. B. Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+X">Xiaohui Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19484v1-abstract-short" style="display: inline;">
        &hellip;dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19484v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19484v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19484v1-abstract-full" style="display: none;">
        Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19484v1-abstract-full').style.display = 'none'; document.getElementById('2601.19484v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.19336">arXiv:2601.19336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.19336">pdf</a>, <a href="https://arxiv.org/ps/2601.19336">ps</a>, <a href="https://arxiv.org/format/2601.19336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Observations to Events: Event-Aware <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> for Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Z">Zhao-Han Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shaohui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruan%2C+S">Shulan Ruan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">You He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.19336v1-abstract-short" style="display: inline;">
        While model-based reinforcement learning (MBRL) improves sample efficiency by learning <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19336v1-abstract-full').style.display = 'inline'; document.getElementById('2601.19336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.19336v1-abstract-full" style="display: none;">
        While model-based reinforcement learning (MBRL) improves sample efficiency by learning <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.19336v1-abstract-full').style.display = 'none'; document.getElementById('2601.19336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">43 pages, accepted by ICLR 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.18987">arXiv:2601.18987</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.18987">pdf</a>, <a href="https://arxiv.org/ps/2601.18987">ps</a>, <a href="https://arxiv.org/format/2601.18987">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLMs versus the Halting Problem: Revisiting Program Termination Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sultan%2C+O">Oren Sultan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Armengol-Estape%2C+J">Jordi Armengol-Estape</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kesseli%2C+P">Pascal Kesseli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vanegue%2C+J">Julien Vanegue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shahaf%2C+D">Dafna Shahaf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adi%2C+Y">Yossi Adi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Hearn%2C+P">Peter O&#39;Hearn</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.18987v2-abstract-short" style="display: inline;">
        &hellip;LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18987v2-abstract-full').style.display = 'inline'; document.getElementById('2601.18987v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.18987v2-abstract-full" style="display: none;">
        Determining whether a program terminates is a central problem in computer science. Turing&#39;s foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18987v2-abstract-full').style.display = 'none'; document.getElementById('2601.18987v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.18620">arXiv:2601.18620</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.18620">pdf</a>, <a href="https://arxiv.org/ps/2601.18620">ps</a>, <a href="https://arxiv.org/format/2601.18620">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Modeling</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lymperopoulos%2C+P">Panagiotis Lymperopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajasekharan%2C+A">Abhiramon Rajasekharan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlot-Attwell%2C+I">Ian Berlot-Attwell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aroca-Ouellette%2C+S">St√©phane Aroca-Ouellette</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suleman%2C+K">Kaheer Suleman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.18620v1-abstract-short" style="display: inline;">
        Building <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18620v1-abstract-full').style.display = 'inline'; document.getElementById('2601.18620v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.18620v1-abstract-full" style="display: none;">
        Building <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span> approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18620v1-abstract-full').style.display = 'none'; document.getElementById('2601.18620v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages, 2 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.18323">arXiv:2601.18323</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.18323">pdf</a>, <a href="https://arxiv.org/ps/2601.18323">ps</a>, <a href="https://arxiv.org/format/2601.18323">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mi%2C+W">Weishi Mi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+Y">Yong Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+X">Xiaowei Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+X">Xiaozhu Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Z">Zhiyuan Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+K">Kuangzhi Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+K">Kai Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+P">Peidong Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shanghang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+J">Jian Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.18323v1-abstract-short" style="display: inline;">
        &hellip;has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18323v1-abstract-full').style.display = 'inline'; document.getElementById('2601.18323v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.18323v1-abstract-full" style="display: none;">
        The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18323v1-abstract-full').style.display = 'none'; document.getElementById('2601.18323v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.18107">arXiv:2601.18107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.18107">pdf</a>, <a href="https://arxiv.org/ps/2601.18107">ps</a>, <a href="https://arxiv.org/format/2601.18107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agand%2C+P">Pedram Agand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mo Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.18107v1-abstract-short" style="display: inline;">
        &hellip;model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we im&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18107v1-abstract-full').style.display = 'inline'; document.getElementById('2601.18107v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.18107v1-abstract-full" style="display: none;">
        Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random&#39;&#39; and ``suboptimal&#39;&#39; data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.18107v1-abstract-full').style.display = 'none'; document.getElementById('2601.18107v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 2 figures, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.17796">arXiv:2601.17796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.17796">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AI and <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Worden%2C+R">Robert Worden</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.17796v1-abstract-short" style="display: inline;">
        &hellip;the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. <span class="search-hit mathjax">World</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17796v1-abstract-full').style.display = 'inline'; document.getElementById('2601.17796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.17796v1-abstract-full" style="display: none;">
        While large neural nets perform impressively on specific tasks, they are unreliable and unsafe, as is shown by the persistent hallucinations of large language models. This paper shows that large neural nets are intrinsically unreliable, because it is not possible to make or validate a tractable theory of how a neural net works. There is no reliable way to extrapolate its performance from a limited number of test cases to an unlimited set of use cases. To have confidence in the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">models</span> have been proposed as a way to do this. This paper discusses the scope and architecture required of <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>. <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">models</span> are often conceived as models of the physical and natural world, using established theories of natural science, or learned regularities, to predict the physical consequences of AI actions. However, unforeseen consequences of AI actions impact the human social world as much as the physical world. To predict and control the consequences of AI, a <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> needs to include a model of the human social world. I explore the challenges that this entails. Human language is based on a Common Ground of mutual understanding of the world, shared by the people conversing. The common ground is an overlapping subset of each persons <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, including their models of the physical, social and mental worlds. LLMs have no stable representation of a common ground. To be reliable, AI systems will need to represent a common ground with their users, including physical, mental and social domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17796v1-abstract-full').style.display = 'none'; document.getElementById('2601.17796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.17507">arXiv:2601.17507</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.17507">pdf</a>, <a href="https://arxiv.org/ps/2601.17507">ps</a>, <a href="https://arxiv.org/format/2601.17507">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MetaWorld: Skill Transfer and Composition in a Hierarchical <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> for Grounding High-Level Instructions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yutong Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hangxu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pei%2C+K">Kailin Pei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+R">Ruizhe Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+T">Tongtong Feng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.17507v1-abstract-short" style="display: inline;">
        &hellip;limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17507v1-abstract-full').style.display = 'inline'; document.getElementById('2601.17507v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.17507v1-abstract-full" style="display: none;">
        Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17507v1-abstract-full').style.display = 'none'; document.getElementById('2601.17507v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 4 figures, Submitted to ICLR 2026 World Model Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.17323">arXiv:2601.17323</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.17323">pdf</a>, <a href="https://arxiv.org/ps/2601.17323">ps</a>, <a href="https://arxiv.org/format/2601.17323">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SkyReels-V3 Technique Report
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Debang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+Z">Zhengcong Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tuanhui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+Y">Yikun Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiangping Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+M">Mingyuan Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jingtao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiahua Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+B">Baoxuan Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+M">Mingshan Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+Y">Yuqiang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+B">Binjie Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Youqiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+N">Nuo Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Y">Yuzhe Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Z">Zhiheng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Dixuan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guibin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yahui Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.17323v1-abstract-short" style="display: inline;">
        Video generation serves as a cornerstone for building <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyRe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17323v1-abstract-full').style.display = 'inline'; document.getElementById('2601.17323v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.17323v1-abstract-full" style="display: none;">
        Video generation serves as a cornerstone for building <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17323v1-abstract-full').style.display = 'none'; document.getElementById('2601.17323v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.17094">arXiv:2601.17094</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.17094">pdf</a>, <a href="https://arxiv.org/ps/2601.17094">ps</a>, <a href="https://arxiv.org/format/2601.17094">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Boltzmann-GPT: Bridging Energy-Based <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span> and Language Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Niimi%2C+J">Junichiro Niimi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.17094v1-abstract-short" style="display: inline;">
        &hellip;the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17094v1-abstract-full').style.display = 'inline'; document.getElementById('2601.17094v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.17094v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM&#39;s energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, providing empirical support for separating linguistic competence from world understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17094v1-abstract-full').style.display = 'none'; document.getElementById('2601.17094v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.17067">arXiv:2601.17067</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.17067">pdf</a>, <a href="https://arxiv.org/ps/2601.17067">ps</a>, <a href="https://arxiv.org/format/2601.17067">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Mechanistic View on Video Generation as <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>: State and Dynamics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Luozhou Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhifei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yihua Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+D">Dongyu Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+W">Wenhang Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+G">Guibao Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xinli Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+L">Leyi Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Man Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+T">Tianshuo Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+P">Peiran Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+X">Xin Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+P">Pengfei Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Ying-Cong Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.17067v1-abstract-short" style="display: inline;">
        Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17067v1-abstract-full').style.display = 'inline'; document.getElementById('2601.17067v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.17067v1-abstract-full" style="display: none;">
        Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>. However, a gap remains between contemporary &#34;stateless&#34; video architectures and classic state-centric <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.17067v1-abstract-full').style.display = 'none'; document.getElementById('2601.17067v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.16985">arXiv:2601.16985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.16985">pdf</a>, <a href="https://arxiv.org/ps/2601.16985">ps</a>, <a href="https://arxiv.org/format/2601.16985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lorang%2C+P">Pierrick Lorang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.16985v1-abstract-short" style="display: inline;">
        &hellip;abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16985v1-abstract-full').style.display = 'inline'; document.getElementById('2601.16985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.16985v1-abstract-full" style="display: none;">
        Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16985v1-abstract-full').style.display = 'none'; document.getElementById('2601.16985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE ICRA 2025 Doctoral Consortium</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.16163">arXiv:2601.16163</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.16163">pdf</a>, <a href="https://arxiv.org/ps/2601.16163">ps</a>, <a href="https://arxiv.org/format/2601.16163">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M+J">Moo Jin Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yihuai Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+T">Tsung-Yi Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yen-Chen Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yunhao Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lam%2C+G">Grace Lam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+P">Percy Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+S">Shuran Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Ming-Yu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+J">Jinwei Gu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.16163v1-abstract-short" style="display: inline;">
        &hellip;vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16163v1-abstract-full').style.display = 'inline'; document.getElementById('2601.16163v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.16163v1-abstract-full" style="display: none;">
        Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16163v1-abstract-full').style.display = 'none'; document.getElementById('2601.16163v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.16007">arXiv:2601.16007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.16007">pdf</a>, <a href="https://arxiv.org/ps/2601.16007">ps</a>, <a href="https://arxiv.org/format/2601.16007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mak%2C+C">Chak-Wing Mak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+G">Guanyu Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Boyi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hongji Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+X">Xiaowei Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kevin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yichen Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yangfan He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+C">Chun-Kai Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wentao Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+K">Kuangzhi Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+X">Xinyu Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+H">Hongyang He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Kuan Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+T">Tianxiang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+Y">Yongxin Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Youhua Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shanghang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.16007v1-abstract-short" style="display: inline;">
        Modern foundational Multimodal Large Language Models (MLLMs) and video <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question A&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16007v1-abstract-full').style.display = 'inline'; document.getElementById('2601.16007v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.16007v1-abstract-full" style="display: none;">
        Modern foundational Multimodal Large Language Models (MLLMs) and video <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.16007v1-abstract-full').style.display = 'none'; document.getElementById('2601.16007v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.15533">arXiv:2601.15533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.15533">pdf</a>, <a href="https://arxiv.org/ps/2601.15533">ps</a>, <a href="https://arxiv.org/format/2601.15533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhikang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+T">Tingting Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.15533v1-abstract-short" style="display: inline;">
        A <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15533v1-abstract-full').style.display = 'inline'; document.getElementById('2601.15533v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.15533v1-abstract-full" style="display: none;">
        A <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model&#39;s</span> value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15533v1-abstract-full').style.display = 'none'; document.getElementById('2601.15533v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.15284">arXiv:2601.15284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.15284">pdf</a>, <a href="https://arxiv.org/ps/2601.15284">ps</a>, <a href="https://arxiv.org/format/2601.15284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Walk through Paintings: Egocentric <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span> from Internet Priors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bagchi%2C+A">Anurag Bagchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+Z">Zhipeng Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bharadhwaj%2C+H">Homanga Bharadhwaj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu-Xiong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tokmakov%2C+P">Pavel Tokmakov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hebert%2C+M">Martial Hebert</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.15284v1-abstract-short" style="display: inline;">
        &hellip;not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric <span class="search-hit mathjax">World</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15284v1-abstract-full').style.display = 'inline'; document.getElementById('2601.15284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.15284v1-abstract-full" style="display: none;">
        What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15284v1-abstract-full').style.display = 'none'; document.getElementById('2601.15284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.15281">arXiv:2601.15281</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.15281">pdf</a>, <a href="https://arxiv.org/ps/2601.15281">ps</a>, <a href="https://arxiv.org/format/2601.15281">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        StableWorld: Towards Stable and Consistent Long Interactive Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Ying Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+Z">Zhengyao Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+T">Tianlin Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haofan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Binxin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Hubery Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Si%2C+C">Chenyang Si</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.15281v1-abstract-short" style="display: inline;">
        &hellip;video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collap&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15281v1-abstract-full').style.display = 'inline'; document.getElementById('2601.15281v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.15281v1-abstract-full" style="display: none;">
        In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.15281v1-abstract-full').style.display = 'none'; document.getElementById('2601.15281v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 21 figures,</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.14514">arXiv:2601.14514</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.14514">pdf</a>, <a href="https://arxiv.org/ps/2601.14514">ps</a>, <a href="https://arxiv.org/format/2601.14514">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#34;Just in Time&#34; <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Modeling</span> Supports Human Planning and Reasoning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tony Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheyette%2C+S">Sam Cheyette</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Allen%2C+K">Kelsey Allen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tenenbaum%2C+J">Joshua Tenenbaum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+K">Kevin Smith</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.14514v1-abstract-short" style="display: inline;">
        Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these si&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.14514v1-abstract-full').style.display = 'inline'; document.getElementById('2601.14514v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.14514v1-abstract-full" style="display: none;">
        Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a &#34;Just-in-Time&#34; framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.14514v1-abstract-full').style.display = 'none'; document.getElementById('2601.14514v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.14354">arXiv:2601.14354</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.14354">pdf</a>, <a href="https://arxiv.org/ps/2601.14354">ps</a>, <a href="https://arxiv.org/format/2601.14354">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yongchao Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.14354v1-abstract-short" style="display: inline;">
        Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Va&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.14354v1-abstract-full').style.display = 'inline'; document.getElementById('2601.14354v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.14354v1-abstract-full" style="display: none;">
        Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.14354v1-abstract-full').style.display = 'none'; document.getElementById('2601.14354v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">77 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.13304">arXiv:2601.13304</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.13304">pdf</a>, <a href="https://arxiv.org/ps/2601.13304">ps</a>, <a href="https://arxiv.org/format/2601.13304">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+W">Wenxin Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chenlong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+R">Ruisheng Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+N">Nanru Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+S+K">S. Kevin Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yijun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuille%2C+A">Alan Yuille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jieneng Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.13304v1-abstract-short" style="display: inline;">
        &hellip;on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">model</span> (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.13304v1-abstract-full').style.display = 'inline'; document.getElementById('2601.13304v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.13304v1-abstract-full" style="display: none;">
        Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer &#34;what-if&#34; questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">model</span> (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.13304v1-abstract-full').style.display = 'none'; document.getElementById('2601.13304v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code is available: https://github.com/CausalSpatial/CausalSpatial</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.13247">arXiv:2601.13247</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.13247">pdf</a>, <a href="https://arxiv.org/ps/2601.13247">ps</a>, <a href="https://arxiv.org/format/2601.13247">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Aligning Agentic <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span> via Knowledgeable Experience Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+B">Baochang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yunzhi Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+R">Rui Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+S">Shuofei Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+N">Ningyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Huajun Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.13247v1-abstract-short" style="display: inline;">
        &hellip;possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.13247v1-abstract-full').style.display = 'inline'; document.getElementById('2601.13247v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.13247v1-abstract-full" style="display: none;">
        Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.13247v1-abstract-full').style.display = 'none'; document.getElementById('2601.13247v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Ongoing work</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.12939">arXiv:2601.12939</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.12939">pdf</a>, <a href="https://arxiv.org/ps/2601.12939">ps</a>, <a href="https://arxiv.org/format/2601.12939">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Active Inference-Driven <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Modeling</span> for Adaptive UAV Swarm Trajectory Design
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arshid%2C+K">Kaleem Arshid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krayani%2C+A">Ali Krayani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marcenaro%2C+L">Lucio Marcenaro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomez%2C+D+M">David Martin Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Regazzoni%2C+C">Carlo Regazzoni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.12939v1-abstract-short" style="display: inline;">
        &hellip;allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12939v1-abstract-full').style.display = 'inline'; document.getElementById('2601.12939v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.12939v1-abstract-full" style="display: none;">
        This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12939v1-abstract-full').style.display = 'none'; document.getElementById('2601.12939v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted for presentation at the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (IEEE ICASSP 2026) Workshop: &#39;Multi-Modal Signal Processing and AI for Communications and Sensing in 6G and Beyond (MuSiC-6GB)&#39;</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.12538">arXiv:2601.12538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.12538">pdf</a>, <a href="https://arxiv.org/ps/2601.12538">ps</a>, <a href="https://arxiv.org/format/2601.12538">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Agentic Reasoning for Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+T">Tianxin Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Ting-Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhining Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ning%2C+X">Xuying Ning</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Ze Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+J">Jiaru Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Z">Zhichen Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+R">Ruizhong Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+X">Xiao Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+D">Dongqi Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zihao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+M">Mengting Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+D">Duo Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+W">Wenxuan Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yunzhe Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Gaotang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+C">Cheng Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+X">Xiangru Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Y">Yin Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+L">Liri Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hui Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+X">Xianfeng Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuji Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chi Wang</a>
      , et al. (4 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.12538v1-abstract-short" style="display: inline;">
        &hellip;reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, scalable multi-agent training, and governance for real-world deployment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12538v1-abstract-full').style.display = 'inline'; document.getElementById('2601.12538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.12538v1-abstract-full" style="display: none;">
        Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, scalable multi-agent training, and governance for real-world deployment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12538v1-abstract-full').style.display = 'none'; document.getElementById('2601.12538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.12428">arXiv:2601.12428</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.12428">pdf</a>, <a href="https://arxiv.org/ps/2601.12428">ps</a>, <a href="https://arxiv.org/format/2601.12428">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ReWorld: Multi-Dimensional Reward Modeling for Embodied <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+B">Baorui Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wenyao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+L">Liang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Z">Zekun Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiazhao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hongsi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+W">Wenjun Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xin Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.12428v1-abstract-short" style="display: inline;">
        Recently, video-based <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12428v1-abstract-full').style.display = 'inline'; document.getElementById('2601.12428v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.12428v1-abstract-full" style="display: none;">
        Recently, video-based <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12428v1-abstract-full').style.display = 'none'; document.getElementById('2601.12428v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.12277">arXiv:2601.12277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.12277">pdf</a>, <a href="https://arxiv.org/ps/2601.12277">ps</a>, <a href="https://arxiv.org/format/2601.12277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Efficient and Multi-Modal Navigation System with One-Step <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+W">Wangtian Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Ziyang Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+J">Jinming Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+M">Mingliang Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+D">Diyun Xiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.12277v1-abstract-short" style="display: inline;">
        &hellip;methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12277v1-abstract-full').style.display = 'inline'; document.getElementById('2601.12277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.12277v1-abstract-full" style="display: none;">
        Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system&#39;s superior efficiency and robustness compared to state-of-the-art baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.12277v1-abstract-full').style.display = 'none'; document.getElementById('2601.12277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.10905">arXiv:2601.10905</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.10905">pdf</a>, <a href="https://arxiv.org/ps/2601.10905">ps</a>, <a href="https://arxiv.org/format/2601.10905">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Action Shapley: A Training Data Selection Metric for <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> in Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghosh%2C+R">Rajat Ghosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+D">Debojyoti Dutta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.10905v1-abstract-short" style="display: inline;">
        Numerous offline and model-based reinforcement learning systems incorporate <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10905v1-abstract-full').style.display = 'inline'; document.getElementById('2601.10905v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.10905v1-abstract-full" style="display: none;">
        Numerous offline and model-based reinforcement learning systems incorporate <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> to emulate the inherent environments. A <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10905v1-abstract-full').style.display = 'none'; document.getElementById('2601.10905v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.10592">arXiv:2601.10592</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.10592">pdf</a>, <a href="https://arxiv.org/ps/2601.10592">ps</a>, <a href="https://arxiv.org/format/2601.10592">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Action100M: A Large-scale Video Action Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+D">Delong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kasarla%2C+T">Tejaswi Kasarla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bang%2C+Y">Yejin Bang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shukor%2C+M">Mustafa Shukor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+W">Willy Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Jade Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bolourchi%2C+A">Allen Bolourchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moutakanni%2C+T">Theo Moutakanni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fung%2C+P">Pascale Fung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.10592v1-abstract-short" style="display: inline;">
        &hellip;and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10592v1-abstract-full').style.display = 'inline'; document.getElementById('2601.10592v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.10592v1-abstract-full" style="display: none;">
        Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10592v1-abstract-full').style.display = 'none'; document.getElementById('2601.10592v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.10553">arXiv:2601.10553</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.10553">pdf</a>, <a href="https://arxiv.org/ps/2601.10553">ps</a>, <a href="https://arxiv.org/format/2601.10553">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inference-time Physics Alignment of Video Generative Models with Latent <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+J">Jianhao Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaofeng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Friedrich%2C+F">Felix Friedrich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beltran-Velez%2C+N">Nicolas Beltran-Velez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hall%2C+M">Melissa Hall</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Askari-Hemmat%2C+R">Reyhane Askari-Hemmat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xiaochuang Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ballas%2C+N">Nicolas Ballas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Drozdzal%2C+M">Michal Drozdzal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Romero-Soriano%2C+A">Adriana Romero-Soriano</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.10553v1-abstract-short" style="display: inline;">
        &hellip;WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10553v1-abstract-full').style.display = 'inline'; document.getElementById('2601.10553v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.10553v1-abstract-full" style="display: none;">
        State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.10553v1-abstract-full').style.display = 'none'; document.getElementById('2601.10553v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.09452">arXiv:2601.09452</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.09452">pdf</a>, <a href="https://arxiv.org/ps/2601.09452">ps</a>, <a href="https://arxiv.org/format/2601.09452">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MAD: Motion Appearance Decoupling for efficient Driving <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rahimi%2C+A">Ahmad Rahimi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gerard%2C+V">Valentin Gerard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zablocki%2C+E">Eloi Zablocki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cord%2C+M">Matthieu Cord</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alahi%2C+A">Alexandre Alahi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.09452v1-abstract-short" style="display: inline;">
        Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.09452v1-abstract-full').style.display = 'inline'; document.getElementById('2601.09452v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.09452v1-abstract-full" style="display: none;">
        Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively &#34;dressing&#34; the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-<span class="search-hit mathjax">World</span>-<span class="search-hit mathjax">Model</span>/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.09452v1-abstract-full').style.display = 'none'; document.getElementById('2601.09452v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08955">arXiv:2601.08955</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08955">pdf</a>, <a href="https://arxiv.org/ps/2601.08955">ps</a>, <a href="https://arxiv.org/format/2601.08955">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imagine-then-Plan: Agent Learning from Adaptive Lookahead with <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Youwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hanlin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+B">Beichen Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenjie Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08955v1-abstract-short" style="display: inline;">
        Recent advances in <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08955v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08955v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08955v1-abstract-full" style="display: none;">
        Recent advances in <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent&#39;s policy model interacts with the learned <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, yielding multi-step ``imagined&#39;&#39; trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \textit{observable} and \textit{imaginable} Markov decision process to guide policy learning. We instantiate \texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents&#39; reasoning capability, providing valuable insights into addressing broader, complex tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08955v1-abstract-full').style.display = 'none'; document.getElementById('2601.08955v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08388">arXiv:2601.08388</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08388">pdf</a>, <a href="https://arxiv.org/format/2601.08388">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Creativity in AI as Emergence from Domain-Limited Generative Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chutaux%2C+C">Corina Chutaux</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08388v1-abstract-short" style="display: inline;">
        &hellip;conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08388v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08388v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08388v1-abstract-full" style="display: none;">
        Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08388v1-abstract-full').style.display = 'none'; document.getElementById('2601.08388v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07964">arXiv:2601.07964</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07964">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Executable Ontologies in Game Development: From Algorithmic Control to Semantic <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Modeling</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Boldachev%2C+A">Alexander Boldachev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07964v1-abstract-short" style="display: inline;">
        &hellip;implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game sce&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07964v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07964v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07964v1-abstract-full" style="display: none;">
        This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">modeling</span>, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07964v1-abstract-full').style.display = 'none'; document.getElementById('2601.07964v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07823">arXiv:2601.07823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07823">pdf</a>, <a href="https://arxiv.org/ps/2601.07823">ps</a>, <a href="https://arxiv.org/format/2601.07823">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mei%2C+Z">Zhiting Mei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+T">Tenny Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shorinwa%2C+O">Ola Shorinwa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Badithela%2C+A">Apurva Badithela</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Z">Zhonghe Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruno%2C+J">Joseph Bruno</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bland%2C+M">Madison Bland</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zha%2C+L">Lihan Zha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hancock%2C+A">Asher Hancock</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fisac%2C+J+F">Jaime Fern√°ndez Fisac</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dames%2C+P">Philip Dames</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Majumdar%2C+A">Anirudha Majumdar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07823v1-abstract-short" style="display: inline;">
        &hellip;simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07823v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07823v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07823v1-abstract-full" style="display: none;">
        Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07823v1-abstract-full').style.display = 'none'; document.getElementById('2601.07823v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07821">arXiv:2601.07821</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07821">pdf</a>, <a href="https://arxiv.org/ps/2601.07821">ps</a>, <a href="https://arxiv.org/format/2601.07821">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huanyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+K">Kun Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zang%2C+S">Sheng Zang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+K">Kaizhe Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+Y">Yongyuan Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+B">Bo An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaoli Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Huazhe Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07821v1-abstract-short" style="display: inline;">
        &hellip;reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a <span class="search-hit mathjax">world</span>-<span class="search-hit mathjax">model</span>-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07821v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07821v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07821v1-abstract-full" style="display: none;">
        Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a <span class="search-hit mathjax">world</span>-<span class="search-hit mathjax">model</span>-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07821v1-abstract-full').style.display = 'none'; document.getElementById('2601.07821v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://failure-aware-rl.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07463">arXiv:2601.07463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07463">pdf</a>, <a href="https://arxiv.org/ps/2601.07463">ps</a>, <a href="https://arxiv.org/format/2601.07463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Puzzle it Out: Local-to-Global <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> for Offline Multi-Agent Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=li%2C+S">Sijia li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinran Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shibo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07463v1-abstract-short" style="display: inline;">
        &hellip;generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07463v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07463v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07463v1-abstract-full" style="display: none;">
        Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span>, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07463v1-abstract-full').style.display = 'none'; document.getElementById('2601.07463v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06604">arXiv:2601.06604</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06604">pdf</a>, <a href="https://arxiv.org/ps/2601.06604">ps</a>, <a href="https://arxiv.org/format/2601.06604">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-032-13612-1_42">10.1007/978-3-032-13612-1_42 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Object-Centric <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span> Meet Monte Carlo Tree Search
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vakhitov%2C+R">Rodion Vakhitov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ugadiarov%2C+L">Leonid Ugadiarov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panov%2C+A">Aleksandr Panov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06604v1-abstract-short" style="display: inline;">
        &hellip;a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06604v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06604v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06604v1-abstract-full" style="display: none;">
        In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model&#39;s understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06604v1-abstract-full').style.display = 'none'; document.getElementById('2601.06604v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06212">arXiv:2601.06212</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06212">pdf</a>, <a href="https://arxiv.org/ps/2601.06212">ps</a>, <a href="https://arxiv.org/format/2601.06212">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meziani%2C+Y">Yani Meziani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06212v1-abstract-short" style="display: inline;">
        &hellip;Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06212v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06212v1-abstract-full" style="display: none;">
        We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'none'; document.getElementById('2601.06212v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures, 3 tables. Includes appendices with pseudocode and implementation details. Supplementary materials eventually at github.com/yanimeziani/akasha</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T45; 70H05
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.10; I.4.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05930">arXiv:2601.05930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05930">pdf</a>, <a href="https://arxiv.org/ps/2601.05930">ps</a>, <a href="https://arxiv.org/format/2601.05930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can We Predict Before Executing Machine Learning Agents?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+J">Jingsheng Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jintian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yujie Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+Y">Yuren Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yunjun Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+L">Lun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Huajun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+N">Ningyu Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05930v1-abstract-short" style="display: inline;">
        &hellip;To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise compa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05930v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05930v1-abstract-full" style="display: none;">
        Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span>. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05930v1-abstract-full').style.display = 'none'; document.getElementById('2601.05930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Work in progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05848">arXiv:2601.05848</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05848">pdf</a>, <a href="https://arxiv.org/ps/2601.05848">ps</a>, <a href="https://arxiv.org/format/2601.05848">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gillman%2C+N">Nate Gillman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yinghua Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zitian Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+E">Evan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chakravarthy%2C+A">Arjan Chakravarthy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aggarwal%2C+D">Daksh Aggarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Freeman%2C+M">Michael Freeman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Herrmann%2C+C">Charles Herrmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+C">Chen Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05848v1-abstract-short" style="display: inline;">
        Recent advancements in video generation have enabled the development of ``<span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>&#39;&#39; capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05848v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05848v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05848v1-abstract-full" style="display: none;">
        Recent advancements in video generation have enabled the development of ``<span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>&#39;&#39; capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05848v1-abstract-full').style.display = 'none'; document.getElementById('2601.05848v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code and interactive demos at https://goal-force.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05653">arXiv:2601.05653</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05653">pdf</a>, <a href="https://arxiv.org/ps/2601.05653">ps</a>, <a href="https://arxiv.org/format/2601.05653">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+P">Phu-Hoa Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+C">Chi-Nguyen Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dao-Sy%2C+D">Duy-Minh Dao-Sy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen-Lam%2C+P">Phu-Quy Nguyen-Lam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huynh%2C+T">Trung-Kiet Huynh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05653v1-abstract-short" style="display: inline;">
        &hellip;traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05653v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05653v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05653v1-abstract-full" style="display: none;">
        Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05653v1-abstract-full').style.display = 'none'; document.getElementById('2601.05653v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 5 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          91A10; 91A22; 60J20
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05230">arXiv:2601.05230</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05230">pdf</a>, <a href="https://arxiv.org/ps/2601.05230">ps</a>, <a href="https://arxiv.org/format/2601.05230">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Latent Action <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Models</span> In The Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Garrido%2C+Q">Quentin Garrido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagarajan%2C+T">Tushar Nagarajan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Terver%2C+B">Basile Terver</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ballas%2C+N">Nicolas Ballas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=LeCun%2C+Y">Yann LeCun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rabbat%2C+M">Michael Rabbat</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05230v2-abstract-short" style="display: inline;">
        Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05230v2-abstract-full').style.display = 'inline'; document.getElementById('2601.05230v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05230v2-abstract-full" style="display: none;">
        Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05230v2-abstract-full').style.display = 'none'; document.getElementById('2601.05230v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">37 pages, 25 figures; updated references and experimental details</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05138">arXiv:2601.05138</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05138">pdf</a>, <a href="https://arxiv.org/ps/2601.05138">ps</a>, <a href="https://arxiv.org/format/2601.05138">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VerseCrafter: Dynamic Realistic Video <span class="search-hit mathjax">World</span> <span class="search-hit mathjax">Model</span> with 4D Geometric Control
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Sixiao Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+M">Minghao Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wenbo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaoyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+Y">Ying Shan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yanwei Fu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05138v1-abstract-short" style="display: inline;">
        Video <span class="search-hit mathjax">world</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05138v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05138v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05138v1-abstract-full" style="display: none;">
        Video <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span> aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">model</span> that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object&#39;s path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05138v1-abstract-full').style.display = 'none'; document.getElementById('2601.05138v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://sixiaozheng.github.io/VerseCrafter_page/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04695">arXiv:2601.04695</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04695">pdf</a>, <a href="https://arxiv.org/ps/2601.04695">ps</a>, <a href="https://arxiv.org/format/2601.04695">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+E">Enze Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04695v1-abstract-short" style="display: inline;">
        &hellip;and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04695v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04695v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04695v1-abstract-full" style="display: none;">
        We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned <span class="search-hit mathjax">world</span> <span class="search-hit mathjax">models</span>, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what &#34;uncertainty reduction&#34; objectives can and cannot guarantee under rule shifts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04695v1-abstract-full').style.display = 'none'; document.getElementById('2601.04695v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 tables</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T01
        

        
      </p>
    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=%22World+Models%22&amp;searchtype=all&amp;source=header&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>