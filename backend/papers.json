[
  {
    "title": "Object-Centric World Models Meet Monte Carlo Tree Search",
    "authors": [
      "Rodion Vakhitov",
      "\n      \n      Leonid Ugadiarov",
      "\n      \n      Aleksandr Panov"
    ],
    "abstract": "In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.06604",
    "publication_date": "2026-01-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942440224,
    "created_at": "2026-02-01T10:40:40.224Z"
  },
  {
    "title": "Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur",
    "authors": [
      "Yani Meziani"
    ],
    "abstract": "We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (<50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.06212",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Representation Learning",
      "Transformers",
      "Diffusion Models"
    ],
    "id": 1769942440225,
    "created_at": "2026-02-01T10:40:40.225Z"
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
      "Jingsheng Zheng",
      "\n      \n      Jintian Zhang",
      "\n      \n      Yujie Luo",
      "\n      \n      Yuren Mao",
      "\n      \n      Yunjun Gao",
      "\n      \n      Lun Du",
      "\n      \n      Huajun Chen",
      "\n      \n      Ningyu Zhang"
    ],
    "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05930",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440225,
    "created_at": "2026-02-01T10:40:40.225Z"
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "authors": [
      "Nate Gillman",
      "\n      \n      Yinghua Zhou",
      "\n      \n      Zitian Tang",
      "\n      \n      Evan Luo",
      "\n      \n      Arjan Chakravarthy",
      "\n      \n      Daksh Aggarwal",
      "\n      \n      Michael Freeman",
      "\n      \n      Charles Herrmann",
      "\n      \n      Chen Sun"
    ],
    "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05848",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440225,
    "created_at": "2026-02-01T10:40:40.225Z"
  },
  {
    "title": "EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium",
    "authors": [
      "Phu-Hoa Pham",
      "\n      \n      Chi-Nguyen Tran",
      "\n      \n      Duy-Minh Dao-Sy",
      "\n      \n      Phu-Quy Nguyen-Lam",
      "\n      \n      Trung-Kiet Huynh"
    ],
    "abstract": "Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative world model with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05653",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440225,
    "created_at": "2026-02-01T10:40:40.225Z"
  },
  {
    "title": "Learning Latent Action World Models In The Wild",
    "authors": [
      "Quentin Garrido",
      "\n      \n      Tushar Nagarajan",
      "\n      \n      Basile Terver",
      "\n      \n      Nicolas Ballas",
      "\n      \n      Yann LeCun",
      "\n      \n      Michael Rabbat"
    ],
    "abstract": "Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05230",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440226,
    "created_at": "2026-02-01T10:40:40.226Z"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
      "Sixiao Zheng",
      "\n      \n      Minghao Yin",
      "\n      \n      Wenbo Hu",
      "\n      \n      Xiaoyu Li",
      "\n      \n      Ying Shan",
      "\n      \n      Yanwei Fu"
    ],
    "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.05138",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942440226,
    "created_at": "2026-02-01T10:40:40.226Z"
  },
  {
    "title": "Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning",
    "authors": [
      "Enze Pan"
    ],
    "abstract": "We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what \"uncertainty reduction\" objectives can and cannot guarantee under rule shifts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04695",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440226,
    "created_at": "2026-02-01T10:40:40.226Z"
  },
  {
    "title": "Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead",
    "authors": [
      "Oluwatosin Oseni",
      "\n      \n      Shengjie Wang",
      "\n      \n      Jun Zhu",
      "\n      \n      Micah Corah"
    ],
    "abstract": "Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04686",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440226,
    "created_at": "2026-02-01T10:40:40.226Z"
  },
  {
    "title": "UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving",
    "authors": [
      "Zhexiao Xiong",
      "\n      \n      Xin Ye",
      "\n      \n      Burhan Yaman",
      "\n      \n      Sheng Cheng",
      "\n      \n      Yiren Lu",
      "\n      \n      Jingru Luo",
      "\n      \n      Nathan Jacobs",
      "\n      \n      Liu Ren"
    ],
    "abstract": "World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04453",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440227,
    "created_at": "2026-02-01T10:40:40.227Z"
  },
  {
    "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
    "authors": [
      "Chun-Kai Fan",
      "\n      \n      Xiaowei Chi",
      "\n      \n      Xiaozhu Ju",
      "\n      \n      Hao Li",
      "\n      \n      Yong Bao",
      "\n      \n      Yu-Kai Wang",
      "\n      \n      Lizhang Chen",
      "\n      \n      Zhiyuan Jiang",
      "\n      \n      Kuangzhi Ge",
      "\n      \n      Ying Li",
      "\n      \n      Weishi Mi",
      "\n      \n      Qingpo Wuwu",
      "\n      \n      Peidong Jia",
      "\n      \n      Yulin Luo",
      "\n      \n      Kevin Zhang",
      "\n      \n      Zhiyuan Qin",
      "\n      \n      Yong Dai",
      "\n      \n      Sirui Han",
      "\n      \n      Yike Guo",
      "\n      \n      Shanghang Zhang",
      "\n      \n      Jian Tang"
    ],
    "abstract": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04137",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440227,
    "created_at": "2026-02-01T10:40:40.227Z"
  },
  {
    "title": "MobileDreamer: Generative Sketch World Model for GUI Agent",
    "authors": [
      "Yilin Cao",
      "\n      \n      Yufeng Zhong",
      "\n      \n      Zhixiong Zeng",
      "\n      \n      Liming Zheng",
      "\n      \n      Jing Huang",
      "\n      \n      Haibo Qiu",
      "\n      \n      Peng Shi",
      "\n      \n      Wenji Mao",
      "\n      \n      Wan Guanglu"
    ],
    "abstract": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.04035",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440227,
    "created_at": "2026-02-01T10:40:40.227Z"
  },
  {
    "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
    "authors": [
      "Cheng Qian",
      "\n      \n      Emre Can Acikgoz",
      "\n      \n      Bingxuan Li",
      "\n      \n      Xiusi Chen",
      "\n      \n      Yuji Zhang",
      "\n      \n      Bingxiang He",
      "\n      \n      Qinyu Luo",
      "\n      \n      Dilek Hakkani-Tür",
      "\n      \n      Gokhan Tur",
      "\n      \n      Yunzhu Li",
      "\n      \n      Heng Ji"
    ],
    "abstract": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03905",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440228,
    "created_at": "2026-02-01T10:40:40.228Z"
  },
  {
    "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
    "authors": [
      "Wenlong Huang",
      "\n      \n      Yu-Wei Chao",
      "\n      \n      Arsalan Mousavian",
      "\n      \n      Ming-Yu Liu",
      "\n      \n      Dieter Fox",
      "\n      \n      Kaichun Mo",
      "\n      \n      Li Fei-Fei"
    ],
    "abstract": "Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03782",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440228,
    "created_at": "2026-02-01T10:40:40.228Z"
  },
  {
    "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
    "authors": [
      "Zhongbin Guo",
      "\n      \n      Zhen Yang",
      "\n      \n      Yushan Li",
      "\n      \n      Xinyue Zhang",
      "\n      \n      Wenyu Gao",
      "\n      \n      Jiacheng Wang",
      "\n      \n      Chengzhi Li",
      "\n      \n      Xiangrui Liu",
      "\n      \n      Ping Jian"
    ],
    "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03590",
    "publication_date": "2026-01-07",
    "tags": [
      "Robotics"
    ],
    "id": 1769942440228,
    "created_at": "2026-02-01T10:40:40.228Z"
  },
  {
    "title": "Semantic Belief-State World Model for 3D Human Motion Prediction",
    "authors": [
      "Sarim Chaudhry"
    ],
    "abstract": "Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.03517",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942440229,
    "created_at": "2026-02-01T10:40:40.229Z"
  },
  {
    "title": "Time-Scaling Is What Agents Need Now",
    "authors": [
      "Zhi Liu",
      "\n      \n      Guangzhi Wang"
    ],
    "abstract": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities.\n  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.\n  This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02714",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942440229,
    "created_at": "2026-02-01T10:40:40.229Z"
  },
  {
    "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
    "authors": [
      "Junhao Cai",
      "\n      \n      Zetao Cai",
      "\n      \n      Jiafei Cao",
      "\n      \n      Yilun Chen",
      "\n      \n      Zeyu He",
      "\n      \n      Lei Jiang",
      "\n      \n      Hang Li",
      "\n      \n      Hengjie Li",
      "\n      \n      Yang Li",
      "\n      \n      Yufei Liu",
      "\n      \n      Yanan Lu",
      "\n      \n      Qi Lv",
      "\n      \n      Haoxiang Ma",
      "\n      \n      Jiangmiao Pang",
      "\n      \n      Yu Qiao",
      "\n      \n      Zherui Qiu",
      "\n      \n      Yanqing Shen",
      "\n      \n      Xu Shi",
      "\n      \n      Yang Tian",
      "\n      \n      Bolun Wang",
      "\n      \n      Hanqing Wang",
      "\n      \n      Jiaheng Wang",
      "\n      \n      Tai Wang",
      "\n      \n      Xueyuan Wei",
      "\n      \n      Chao Wu\n      ",
      "et al. (17 additional authors not shown)"
    ],
    "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02456",
    "publication_date": "2026-01-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942440230,
    "created_at": "2026-02-01T10:40:40.230Z"
  },
  {
    "title": "Modeling the Mental World for Embodied AI: A Comprehensive Review",
    "authors": [
      "Biyuan Liu",
      "\n      \n      Daigang Xu",
      "\n      \n      Lei Jiang",
      "\n      \n      Wenjun Guo",
      "\n      \n      Ping Chen"
    ],
    "abstract": "As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice.\n  To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.02378",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440231,
    "created_at": "2026-02-01T10:40:40.231Z"
  },
  {
    "title": "AI Agent Systems: Architectures, Applications, and Evaluation",
    "authors": [
      "Bin Xu"
    ],
    "abstract": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01743",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440231,
    "created_at": "2026-02-01T10:40:40.231Z"
  },
  {
    "title": "Explicit World Models for Reliable Human-Robot Collaboration",
    "authors": [
      "Kenneth Kwok",
      "\n      \n      Basura Fernando",
      "\n      \n      Qianli Xu",
      "\n      \n      Vigneshwaran Subbaraju",
      "\n      \n      Dongkyu Choi",
      "\n      \n      Boon Kiat Quek"
    ],
    "abstract": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01705",
    "publication_date": "2026-01-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440232,
    "created_at": "2026-02-01T10:40:40.232Z"
  },
  {
    "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller",
    "authors": [
      "Tran Tien Dat",
      "\n      \n      Nguyen Hai An",
      "\n      \n      Nguyen Khanh Viet Dung",
      "\n      \n      Nguyen Duy Duc"
    ],
    "abstract": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01577",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942440232,
    "created_at": "2026-02-01T10:40:40.232Z"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "authors": [
      "Yang Zhou",
      "\n      \n      Hao Shao",
      "\n      \n      Letian Wang",
      "\n      \n      Zhuofan Zong",
      "\n      \n      Hongsheng Li",
      "\n      \n      Steven L. Waslander"
    ],
    "abstract": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01528",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440233,
    "created_at": "2026-02-01T10:40:40.233Z"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "authors": [
      "Rong Zhou",
      "\n      \n      Dongping Chen",
      "\n      \n      Zihan Jia",
      "\n      \n      Yao Su",
      "\n      \n      Yixin Liu",
      "\n      \n      Yiwen Lu",
      "\n      \n      Dongwei Shi",
      "\n      \n      Yue Huang",
      "\n      \n      Tianyang Xu",
      "\n      \n      Yi Pan",
      "\n      \n      Xinliang Li",
      "\n      \n      Yohannes Abate",
      "\n      \n      Qingyu Chen",
      "\n      \n      Zhengzhong Tu",
      "\n      \n      Yu Yang",
      "\n      \n      Yu Zhang",
      "\n      \n      Qingsong Wen",
      "\n      \n      Gengchen Mai",
      "\n      \n      Sunyang Fu",
      "\n      \n      Jiachen Li",
      "\n      \n      Xuyu Wang",
      "\n      \n      Ziran Wang",
      "\n      \n      Jing Huang",
      "\n      \n      Tianming Liu",
      "\n      \n      Yong Chen\n      ",
      "et al. (2 additional authors not shown)"
    ],
    "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01321",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440233,
    "created_at": "2026-02-01T10:40:40.233Z"
  },
  {
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "authors": [
      "Hansen Jin Lillemark",
      "\n      \n      Benhao Huang",
      "\n      \n      Fangneng Zhan",
      "\n      \n      Yilun Du",
      "\n      \n      Thomas Anderson Keller"
    ],
    "abstract": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.01075",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942440233,
    "created_at": "2026-02-01T10:40:40.233Z"
  },
  {
    "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
    "authors": [
      "Nicolas Bougie",
      "\n      \n      Gian Maria Marconi",
      "\n      \n      Tony Yip",
      "\n      \n      Narimasa Watanabe"
    ],
    "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00930",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440234,
    "created_at": "2026-02-01T10:40:40.234Z"
  },
  {
    "title": "Device-Native Autonomous Agents for Privacy-Preserving Negotiations",
    "authors": [
      "Joyjit Roy",
      "\n      \n      Samaresh Kumar Singh"
    ],
    "abstract": "Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00911",
    "publication_date": "2026-01-23",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440234,
    "created_at": "2026-02-01T10:40:40.234Z"
  },
  {
    "title": "Value-guided action planning with JEPA world models",
    "authors": [
      "Matthieu Destrade",
      "\n      \n      Oumayma Bounou",
      "\n      \n      Quentin Le Lidec",
      "\n      \n      Jean Ponce",
      "\n      \n      Yann LeCun"
    ],
    "abstract": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00844",
    "publication_date": "2025-12-28",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942440235,
    "created_at": "2026-02-01T10:40:40.235Z"
  },
  {
    "title": "A formal theory on problem space as a semantic world model in systems engineering",
    "authors": [
      "Mayuranath SureshKumar",
      "\n      \n      Hanumanthrao Kannan"
    ],
    "abstract": "Classic problem-space theory models problem solving as a navigation through a structured space of states, operators, goals, and constraints. Systems Engineering (SE) employs analogous constructs (functional analysis, operational analysis, scenarios, trade studies), yet still lacks a rigorous systems-theoretic representation of the problem space itself. In current practice, reasoning often proceeds directly from stakeholder goals to prescriptive artifacts. This makes foundational assumptions about the operational environment, admissible interactions, and contextual conditions implicit or prematurely embedded in architectures or requirements. This paper addresses that gap by formalizing the problem space as an explicit semantic world model containing theoretical constructs that are defined prior to requirements and solution commitments. These constructs along with the developed axioms, theorems and corollary establish a rigorous criterion for unambiguous boundary semantics, context-dependent interaction traceability to successful stakeholder goal satisfaction, and sufficiency of problem-space specification over which disciplined reasoning can occur independent of solution design. It offers a clear distinction between what is true of the problem domain and what is chosen as a solution. The paper concludes by discussing the significance of the theory on practitioners and provides a dialogue-based hypothetical case study between a stakeholder and an engineer, demonstrating how the theory guides problem framing before designing any prescriptive artifacts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00755",
    "publication_date": "2026-01-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440235,
    "created_at": "2026-02-01T10:40:40.235Z"
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "authors": [
      "Yuxue Yang",
      "\n      \n      Lue Fan",
      "\n      \n      Ziqi Shi",
      "\n      \n      Junran Peng",
      "\n      \n      Feng Wang",
      "\n      \n      Zhaoxiang Zhang"
    ],
    "abstract": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00393",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942440236,
    "created_at": "2026-02-01T10:40:40.236Z"
  },
  {
    "title": "TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model",
    "authors": [
      "Yabo Chen",
      "\n      \n      Yuanzhi Liang",
      "\n      \n      Jiepeng Wang",
      "\n      \n      Tingxi Chen",
      "\n      \n      Junfei Cheng",
      "\n      \n      Zixiao Gu",
      "\n      \n      Yuyang Huang",
      "\n      \n      Zicheng Jiang",
      "\n      \n      Wei Li",
      "\n      \n      Tian Li",
      "\n      \n      Weichen Li",
      "\n      \n      Zuoxin Li",
      "\n      \n      Guangce Liu",
      "\n      \n      Jialun Liu",
      "\n      \n      Junqi Liu",
      "\n      \n      Haoyuan Wang",
      "\n      \n      Qizhen Weng",
      "\n      \n      Xuan'er Wu",
      "\n      \n      Xunzhi Xiang",
      "\n      \n      Xiaoyan Yang",
      "\n      \n      Xin Zhang",
      "\n      \n      Shiwen Zhang",
      "\n      \n      Junyu Zhou",
      "\n      \n      Chengcheng Zhou",
      "\n      \n      Haibin Huang\n      ",
      "et al. (2 additional authors not shown)"
    ],
    "abstract": "World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.00051",
    "publication_date": "2025-12-31",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning"
    ],
    "id": 1769942440236,
    "created_at": "2026-02-01T10:40:40.236Z"
  },
  {
    "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving",
    "authors": [
      "Qian Cheng",
      "\n      \n      Weitao Zhou",
      "\n      \n      Cheng Jing",
      "\n      \n      Nanshan Deng",
      "\n      \n      Junze Wen",
      "\n      \n      Zhaoyang Liu",
      "\n      \n      Kun Jiang",
      "\n      \n      Diange Yang"
    ],
    "abstract": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment. This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24712",
    "publication_date": "2026-01-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "id": 1769942440236,
    "created_at": "2026-02-01T10:40:40.237Z"
  },
  {
    "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
    "authors": [
      "Basile Terver",
      "\n      \n      Tsung-Yen Yang",
      "\n      \n      Jean Ponce",
      "\n      \n      Adrien Bardes",
      "\n      \n      Yann LeCun"
    ],
    "abstract": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24497",
    "publication_date": "2026-01-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942440237,
    "created_at": "2026-02-01T10:40:40.237Z"
  },
  {
    "title": "World model inspired sarcasm reasoning with large language model agents",
    "authors": [
      "Keito Inoshita",
      "\n      \n      Shinnosuke Mizuno"
    ],
    "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24329",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440238,
    "created_at": "2026-02-01T10:40:40.238Z"
  },
  {
    "title": "Large Emotional World Model",
    "authors": [
      "Changhao Song",
      "\n      \n      Yazhou Zhang",
      "\n      \n      Hui Gao",
      "\n      \n      Chang Yang",
      "\n      \n      Peng Zhang"
    ],
    "abstract": "World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.24149",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440238,
    "created_at": "2026-02-01T10:40:40.238Z"
  },
  {
    "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
    "authors": [
      "Tianxin Xie",
      "\n      \n      Wentao Lei",
      "\n      \n      Guanjie Huang",
      "\n      \n      Pengfei Zhang",
      "\n      \n      Kai Jiang",
      "\n      \n      Chunhui Zhang",
      "\n      \n      Fengji Ma",
      "\n      \n      Haoyu He",
      "\n      \n      Han Zhang",
      "\n      \n      Jiangshan He",
      "\n      \n      Jinting Wang",
      "\n      \n      Linghan Fang",
      "\n      \n      Lufei Gao",
      "\n      \n      Orkesh Ablet",
      "\n      \n      Peihua Zhang",
      "\n      \n      Ruolin Hu",
      "\n      \n      Shengyu Li",
      "\n      \n      Weilin Lin",
      "\n      \n      Xiaoyang Feng",
      "\n      \n      Xinyue Yang",
      "\n      \n      Yan Rong",
      "\n      \n      Yanyun Wang",
      "\n      \n      Zihang Shao",
      "\n      \n      Zelin Zhao",
      "\n      \n      Chenxing Li\n      ",
      "et al. (5 additional authors not shown)"
    ],
    "abstract": "Text-to-audio-video (T2AV) generation underpins a wide range of applications demanding realistic audio-visual content, including virtual reality, world modeling, gaming, and filmmaking. However, existing T2AV models remain incapable of generating physically plausible sounds, primarily due to their limited understanding of physical principles. To situate current research progress, we present PhyAVBench, a challenging audio physics-sensitivity benchmark designed to systematically evaluate the audio physics grounding capabilities of existing T2AV models. PhyAVBench comprises 1,000 groups of paired text prompts with controlled physical variables that implicitly induce sound variations, enabling a fine-grained assessment of models' sensitivity to changes in underlying acoustic conditions. We term this evaluation paradigm the Audio-Physics Sensitivity Test (APST). Unlike prior benchmarks that primarily focus on audio-video synchronization, PhyAVBench explicitly evaluates models' understanding of the physical mechanisms underlying sound generation, covering 6 major audio physics dimensions, 4 daily scenarios (music, sound effects, speech, and their mix), and 50 fine-grained test points, ranging from fundamental aspects such as sound diffraction to more complex phenomena, e.g., Helmholtz resonance. Each test point consists of multiple groups of paired prompts, where each prompt is grounded by at least 20 newly recorded or collected real-world videos, thereby minimizing the risk of data leakage during model pre-training. Both prompts and videos are iteratively refined through rigorous human-involved error correction and quality control to ensure high quality. We argue that only models with a genuine grasp of audio-related physical principles can generate physically consistent audio-visual content. We hope PhyAVBench will stimulate future progress in this critical yet largely unexplored domain.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23994",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942440239,
    "created_at": "2026-02-01T10:40:40.239Z"
  },
  {
    "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
    "authors": [
      "Guo Ye",
      "\n      \n      Zexi Zhang",
      "\n      \n      Xu Zhao",
      "\n      \n      Shang Wu",
      "\n      \n      Haoran Lu",
      "\n      \n      Shihan Lu",
      "\n      \n      Han Liu"
    ],
    "abstract": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23864",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440239,
    "created_at": "2026-02-01T10:40:40.239Z"
  },
  {
    "title": "Emergent World Beliefs: Exploring Transformers in Stochastic Games",
    "authors": [
      "Adam Kamel",
      "\n      \n      Tanish Rastogi",
      "\n      \n      Michael Ma",
      "\n      \n      Kailash Ranganathan",
      "\n      \n      Kevin Zhu"
    ],
    "abstract": "Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23722",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942440240,
    "created_at": "2026-02-01T10:40:40.240Z"
  },
  {
    "title": "Web World Models",
    "authors": [
      "Jichen Feng",
      "\n      \n      Yifan Zhang",
      "\n      \n      Chenggong Zhang",
      "\n      \n      Yifu Lu",
      "\n      \n      Shilong Liu",
      "\n      \n      Mengdi Wang"
    ],
    "abstract": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23676",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440241,
    "created_at": "2026-02-01T10:40:40.241Z"
  },
  {
    "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
    "authors": [
      "Pengfei Zhou",
      "\n      \n      Liliang Chen",
      "\n      \n      Shengcong Chen",
      "\n      \n      Di Chen",
      "\n      \n      Wenzhi Zhao",
      "\n      \n      Rongjun Jin",
      "\n      \n      Guanghui Ren",
      "\n      \n      Jianlan Luo"
    ],
    "abstract": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23541",
    "publication_date": "2025-12-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440241,
    "created_at": "2026-02-01T10:40:40.241Z"
  },
  {
    "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
    "authors": [
      "Tianze Xia",
      "\n      \n      Yongkang Li",
      "\n      \n      Lijun Zhou",
      "\n      \n      Jingfeng Yao",
      "\n      \n      Kaixin Xiong",
      "\n      \n      Haiyang Sun",
      "\n      \n      Bing Wang",
      "\n      \n      Kun Ma",
      "\n      \n      Guang Chen",
      "\n      \n      Hangjun Ye",
      "\n      \n      Wenyu Liu",
      "\n      \n      Xinggang Wang"
    ],
    "abstract": "World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23421",
    "publication_date": "2025-12-30",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning"
    ],
    "id": 1769942440242,
    "created_at": "2026-02-01T10:40:40.242Z"
  },
  {
    "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
    "authors": [
      "Tianchen Deng",
      "\n      \n      Xuefeng Chen",
      "\n      \n      Yi Chen",
      "\n      \n      Qu Chen",
      "\n      \n      Yuyao Xu",
      "\n      \n      Lijin Yang",
      "\n      \n      Le Xu",
      "\n      \n      Yu Zhang",
      "\n      \n      Bo Zhang",
      "\n      \n      Wuxiong Huang",
      "\n      \n      Hesheng Wang"
    ],
    "abstract": "Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23180",
    "publication_date": "2026-01-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942440243,
    "created_at": "2026-02-01T10:40:40.243Z"
  },
  {
    "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "authors": [
      "Yufan He",
      "\n      \n      Pengfei Guo",
      "\n      \n      Mengya Xu",
      "\n      \n      Zhaoshuo Li",
      "\n      \n      Andriy Myronenko",
      "\n      \n      Dillan Imans",
      "\n      \n      Bingjie Liu",
      "\n      \n      Dongren Yang",
      "\n      \n      Mingxue Gu",
      "\n      \n      Yongnan Ji",
      "\n      \n      Yueming Jin",
      "\n      \n      Ren Zhao",
      "\n      \n      Baiyong Shen",
      "\n      \n      Daguang Xu"
    ],
    "abstract": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.23162",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942440244,
    "created_at": "2026-02-01T10:40:40.244Z"
  },
  {
    "title": "YOLO-IOD: Towards Real Time Incremental Object Detection",
    "authors": [
      "Shizhou Zhang",
      "\n      \n      Xueqiang Lv",
      "\n      \n      Yinghui Xing",
      "\n      \n      Qirui Wu",
      "\n      \n      Di Xu",
      "\n      \n      Chen Zhao",
      "\n      \n      Yanning Zhang"
    ],
    "abstract": "Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22973",
    "publication_date": "2025-12-31",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440244,
    "created_at": "2026-02-01T10:40:40.244Z"
  },
  {
    "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback",
    "authors": [
      "Mengkang Hu",
      "\n      \n      Bowei Xia",
      "\n      \n      Yuran Wu",
      "\n      \n      Ailing Yu",
      "\n      \n      Yude Zou",
      "\n      \n      Qiguang Chen",
      "\n      \n      Shijian Wang",
      "\n      \n      Jiarui Jin",
      "\n      \n      Kexin Li",
      "\n      \n      Wenxiang Jiao",
      "\n      \n      Yuan Lu",
      "\n      \n      Ping Luo"
    ],
    "abstract": "Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22336",
    "publication_date": "2025-12-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942440245,
    "created_at": "2026-02-01T10:40:40.245Z"
  },
  {
    "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling",
    "authors": [
      "Conor Wallace",
      "\n      \n      Umer Siddique",
      "\n      \n      Yongcan Cao"
    ],
    "abstract": "Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \\Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \\ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \\Collab effectively distinguishes teammate types, while \\ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.22129",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942440246,
    "created_at": "2026-02-01T10:40:40.246Z"
  },
  {
    "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space",
    "authors": [
      "Weichen Zhang",
      "\n      \n      Peizhi Tang",
      "\n      \n      Xin Zeng",
      "\n      \n      Fanhang Man",
      "\n      \n      Shiquan Yu",
      "\n      \n      Zichao Dai",
      "\n      \n      Baining Zhao",
      "\n      \n      Hongjin Chen",
      "\n      \n      Yu Shang",
      "\n      \n      Wei Wu",
      "\n      \n      Chen Gao",
      "\n      \n      Xinlei Chen",
      "\n      \n      Xin Wang",
      "\n      \n      Yong Li",
      "\n      \n      Wenwu Zhu"
    ],
    "abstract": "Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21887",
    "publication_date": "2026-01-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Planning"
    ],
    "id": 1769942440246,
    "created_at": "2026-02-01T10:40:40.246Z"
  },
  {
    "title": "AstraNav-World: World Model for Foresight Control and Consistency",
    "authors": [
      "Junjun Hu",
      "\n      \n      Jintao Chen",
      "\n      \n      Haochen Bai",
      "\n      \n      Minghua Luo",
      "\n      \n      Shichao Xie",
      "\n      \n      Ziyi Chen",
      "\n      \n      Fei Liu",
      "\n      \n      Zedong Chu",
      "\n      \n      Xinda Xue",
      "\n      \n      Botao Ren",
      "\n      \n      Xiaolong Wu",
      "\n      \n      Mu Xu",
      "\n      \n      Shanghang Zhang"
    ],
    "abstract": "Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled \"envision-then-plan\" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21714",
    "publication_date": "2025-12-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning"
    ],
    "id": 1769942440247,
    "created_at": "2026-02-01T10:40:40.247Z"
  },
  {
    "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid",
    "authors": [
      "Emmy Liu",
      "\n      \n      Varun Gangal",
      "\n      \n      Chelsea Zou",
      "\n      \n      Xiaoqi Huang",
      "\n      \n      Michael Yu",
      "\n      \n      Alex Chang",
      "\n      \n      Zhuofu Tao",
      "\n      \n      Sachin Kumar",
      "\n      \n      Steven Y. Feng"
    ],
    "abstract": "Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.\n  We argue that this unified view is useful because it forces evaluations to make clear their assumed \"world\" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21577",
    "publication_date": "2025-12-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942440248,
    "created_at": "2026-02-01T10:40:40.248Z"
  },
  {
    "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
    "authors": [
      "Yu He",
      "\n      \n      Da Huang",
      "\n      \n      Zhenyang Liu",
      "\n      \n      Zixiao Gu",
      "\n      \n      Qiang Sun",
      "\n      \n      Guangnan Ye",
      "\n      \n      Yanwei Fu"
    ],
    "abstract": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21201",
    "publication_date": "2025-12-24",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942440249,
    "created_at": "2026-02-01T10:40:40.249Z"
  },
  {
    "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
    "authors": [
      "Hang Ding",
      "\n      \n      Peidong Liu",
      "\n      \n      Junqiao Wang",
      "\n      \n      Ziwei Ji",
      "\n      \n      Meng Cao",
      "\n      \n      Rongzhao Zhang",
      "\n      \n      Lynn Ai",
      "\n      \n      Eric Yang",
      "\n      \n      Tianyu Shi",
      "\n      \n      Lei Yu"
    ],
    "abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22149",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459138,
    "created_at": "2026-02-01T10:40:59.138Z"
  },
  {
    "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
    "authors": [
      "Lakshya Gupta",
      "\n      \n      Litao Li",
      "\n      \n      Yizhe Liu",
      "\n      \n      Sriram Ganapathi Subramanian",
      "\n      \n      Kaheer Suleman",
      "\n      \n      Zichen Zhang",
      "\n      \n      Haoye Lu",
      "\n      \n      Sumit Pasupalak"
    ],
    "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22130",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction"
    ],
    "id": 1769942459140,
    "created_at": "2026-02-01T10:40:59.140Z"
  },
  {
    "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
    "authors": [
      "Irsyad Adam",
      "\n      \n      Zekai Chen",
      "\n      \n      David Laprade",
      "\n      \n      Shaun Porwal",
      "\n      \n      David Laub",
      "\n      \n      Erik Reinertsen",
      "\n      \n      Arda Pekis",
      "\n      \n      Kevin Brown"
    ],
    "abstract": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22128",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "id": 1769942459141,
    "created_at": "2026-02-01T10:40:59.141Z"
  },
  {
    "title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
    "authors": [
      "Onur T. Doganay",
      "\n      \n      Alexander Klawonn",
      "\n      \n      Martin Eigel",
      "\n      \n      Hanno Gottschalk"
    ],
    "abstract": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22086",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942459142,
    "created_at": "2026-02-01T10:40:59.142Z"
  },
  {
    "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
    "authors": [
      "Linhan Wang",
      "\n      \n      Zichong Yang",
      "\n      \n      Chen Bai",
      "\n      \n      Guoxiang Zhang",
      "\n      \n      Xiaotong Liu",
      "\n      \n      Xiaoyin Zheng",
      "\n      \n      Xiao-Xiao Long",
      "\n      \n      Chang-Tien Lu",
      "\n      \n      Cheng Lu"
    ],
    "abstract": "End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.22032",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning",
      "Representation Learning",
      "Transformers"
    ],
    "id": 1769942459143,
    "created_at": "2026-02-01T10:40:59.143Z"
  },
  {
    "title": "Causal World Modeling for Robot Control",
    "authors": [
      "Lin Li",
      "\n      \n      Qihang Zhang",
      "\n      \n      Yiming Luo",
      "\n      \n      Shuai Yang",
      "\n      \n      Ruilin Wang",
      "\n      \n      Fei Han",
      "\n      \n      Mingrui Yu",
      "\n      \n      Zelin Gao",
      "\n      \n      Nan Xue",
      "\n      \n      Xing Zhu",
      "\n      \n      Yujun Shen",
      "\n      \n      Yinghao Xu"
    ],
    "abstract": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21998",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Representation Learning",
      "Transformers"
    ],
    "id": 1769942459143,
    "created_at": "2026-02-01T10:40:59.143Z"
  },
  {
    "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
    "authors": [
      "Weidong Huang",
      "\n      \n      Zhehan Li",
      "\n      \n      Hangxin Liu",
      "\n      \n      Biao Hou",
      "\n      \n      Yao Su",
      "\n      \n      Jingwen Zhang"
    ],
    "abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21363",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459144,
    "created_at": "2026-02-01T10:40:59.144Z"
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "authors": [
      "Rishi Upadhyay",
      "\n      \n      Howard Zhang",
      "\n      \n      Jim Solomon",
      "\n      \n      Ayush Agrawal",
      "\n      \n      Pranay Boreddy",
      "\n      \n      Shruti Satya Narayana",
      "\n      \n      Yunhao Ba",
      "\n      \n      Alex Wong",
      "\n      \n      Celso M de Melo",
      "\n      \n      Achuta Kadambi"
    ],
    "abstract": "Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.21282",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459145,
    "created_at": "2026-02-01T10:40:59.145Z"
  },
  {
    "title": "Advancing Open-source World Models",
    "authors": [
      "Robbyant Team",
      "\n      \n      Zelin Gao",
      "\n      \n      Qiuyu Wang",
      "\n      \n      Yanhong Zeng",
      "\n      \n      Jiapeng Zhu",
      "\n      \n      Ka Leong Cheng",
      "\n      \n      Yixuan Li",
      "\n      \n      Hanlin Wang",
      "\n      \n      Yinghao Xu",
      "\n      \n      Shuailei Ma",
      "\n      \n      Yihang Chen",
      "\n      \n      Jie Liu",
      "\n      \n      Yansong Cheng",
      "\n      \n      Yao Yao",
      "\n      \n      Jiayi Zhu",
      "\n      \n      Yihao Meng",
      "\n      \n      Kecheng Zheng",
      "\n      \n      Qingyan Bai",
      "\n      \n      Jingye Chen",
      "\n      \n      Zehong Shen",
      "\n      \n      Yue Yu",
      "\n      \n      Xing Zhu",
      "\n      \n      Yujun Shen",
      "\n      \n      Hao Ouyang"
    ],
    "abstract": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20540",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942459146,
    "created_at": "2026-02-01T10:40:59.146Z"
  },
  {
    "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs",
    "authors": [
      "Oguzhan Gungordu",
      "\n      \n      Siheng Xiong",
      "\n      \n      Faramarz Fekri"
    ],
    "abstract": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20539",
    "publication_date": "2026-01-29",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942459147,
    "created_at": "2026-02-01T10:40:59.147Z"
  },
  {
    "title": "Distributional value gradients for stochastic environments",
    "authors": [
      "Baptiste Debes",
      "\n      \n      Tinne Tuytelaars"
    ],
    "abstract": "Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.20071",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942459148,
    "created_at": "2026-02-01T10:40:59.148Z"
  },
  {
    "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
    "authors": [
      "Jeanne Malécot",
      "\n      \n      Hamed Rahimi",
      "\n      \n      Jeanne Cattoni",
      "\n      \n      Marie Samson",
      "\n      \n      Mouad Abrini",
      "\n      \n      Mahdi Khoramshahi",
      "\n      \n      Maribel Pino",
      "\n      \n      Mohamed Chetouani"
    ],
    "abstract": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19839",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459149,
    "created_at": "2026-02-01T10:40:59.149Z"
  },
  {
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "authors": [
      "Jialong Wu",
      "\n      \n      Xiaoying Zhang",
      "\n      \n      Hongyi Yuan",
      "\n      \n      Xiangcheng Zhang",
      "\n      \n      Tianhao Huang",
      "\n      \n      Changjing He",
      "\n      \n      Chaoyi Deng",
      "\n      \n      Renrui Zhang",
      "\n      \n      Youbin Wu",
      "\n      \n      Mingsheng Long"
    ],
    "abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19834",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459150,
    "created_at": "2026-02-01T10:40:59.150Z"
  },
  {
    "title": "Comment on \"Multidimensional arrow of time\" (arXiv:2601.14134)",
    "authors": [
      "Andrei Galiautdinov"
    ],
    "abstract": "In a recent preprint [arXiv:2601.14134v1], Rubin argues that the arrow of time originates from the monotonic growth of the volume of extra dimensions. While the identification of a geometric origin for time's arrow is compelling in the case of brane-world models, we point out a possible tension between the proposed volume growth and the observational stability of the effective four-dimensional Newton's gravitational constant, G, that may arise in Kaluza-Klein (KK) theory. In standard KK approaches, such volume growth induces a time-variation of G that exceeds Big Bang Nucleosynthesis (BBN) and Lunar Laser Ranging (LLR) bounds by many orders of magnitude. To resolve this tension while preserving the author's key insight in the Kaluza-Klein case, we propose an extension: the \"shape-dynamic arrow of time\". By utilizing the scale-invariant monotonicity of Perelman's nu-entropy under normalized Ricci flow, we demonstrate how an arrow of time can emerge from the geometric smoothing of extra dimensions at fixed volume, thereby satisfying observational constraints on fundamental constants.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19819",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459151,
    "created_at": "2026-02-01T10:40:59.151Z"
  },
  {
    "title": "Agentic Design Patterns: A System-Theoretic Framework",
    "authors": [
      "Minh-Dung Dao",
      "\n      \n      Quy Minh Le",
      "\n      \n      Hoang Thanh Lam",
      "\n      \n      Duc-Trong Le",
      "\n      \n      Quoc-Viet Pham",
      "\n      \n      Barry O'Sullivan",
      "\n      \n      Hoang D. Nguyen"
    ],
    "abstract": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19752",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459152,
    "created_at": "2026-02-01T10:40:59.152Z"
  },
  {
    "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
    "authors": [
      "Yin Wang",
      "\n      \n      Zhiying Leng",
      "\n      \n      Haitian Liu",
      "\n      \n      Frederick W. B. Li",
      "\n      \n      Mu Li",
      "\n      \n      Xiaohui Liang"
    ],
    "abstract": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19484",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942459153,
    "created_at": "2026-02-01T10:40:59.153Z"
  },
  {
    "title": "From Observations to Events: Event-Aware World Model for Reinforcement Learning",
    "authors": [
      "Zhao-Han Peng",
      "\n      \n      Shaohui Li",
      "\n      \n      Zhi Li",
      "\n      \n      Shulan Ruan",
      "\n      \n      Yu Liu",
      "\n      \n      You He"
    ],
    "abstract": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.19336",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459154,
    "created_at": "2026-02-01T10:40:59.154Z"
  },
  {
    "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction",
    "authors": [
      "Oren Sultan",
      "\n      \n      Jordi Armengol-Estape",
      "\n      \n      Pascal Kesseli",
      "\n      \n      Julien Vanegue",
      "\n      \n      Dafna Shahaf",
      "\n      \n      Yossi Adi",
      "\n      \n      Peter O'Hearn"
    ],
    "abstract": "Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18987",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942459155,
    "created_at": "2026-02-01T10:40:59.155Z"
  },
  {
    "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling",
    "authors": [
      "Panagiotis Lymperopoulos",
      "\n      \n      Abhiramon Rajasekharan",
      "\n      \n      Ian Berlot-Attwell",
      "\n      \n      Stéphane Aroca-Ouellette",
      "\n      \n      Kaheer Suleman"
    ],
    "abstract": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18620",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942459156,
    "created_at": "2026-02-01T10:40:59.156Z"
  },
  {
    "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
    "authors": [
      "Weishi Mi",
      "\n      \n      Yong Bao",
      "\n      \n      Xiaowei Chi",
      "\n      \n      Xiaozhu Ju",
      "\n      \n      Zhiyuan Qin",
      "\n      \n      Kuangzhi Ge",
      "\n      \n      Kai Tang",
      "\n      \n      Peidong Jia",
      "\n      \n      Shanghang Zhang",
      "\n      \n      Jian Tang"
    ],
    "abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18323",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459157,
    "created_at": "2026-02-01T10:40:59.157Z"
  },
  {
    "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions",
    "authors": [
      "Pedram Agand",
      "\n      \n      Mo Chen"
    ],
    "abstract": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.18107",
    "publication_date": "2026-01-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1769942459158,
    "created_at": "2026-02-01T10:40:59.158Z"
  },
  {
    "title": "AI and World Models",
    "authors": [
      "Robert Worden"
    ],
    "abstract": "While large neural nets perform impressively on specific tasks, they are unreliable and unsafe, as is shown by the persistent hallucinations of large language models. This paper shows that large neural nets are intrinsically unreliable, because it is not possible to make or validate a tractable theory of how a neural net works. There is no reliable way to extrapolate its performance from a limited number of test cases to an unlimited set of use cases. To have confidence in the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. World models have been proposed as a way to do this. This paper discusses the scope and architecture required of world models. World models are often conceived as models of the physical and natural world, using established theories of natural science, or learned regularities, to predict the physical consequences of AI actions. However, unforeseen consequences of AI actions impact the human social world as much as the physical world. To predict and control the consequences of AI, a world model needs to include a model of the human social world. I explore the challenges that this entails. Human language is based on a Common Ground of mutual understanding of the world, shared by the people conversing. The common ground is an overlapping subset of each persons world model, including their models of the physical, social and mental worlds. LLMs have no stable representation of a common ground. To be reliable, AI systems will need to represent a common ground with their users, including physical, mental and social domains.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17796",
    "publication_date": "2026-01-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459159,
    "created_at": "2026-02-01T10:40:59.159Z"
  },
  {
    "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
    "authors": [
      "Yutong Shen",
      "\n      \n      Hangxu Liu",
      "\n      \n      Kailin Pei",
      "\n      \n      Ruizhe Xia",
      "\n      \n      Tongtong Feng"
    ],
    "abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17507",
    "publication_date": "2026-01-24",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459160,
    "created_at": "2026-02-01T10:40:59.160Z"
  },
  {
    "title": "SkyReels-V3 Technique Report",
    "authors": [
      "Debang Li",
      "\n      \n      Zhengcong Fei",
      "\n      \n      Tuanhui Li",
      "\n      \n      Yikun Dou",
      "\n      \n      Zheng Chen",
      "\n      \n      Jiangping Yang",
      "\n      \n      Mingyuan Fan",
      "\n      \n      Jingtao Xu",
      "\n      \n      Jiahua Wang",
      "\n      \n      Baoxuan Gu",
      "\n      \n      Mingshan Chang",
      "\n      \n      Wenjing Cai",
      "\n      \n      Yuqiang Xie",
      "\n      \n      Binjie Mao",
      "\n      \n      Youqiang Zhang",
      "\n      \n      Nuo Pang",
      "\n      \n      Hao Zhang",
      "\n      \n      Yuzhe Jin",
      "\n      \n      Zhiheng Xu",
      "\n      \n      Dixuan Lin",
      "\n      \n      Guibin Chen",
      "\n      \n      Yahui Zhou"
    ],
    "abstract": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17323",
    "publication_date": "2026-01-28",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Transformers"
    ],
    "id": 1769942459161,
    "created_at": "2026-02-01T10:40:59.161Z"
  },
  {
    "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation",
    "authors": [
      "Junichiro Niimi"
    ],
    "abstract": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17094",
    "publication_date": "2026-01-23",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942459162,
    "created_at": "2026-02-01T10:40:59.162Z"
  },
  {
    "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
    "authors": [
      "Luozhou Wang",
      "\n      \n      Zhifei Chen",
      "\n      \n      Yihua Du",
      "\n      \n      Dongyu Yan",
      "\n      \n      Wenhang Ge",
      "\n      \n      Guibao Shen",
      "\n      \n      Xinli Xu",
      "\n      \n      Leyi Wu",
      "\n      \n      Man Chen",
      "\n      \n      Tianshuo Xu",
      "\n      \n      Peiran Ren",
      "\n      \n      Xin Tao",
      "\n      \n      Pengfei Wan",
      "\n      \n      Ying-Cong Chen"
    ],
    "abstract": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.17067",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942459163,
    "created_at": "2026-02-01T10:40:59.163Z"
  },
  {
    "title": "Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics",
    "authors": [
      "Pierrick Lorang"
    ],
    "abstract": "Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16985",
    "publication_date": "2026-01-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459164,
    "created_at": "2026-02-01T10:40:59.164Z"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "authors": [
      "Moo Jin Kim",
      "\n      \n      Yihuai Gao",
      "\n      \n      Tsung-Yi Lin",
      "\n      \n      Yen-Chen Lin",
      "\n      \n      Yunhao Ge",
      "\n      \n      Grace Lam",
      "\n      \n      Percy Liang",
      "\n      \n      Shuran Song",
      "\n      \n      Ming-Yu Liu",
      "\n      \n      Chelsea Finn",
      "\n      \n      Jinwei Gu"
    ],
    "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16163",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459165,
    "created_at": "2026-02-01T10:40:59.165Z"
  },
  {
    "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
    "authors": [
      "Chak-Wing Mak",
      "\n      \n      Guanyu Zhu",
      "\n      \n      Boyi Zhang",
      "\n      \n      Hongji Li",
      "\n      \n      Xiaowei Chi",
      "\n      \n      Kevin Zhang",
      "\n      \n      Yichen Wu",
      "\n      \n      Yangfan He",
      "\n      \n      Chun-Kai Fan",
      "\n      \n      Wentao Lu",
      "\n      \n      Kuangzhi Ge",
      "\n      \n      Xinyu Fang",
      "\n      \n      Hongyang He",
      "\n      \n      Kuan Lu",
      "\n      \n      Tianxiang Xu",
      "\n      \n      Li Zhang",
      "\n      \n      Yongxin Ni",
      "\n      \n      Youhua Li",
      "\n      \n      Shanghang Zhang"
    ],
    "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.16007",
    "publication_date": "2026-01-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942459167,
    "created_at": "2026-02-01T10:40:59.167Z"
  },
  {
    "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models",
    "authors": [
      "Zhikang Chen",
      "\n      \n      Tingting Zhu"
    ],
    "abstract": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15533",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning"
    ],
    "id": 1769942459168,
    "created_at": "2026-02-01T10:40:59.168Z"
  },
  {
    "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
    "authors": [
      "Anurag Bagchi",
      "\n      \n      Zhipeng Bao",
      "\n      \n      Homanga Bharadhwaj",
      "\n      \n      Yu-Xiong Wang",
      "\n      \n      Pavel Tokmakov",
      "\n      \n      Martial Hebert"
    ],
    "abstract": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15284",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942459169,
    "created_at": "2026-02-01T10:40:59.169Z"
  },
  {
    "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation",
    "authors": [
      "Ying Yang",
      "\n      \n      Zhengyao Lv",
      "\n      \n      Tianlin Pan",
      "\n      \n      Haofan Wang",
      "\n      \n      Binxin Yang",
      "\n      \n      Hubery Yin",
      "\n      \n      Chen Li",
      "\n      \n      Ziwei Liu",
      "\n      \n      Chenyang Si"
    ],
    "abstract": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.15281",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942459170,
    "created_at": "2026-02-01T10:40:59.170Z"
  },
  {
    "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning",
    "authors": [
      "Tony Chen",
      "\n      \n      Sam Cheyette",
      "\n      \n      Kelsey Allen",
      "\n      \n      Joshua Tenenbaum",
      "\n      \n      Kevin Smith"
    ],
    "abstract": "Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.14514",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942459171,
    "created_at": "2026-02-01T10:40:59.171Z"
  },
  {
    "title": "VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models",
    "authors": [
      "Yongchao Huang"
    ],
    "abstract": "Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \\textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \\emph{Variational JEPA (VJEPA)}, a \\textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \\emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.14354",
    "publication_date": "2026-01-20",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942459172,
    "created_at": "2026-02-01T10:40:59.172Z"
  },
  {
    "title": "CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning",
    "authors": [
      "Wenxin Ma",
      "\n      \n      Chenlong Wang",
      "\n      \n      Ruisheng Yuan",
      "\n      \n      Hao Chen",
      "\n      \n      Nanru Dai",
      "\n      \n      S. Kevin Zhou",
      "\n      \n      Yijun Yang",
      "\n      \n      Alan Yuille",
      "\n      \n      Jieneng Chen"
    ],
    "abstract": "Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer \"what-if\" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.13304",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942459173,
    "created_at": "2026-02-01T10:40:59.173Z"
  },
  {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "authors": [
      "Baochang Ren",
      "\n      \n      Yunzhi Yao",
      "\n      \n      Rui Sun",
      "\n      \n      Shuofei Qiao",
      "\n      \n      Ningyu Zhang",
      "\n      \n      Huajun Chen"
    ],
    "abstract": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.13247",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459174,
    "created_at": "2026-02-01T10:40:59.174Z"
  },
  {
    "title": "Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design",
    "authors": [
      "Kaleem Arshid",
      "\n      \n      Ali Krayani",
      "\n      \n      Lucio Marcenaro",
      "\n      \n      David Martin Gomez",
      "\n      \n      Carlo Regazzoni"
    ],
    "abstract": "This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12939",
    "publication_date": "2026-01-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459175,
    "created_at": "2026-02-01T10:40:59.175Z"
  },
  {
    "title": "Agentic Reasoning for Large Language Models",
    "authors": [
      "Tianxin Wei",
      "\n      \n      Ting-Wei Li",
      "\n      \n      Zhining Liu",
      "\n      \n      Xuying Ning",
      "\n      \n      Ze Yang",
      "\n      \n      Jiaru Zou",
      "\n      \n      Zhichen Zeng",
      "\n      \n      Ruizhong Qiu",
      "\n      \n      Xiao Lin",
      "\n      \n      Dongqi Fu",
      "\n      \n      Zihao Li",
      "\n      \n      Mengting Ai",
      "\n      \n      Duo Zhou",
      "\n      \n      Wenxuan Bao",
      "\n      \n      Yunzhe Li",
      "\n      \n      Gaotang Li",
      "\n      \n      Cheng Qian",
      "\n      \n      Yu Wang",
      "\n      \n      Xiangru Tang",
      "\n      \n      Yin Xiao",
      "\n      \n      Liri Fang",
      "\n      \n      Hui Liu",
      "\n      \n      Xianfeng Tang",
      "\n      \n      Yuji Zhang",
      "\n      \n      Chi Wang\n      ",
      "et al. (4 additional authors not shown)"
    ],
    "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12538",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459177,
    "created_at": "2026-02-01T10:40:59.177Z"
  },
  {
    "title": "ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models",
    "authors": [
      "Baorui Peng",
      "\n      \n      Wenyao Zhang",
      "\n      \n      Liang Xu",
      "\n      \n      Zekun Qi",
      "\n      \n      Jiazhao Zhang",
      "\n      \n      Hongsi Liu",
      "\n      \n      Wenjun Zeng",
      "\n      \n      Xin Jin"
    ],
    "abstract": "Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12428",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459178,
    "created_at": "2026-02-01T10:40:59.178Z"
  },
  {
    "title": "An Efficient and Multi-Modal Navigation System with One-Step World Model",
    "authors": [
      "Wangtian Shen",
      "\n      \n      Ziyang Meng",
      "\n      \n      Jinming Ma",
      "\n      \n      Mingliang Zhou",
      "\n      \n      Diyun Xiang"
    ],
    "abstract": "Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.12277",
    "publication_date": "2026-01-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning",
      "Transformers"
    ],
    "id": 1769942459179,
    "created_at": "2026-02-01T10:40:59.179Z"
  },
  {
    "title": "Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning",
    "authors": [
      "Rajat Ghosh",
      "\n      \n      Debojyoti Dutta"
    ],
    "abstract": "Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10905",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459180,
    "created_at": "2026-02-01T10:40:59.180Z"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "authors": [
      "Delong Chen",
      "\n      \n      Tejaswi Kasarla",
      "\n      \n      Yejin Bang",
      "\n      \n      Mustafa Shukor",
      "\n      \n      Willy Chung",
      "\n      \n      Jade Yu",
      "\n      \n      Allen Bolourchi",
      "\n      \n      Theo Moutakanni",
      "\n      \n      Pascale Fung"
    ],
    "abstract": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10592",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Representation Learning",
      "Transformers"
    ],
    "id": 1769942459182,
    "created_at": "2026-02-01T10:40:59.182Z"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "authors": [
      "Jianhao Yuan",
      "\n      \n      Xiaofeng Zhang",
      "\n      \n      Felix Friedrich",
      "\n      \n      Nicolas Beltran-Velez",
      "\n      \n      Melissa Hall",
      "\n      \n      Reyhane Askari-Hemmat",
      "\n      \n      Xiaochuang Han",
      "\n      \n      Nicolas Ballas",
      "\n      \n      Michal Drozdzal",
      "\n      \n      Adriana Romero-Soriano"
    ],
    "abstract": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.10553",
    "publication_date": "2026-01-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Representation Learning",
      "Diffusion Models"
    ],
    "id": 1769942459183,
    "created_at": "2026-02-01T10:40:59.183Z"
  },
  {
    "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
    "authors": [
      "Ahmad Rahimi",
      "\n      \n      Valentin Gerard",
      "\n      \n      Eloi Zablocki",
      "\n      \n      Matthieu Cord",
      "\n      \n      Alexandre Alahi"
    ],
    "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.09452",
    "publication_date": "2026-01-14",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942459184,
    "created_at": "2026-02-01T10:40:59.184Z"
  },
  {
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "authors": [
      "Youwei Liu",
      "\n      \n      Jian Wang",
      "\n      \n      Hanlin Wang",
      "\n      \n      Beichen Guo",
      "\n      \n      Wenjie Li"
    ],
    "abstract": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.08955",
    "publication_date": "2026-01-13",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942459185,
    "created_at": "2026-02-01T10:40:59.185Z"
  },
  {
    "title": "Creativity in AI as Emergence from Domain-Limited Generative Models",
    "authors": [
      "Corina Chutaux"
    ],
    "abstract": "Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.08388",
    "publication_date": "2026-01-13",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942459186,
    "created_at": "2026-02-01T10:40:59.186Z"
  },
  {
    "title": "Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling",
    "authors": [
      "Alexander Boldachev"
    ],
    "abstract": "This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07964",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459188,
    "created_at": "2026-02-01T10:40:59.188Z"
  },
  {
    "title": "Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions",
    "authors": [
      "Zhiting Mei",
      "\n      \n      Tenny Yin",
      "\n      \n      Ola Shorinwa",
      "\n      \n      Apurva Badithela",
      "\n      \n      Zhonghe Zheng",
      "\n      \n      Joseph Bruno",
      "\n      \n      Madison Bland",
      "\n      \n      Lihan Zha",
      "\n      \n      Asher Hancock",
      "\n      \n      Jaime Fernández Fisac",
      "\n      \n      Philip Dames",
      "\n      \n      Anirudha Majumdar"
    ],
    "abstract": "Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07823",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942459190,
    "created_at": "2026-02-01T10:40:59.190Z"
  },
  {
    "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
    "authors": [
      "Huanyu Li",
      "\n      \n      Kun Lei",
      "\n      \n      Sheng Zang",
      "\n      \n      Kaizhe Hu",
      "\n      \n      Yongyuan Liang",
      "\n      \n      Bo An",
      "\n      \n      Xiaoli Li",
      "\n      \n      Huazhe Xu"
    ],
    "abstract": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07821",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942459191,
    "created_at": "2026-02-01T10:40:59.191Z"
  },
  {
    "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning",
    "authors": [
      "Sijia li",
      "\n      \n      Xinran Li",
      "\n      \n      Shibo Chen",
      "\n      \n      Jun Zhang"
    ],
    "abstract": "Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2601.07463",
    "publication_date": "2026-01-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942459192,
    "created_at": "2026-02-01T10:40:59.192Z"
  },
  {
    "title": "Active inference and artificial reasoning",
    "authors": [
      "Karl Friston",
      "\n      \n      Lancelot Da Costa",
      "\n      \n      Alexander Tschantz",
      "\n      \n      Conor Heins",
      "\n      \n      Christopher Buckley",
      "\n      \n      Tim Verbelen",
      "\n      \n      Thomas Parr"
    ],
    "abstract": "This technical note considers the sampling of outcomes that provide the greatest amount of information about the structure of underlying world models. This generalisation furnishes a principled approach to structure learning under a plausible set of generative models or hypotheses. In active inference, policies - i.e., combinations of actions - are selected based on their expected free energy, which comprises expected information gain and value. Information gain corresponds to the KL divergence between predictive posteriors with, and without, the consequences of action. Posteriors over models can be evaluated quickly and efficiently using Bayesian Model Reduction, based upon accumulated posterior beliefs about model parameters. The ensuing information gain can then be used to select actions that disambiguate among alternative models, in the spirit of optimal experimental design. We illustrate this kind of active selection or reasoning using partially observed discrete models; namely, a 'three-ball' paradigm used previously to describe artificial insight and 'aha moments' via (synthetic) introspection or sleep. We focus on the sample efficiency afforded by seeking outcomes that resolve the greatest uncertainty about the world model, under which outcomes are generated.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.21129",
    "publication_date": "2025-12-24",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942461486,
    "created_at": "2026-02-01T10:41:01.486Z"
  },
  {
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "authors": [
      "Saeed Mohammadzadeh",
      "\n      \n      Erfan Hamdi",
      "\n      \n      Joel Shor",
      "\n      \n      Emma Lejeune"
    ],
    "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20732",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942461488,
    "created_at": "2026-02-01T10:41:01.488Z"
  },
  {
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "authors": [
      "Xuanhua He",
      "\n      \n      Tianyu Yang",
      "\n      \n      Ke Cao",
      "\n      \n      Ruiqi Wu",
      "\n      \n      Cheng Meng",
      "\n      \n      Yong Zhang",
      "\n      \n      Zhuoliang Kang",
      "\n      \n      Xiaoming Wei",
      "\n      \n      Qifeng Chen"
    ],
    "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20615",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942461490,
    "created_at": "2026-02-01T10:41:01.490Z"
  },
  {
    "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
    "authors": [
      "Zepeng Xin",
      "\n      \n      Kaiyu Li",
      "\n      \n      Luodi Chen",
      "\n      \n      Wanchen Li",
      "\n      \n      Yuchen Xiao",
      "\n      \n      Hui Qiao",
      "\n      \n      Weizhan Zhang",
      "\n      \n      Deyu Meng",
      "\n      \n      Xiangyong Cao"
    ],
    "abstract": "Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.20013",
    "publication_date": "2025-12-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461492,
    "created_at": "2026-02-01T10:41:01.492Z"
  },
  {
    "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
    "authors": [
      "Pengxuan Yang",
      "\n      \n      Ben Lu",
      "\n      \n      Zhongpu Xia",
      "\n      \n      Chao Han",
      "\n      \n      Yinfeng Gao",
      "\n      \n      Teng Zhang",
      "\n      \n      Kun Zhan",
      "\n      \n      XianPeng Lang",
      "\n      \n      Yupeng Zheng",
      "\n      \n      Qichao Zhang"
    ],
    "abstract": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.19133",
    "publication_date": "2025-12-22",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942461493,
    "created_at": "2026-02-01T10:41:01.493Z"
  },
  {
    "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
    "authors": [
      "Utae Jeong",
      "\n      \n      Sumin In",
      "\n      \n      Hyunju Ryu",
      "\n      \n      Jaewan Choi",
      "\n      \n      Feng Yang",
      "\n      \n      Jongheon Jeong",
      "\n      \n      Seungryong Kim",
      "\n      \n      Sangpil Kim"
    ],
    "abstract": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.19048",
    "publication_date": "2025-12-22",
    "tags": [
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942461495,
    "created_at": "2026-02-01T10:41:01.495Z"
  },
  {
    "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement",
    "authors": [
      "Feeza Khan Khanzada",
      "\n      \n      Jaerock Kwon"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18850",
    "publication_date": "2025-12-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461496,
    "created_at": "2026-02-01T10:41:01.496Z"
  },
  {
    "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "authors": [
      "Yixia Li",
      "\n      \n      Hongru Wang",
      "\n      \n      Jiahao Qiu",
      "\n      \n      Zhenfei Yin",
      "\n      \n      Dongdong Zhang",
      "\n      \n      Cheng Qian",
      "\n      \n      Zeping Li",
      "\n      \n      Pony Ma",
      "\n      \n      Guanhua Chen",
      "\n      \n      Heng Ji",
      "\n      \n      Mengdi Wang"
    ],
    "abstract": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18832",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461498,
    "created_at": "2026-02-01T10:41:01.498Z"
  },
  {
    "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
    "authors": [
      "Tianrui Zhu",
      "\n      \n      Shiyi Zhang",
      "\n      \n      Zhirui Sun",
      "\n      \n      Jingqi Tian",
      "\n      \n      Yansong Tang"
    ],
    "abstract": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18741",
    "publication_date": "2025-12-23",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Diffusion Models"
    ],
    "id": 1769942461499,
    "created_at": "2026-02-01T10:41:01.499Z"
  },
  {
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "authors": [
      "Pierre Colombo",
      "\n      \n      Malik Boudiaf",
      "\n      \n      Allyn Sweet",
      "\n      \n      Michael Desa",
      "\n      \n      Hongxi Wang",
      "\n      \n      Kevin Candra",
      "\n      \n      Syméon del Marmol"
    ],
    "abstract": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18658",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461500,
    "created_at": "2026-02-01T10:41:01.500Z"
  },
  {
    "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
    "authors": [
      "Zhenhao Zhou",
      "\n      \n      Dan Negrut"
    ],
    "abstract": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18619",
    "publication_date": "2025-12-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Transformers"
    ],
    "id": 1769942461502,
    "created_at": "2026-02-01T10:41:01.502Z"
  },
  {
    "title": "Large Language Models as Discounted Bayesian Filters",
    "authors": [
      "Jensen Zhang",
      "\n      \n      Jing Yang",
      "\n      \n      Keze Wang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18489",
    "publication_date": "2025-12-20",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461503,
    "created_at": "2026-02-01T10:41:01.503Z"
  },
  {
    "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
    "authors": [
      "Wenjun Lin",
      "\n      \n      Jensen Zhang",
      "\n      \n      Kaitong Cai",
      "\n      \n      Keze Wang"
    ],
    "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18477",
    "publication_date": "2025-12-20",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning"
    ],
    "id": 1769942461505,
    "created_at": "2026-02-01T10:41:01.505Z"
  },
  {
    "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
    "authors": [
      "Yulu Wu",
      "\n      \n      Jiujun Cheng",
      "\n      \n      Haowen Wang",
      "\n      \n      Dengyang Suo",
      "\n      \n      Pei Ren",
      "\n      \n      Qichao Mao",
      "\n      \n      Shangce Gao",
      "\n      \n      Yakun Huang"
    ],
    "abstract": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.18396",
    "publication_date": "2025-12-20",
    "tags": [
      "Robotics"
    ],
    "id": 1769942461506,
    "created_at": "2026-02-01T10:41:01.506Z"
  },
  {
    "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
    "authors": [
      "Qianwei Wang",
      "\n      \n      Bowen Li",
      "\n      \n      Zhanpeng Luo",
      "\n      \n      Yifan Xu",
      "\n      \n      Alexander Gray",
      "\n      \n      Tom Silver",
      "\n      \n      Sebastian Scherer",
      "\n      \n      Katia Sycara",
      "\n      \n      Yaqi Xie"
    ],
    "abstract": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17992",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461508,
    "created_at": "2026-02-01T10:41:01.508Z"
  },
  {
    "title": "Dexterous World Models",
    "authors": [
      "Byungjun Kim",
      "\n      \n      Taeksoo Kim",
      "\n      \n      Junyoung Lee",
      "\n      \n      Hanbyul Joo"
    ],
    "abstract": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17907",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942461510,
    "created_at": "2026-02-01T10:41:01.510Z"
  },
  {
    "title": "Animate Any Character in Any World",
    "authors": [
      "Yitong Wang",
      "\n      \n      Fangyun Wei",
      "\n      \n      Hongyang Zhang",
      "\n      \n      Bo Dai",
      "\n      \n      Yan Lu"
    ],
    "abstract": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17796",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942461511,
    "created_at": "2026-02-01T10:41:01.511Z"
  },
  {
    "title": "Investigating methods to solve large windfarm optimization problems with a minimum number of qubits using circuit-based quantum computers",
    "authors": [
      "James Hancock",
      "\n      \n      Matthew Craven",
      "\n      \n      Craig McNeile"
    ],
    "abstract": "This study investigates quantum computing approaches for solving the windfarm layout optimization (WFLO) problems formulated as a quadratic unconstrained binary optimization (QUBO) problem. We investigate two encoding methods that require fewer than one qubit per grid point: the previously developed Pauli correlation encoding (PCE) and a novel single-qubit operator encoding (SQOE). These methods are tested on three windfarm configurations - two from prior WFLO scaling studies and a new real-world model based on an existing windfarm in Wales. The improved encoding methods allow us to solve WFLO problems on $9\\times 9$ grids using up to 20 qubits on a quantum computer simulator. The results show that both encoding methods perform competitively and demonstrate favorable scaling characteristics across the tested systems.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17582",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461513,
    "created_at": "2026-02-01T10:41:01.513Z"
  },
  {
    "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction",
    "authors": [
      "Ziyang Lin",
      "\n      \n      Zixuan Sun",
      "\n      \n      Sanhorn Chen",
      "\n      \n      Xiaoyang Chen",
      "\n      \n      Roy Zhao"
    ],
    "abstract": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17250",
    "publication_date": "2025-12-19",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Transformers"
    ],
    "id": 1769942461514,
    "created_at": "2026-02-01T10:41:01.514Z"
  },
  {
    "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
    "authors": [
      "Nan Zhou",
      "\n      \n      Huandong Wang",
      "\n      \n      Jiahao Li",
      "\n      \n      Yang Li",
      "\n      \n      Xiao-Ping Zhang",
      "\n      \n      Yong Li",
      "\n      \n      Xinlei Chen"
    ],
    "abstract": "Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.17152",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942461516,
    "created_at": "2026-02-01T10:41:01.516Z"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": [
      "Hanlin Wang",
      "\n      \n      Hao Ouyang",
      "\n      \n      Qiuyu Wang",
      "\n      \n      Yue Yu",
      "\n      \n      Yihao Meng",
      "\n      \n      Wen Wang",
      "\n      \n      Ka Leong Cheng",
      "\n      \n      Shuailei Ma",
      "\n      \n      Qingyan Bai",
      "\n      \n      Yixuan Li",
      "\n      \n      Cheng Chen",
      "\n      \n      Yanhong Zeng",
      "\n      \n      Xing Zhu",
      "\n      \n      Yujun Shen",
      "\n      \n      Qifeng Chen"
    ],
    "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.16924",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461518,
    "created_at": "2026-02-01T10:41:01.518Z"
  },
  {
    "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
    "authors": [
      "Tin Stribor Sohn",
      "\n      \n      Maximilian Dillitzer",
      "\n      \n      Jason J. Corso",
      "\n      \n      Eric Sax"
    ],
    "abstract": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.16461",
    "publication_date": "2025-12-18",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461520,
    "created_at": "2026-02-01T10:41:01.520Z"
  },
  {
    "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
    "authors": [
      "Dimitrios Danopoulos",
      "\n      \n      Enrico Lupi",
      "\n      \n      Chang Sun",
      "\n      \n      Sebastian Dittmeier",
      "\n      \n      Michael Kagan",
      "\n      \n      Vladimir Loncar",
      "\n      \n      Maurizio Pierini"
    ],
    "abstract": "Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15946",
    "publication_date": "2026-01-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461522,
    "created_at": "2026-02-01T10:41:01.522Z"
  },
  {
    "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
    "authors": [
      "Tin Stribor Sohn",
      "\n      \n      Maximilian Dillitzer",
      "\n      \n      Jason J. Corso",
      "\n      \n      Eric Sax"
    ],
    "abstract": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15940",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461523,
    "created_at": "2026-02-01T10:41:01.523Z"
  },
  {
    "title": "OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence",
    "authors": [
      "Yu Zheng",
      "\n      \n      Jie Hu",
      "\n      \n      Kailun Yang",
      "\n      \n      Jiaming Zhang"
    ],
    "abstract": "Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: \"what would happen given a specific future action\". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15621",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461525,
    "created_at": "2026-02-01T10:41:01.525Z"
  },
  {
    "title": "Soft Geometric Inductive Bias for Object Centric Dynamics",
    "authors": [
      "Hampus Linander",
      "\n      \n      Conor Heins",
      "\n      \n      Alexander Tschantz",
      "\n      \n      Marco Perin",
      "\n      \n      Christopher Buckley"
    ],
    "abstract": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.15493",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction"
    ],
    "id": 1769942461527,
    "created_at": "2026-02-01T10:41:01.527Z"
  },
  {
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": [
      "Zefan Cai",
      "\n      \n      Haoyi Qiu",
      "\n      \n      Tianyi Ma",
      "\n      \n      Haozhe Zhao",
      "\n      \n      Gengze Zhou",
      "\n      \n      Kung-Hsiang Huang",
      "\n      \n      Parisa Kordjamshidi",
      "\n      \n      Minjia Zhang",
      "\n      \n      Wen Xiao",
      "\n      \n      Jiuxiang Gu",
      "\n      \n      Nanyun Peng",
      "\n      \n      Junjie Hu"
    ],
    "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14691",
    "publication_date": "2025-12-17",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Planning",
      "Transformers"
    ],
    "id": 1769942461529,
    "created_at": "2026-02-01T10:41:01.529Z"
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "authors": [
      "Wenqiang Sun",
      "\n      \n      Haiyu Zhang",
      "\n      \n      Haoyuan Wang",
      "\n      \n      Junta Wu",
      "\n      \n      Zehan Wang",
      "\n      \n      Zhenwei Wang",
      "\n      \n      Yunhong Wang",
      "\n      \n      Jun Zhang",
      "\n      \n      Tengfei Wang",
      "\n      \n      Chunchao Guo"
    ],
    "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14614",
    "publication_date": "2025-12-16",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942461531,
    "created_at": "2026-02-01T10:41:01.531Z"
  },
  {
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "authors": [
      "Shufan Li",
      "\n      \n      Konstantinos Kallidromitis",
      "\n      \n      Akash Gokul",
      "\n      \n      Yusuke Kato",
      "\n      \n      Kazuki Kozuka",
      "\n      \n      Aditya Grover"
    ],
    "abstract": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.14014",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942461533,
    "created_at": "2026-02-01T10:41:01.533Z"
  },
  {
    "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces",
    "authors": [
      "Subramanyam Sahoo",
      "\n      \n      Jared Junkin"
    ],
    "abstract": "Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13821",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461535,
    "created_at": "2026-02-01T10:41:01.535Z"
  },
  {
    "title": "Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation",
    "authors": [
      "Daoyuan Qian",
      "\n      \n      Qiyao Liang",
      "\n      \n      Ila Fiete"
    ],
    "abstract": "Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13707",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461537,
    "created_at": "2026-02-01T10:41:01.537Z"
  },
  {
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "authors": [
      "Raktim Gautam Goswami",
      "\n      \n      Amir Bar",
      "\n      \n      David Fan",
      "\n      \n      Tsung-Yen Yang",
      "\n      \n      Gaoyue Zhou",
      "\n      \n      Prashanth Krishnamurthy",
      "\n      \n      Michael Rabbat",
      "\n      \n      Farshad Khorrami",
      "\n      \n      Yann LeCun"
    ],
    "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13644",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1769942461539,
    "created_at": "2026-02-01T10:41:01.539Z"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "authors": [
      "Jianxiong Gao",
      "\n      \n      Zhaoxi Chen",
      "\n      \n      Xian Liu",
      "\n      \n      Junhao Zhuang",
      "\n      \n      Chengming Xu",
      "\n      \n      Jianfeng Feng",
      "\n      \n      Yu Qiao",
      "\n      \n      Yanwei Fu",
      "\n      \n      Chenyang Si",
      "\n      \n      Ziwei Liu"
    ],
    "abstract": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13604",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics"
    ],
    "id": 1769942461541,
    "created_at": "2026-02-01T10:41:01.541Z"
  },
  {
    "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
    "authors": [
      "Raymond Khazoum",
      "\n      \n      Daniela Fernandes",
      "\n      \n      Aleksandr Krylov",
      "\n      \n      Qin Li",
      "\n      \n      Stephane Deny"
    ],
    "abstract": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13517",
    "publication_date": "2025-12-15",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "id": 1769942461543,
    "created_at": "2026-02-01T10:41:01.543Z"
  },
  {
    "title": "Motus: A Unified Latent Action World Model",
    "authors": [
      "Hongzhe Bi",
      "\n      \n      Hengkai Tan",
      "\n      \n      Shenghao Xie",
      "\n      \n      Zeyuan Wang",
      "\n      \n      Shuhe Huang",
      "\n      \n      Haitian Liu",
      "\n      \n      Ruowen Zhao",
      "\n      \n      Yao Feng",
      "\n      \n      Chendong Xiang",
      "\n      \n      Yinze Rong",
      "\n      \n      Hongyan Zhao",
      "\n      \n      Hanyu Liu",
      "\n      \n      Zhizhong Su",
      "\n      \n      Lei Ma",
      "\n      \n      Hang Su",
      "\n      \n      Jun Zhu"
    ],
    "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.13030",
    "publication_date": "2025-12-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942461545,
    "created_at": "2026-02-01T10:41:01.545Z"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "authors": [
      "Zhenya Yang",
      "\n      \n      Zhe Liu",
      "\n      \n      Yuxiang Lu",
      "\n      \n      Liping Hou",
      "\n      \n      Chenxuan Miao",
      "\n      \n      Siyi Peng",
      "\n      \n      Bailan Feng",
      "\n      \n      Xiang Bai",
      "\n      \n      Hengshuang Zhao"
    ],
    "abstract": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12751",
    "publication_date": "2025-12-14",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Diffusion Models"
    ],
    "id": 1769942461547,
    "created_at": "2026-02-01T10:41:01.547Z"
  },
  {
    "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents",
    "authors": [
      "Yesid Fonseca",
      "\n      \n      Manuel S. Ríos",
      "\n      \n      Nicanor Quijano",
      "\n      \n      Luis F. Giraldo"
    ],
    "abstract": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12548",
    "publication_date": "2025-12-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461549,
    "created_at": "2026-02-01T10:41:01.549Z"
  },
  {
    "title": "From Human Intention to Action Prediction: Intention-Driven End-to-End Autonomous Driving",
    "authors": [
      "Huan Zheng",
      "\n      \n      Yucheng Zhou",
      "\n      \n      Tianyi Yan",
      "\n      \n      Jiayi Su",
      "\n      \n      Hongjun Chen",
      "\n      \n      Dubing Chen",
      "\n      \n      Xingtai Gui",
      "\n      \n      Wencheng Han",
      "\n      \n      Runzhou Tao",
      "\n      \n      Zhongying Qiu",
      "\n      \n      Jianfei Yang",
      "\n      \n      Jianbing Shen"
    ],
    "abstract": "While end-to-end autonomous driving has achieved remarkable progress in geometric control, current systems remain constrained by a command-following paradigm that relies on simple navigational instructions. Transitioning to genuinely intelligent agents requires the capability to interpret and fulfill high-level, abstract human intentions. However, this advancement is hindered by the lack of dedicated benchmarks and semantic-aware evaluation metrics. In this paper, we formally define the task of Intention-Driven End-to-End Autonomous Driving and present Intention-Drive, a comprehensive benchmark designed to bridge this gap. We construct a large-scale dataset featuring complex natural language intentions paired with high-fidelity sensor data. To overcome the limitations of conventional trajectory-based metrics, we introduce the Imagined Future Alignment (IFA), a novel evaluation protocol leveraging generative world models to assess the semantic fulfillment of human goals beyond mere geometric accuracy. Furthermore, we explore the solution space by proposing two distinct paradigms: an end-to-end vision-language planner and a hierarchical agent-based framework. The experiments reveal a critical dichotomy where existing models exhibit satisfactory driving stability but struggle significantly with intention fulfillment. Notably, the proposed frameworks demonstrate superior alignment with human intentions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12302",
    "publication_date": "2026-01-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461551,
    "created_at": "2026-02-01T10:41:01.551Z"
  },
  {
    "title": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes",
    "authors": [
      "Mohammad Pivezhandi",
      "\n      \n      Mahdi Banisharif",
      "\n      \n      Saeed Bakhshan",
      "\n      \n      Abusayeed Saifullah",
      "\n      \n      Ali Jannesari"
    ],
    "abstract": "Autonomous AI agents on embedded platforms require real-time, risk-aware scheduling under resource and thermal constraints. Classical heuristics struggle with workload irregularity, tabular regressors discard structural information, and model-free reinforcement learning (RL) risks overheating. We introduce GraphPerf-RT, a graph neural network surrogate achieving deep learning accuracy at heuristic speeds (2-7ms). GraphPerf-RT is, to our knowledge, the first to unify task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph with typed edges encoding precedence, placement, and contention. Evidential regression with Normal-Inverse-Gamma priors provides calibrated uncertainty; we validate on makespan prediction for risk-aware scheduling. Experiments on three ARM platforms (Jetson TX2, Orin NX, RUBIK Pi) achieve R^2 = 0.81 on log-transformed makespan with Spearman rho = 0.95 and conservative uncertainty calibration (PICP = 99.9% at 95% confidence). Integration with four RL methods demonstrates that multi-agent model-based RL with GraphPerf-RT as the world model achieves 66% makespan reduction and 82% energy reduction versus model-free baselines, with zero thermal violations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12091",
    "publication_date": "2026-01-21",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461553,
    "created_at": "2026-02-01T10:41:01.553Z"
  },
  {
    "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models",
    "authors": [
      "Ryan Po",
      "\n      \n      Eric Ryan Chan",
      "\n      \n      Changan Chen",
      "\n      \n      Gordon Wetzstein"
    ],
    "abstract": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.12080",
    "publication_date": "2025-12-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Representation Learning",
      "Transformers",
      "Diffusion Models"
    ],
    "id": 1769942461555,
    "created_at": "2026-02-01T10:41:01.555Z"
  },
  {
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "authors": [
      "Junjie Ye",
      "\n      \n      Rong Xue",
      "\n      \n      Basile Van Hoorick",
      "\n      \n      Pavel Tokmakov",
      "\n      \n      Muhammad Zubair Irshad",
      "\n      \n      Yue Wang",
      "\n      \n      Vitor Guizilini"
    ],
    "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11797",
    "publication_date": "2025-12-12",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Diffusion Models"
    ],
    "id": 1769942461557,
    "created_at": "2026-02-01T10:41:01.557Z"
  },
  {
    "title": "FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model",
    "authors": [
      "Hongbin Lin",
      "\n      \n      Yiming Yang",
      "\n      \n      Yifan Zhang",
      "\n      \n      Chaoda Zheng",
      "\n      \n      Jie Feng",
      "\n      \n      Sheng Wang",
      "\n      \n      Zhennan Wang",
      "\n      \n      Shijia Chen",
      "\n      \n      Boyang Wang",
      "\n      \n      Yu Zhang",
      "\n      \n      Xianming Liu",
      "\n      \n      Shuguang Cui",
      "\n      \n      Zhen Li"
    ],
    "abstract": "In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11226",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942461559,
    "created_at": "2026-02-01T10:41:01.559Z"
  },
  {
    "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
    "authors": [
      "Gabrijel Boduljak",
      "\n      \n      Yushi Lan",
      "\n      \n      Christian Rupprecht",
      "\n      \n      Andrea Vedaldi"
    ],
    "abstract": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11225",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Representation Learning"
    ],
    "id": 1769942461561,
    "created_at": "2026-02-01T10:41:01.561Z"
  },
  {
    "title": "VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation",
    "authors": [
      "Felix O'Mahony",
      "\n      \n      Roberto Cipolla",
      "\n      \n      Ayush Tewari"
    ],
    "abstract": "Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.11061",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942461564,
    "created_at": "2026-02-01T10:41:01.564Z"
  },
  {
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "authors": [
      "Ao Liang",
      "\n      \n      Lingdong Kong",
      "\n      \n      Tianyi Yan",
      "\n      \n      Hongsi Liu",
      "\n      \n      Wesley Yang",
      "\n      \n      Ziqi Huang",
      "\n      \n      Wei Yin",
      "\n      \n      Jialong Zuo",
      "\n      \n      Yixuan Hu",
      "\n      \n      Dekai Zhu",
      "\n      \n      Dongyue Lu",
      "\n      \n      Youquan Liu",
      "\n      \n      Guangfeng Jiang",
      "\n      \n      Linfeng Li",
      "\n      \n      Xiangtai Li",
      "\n      \n      Long Zhuo",
      "\n      \n      Lai Xing Ng",
      "\n      \n      Benoit R. Cottereau",
      "\n      \n      Changxin Gao",
      "\n      \n      Liang Pan",
      "\n      \n      Wei Tsang Ooi",
      "\n      \n      Ziwei Liu"
    ],
    "abstract": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10958",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461566,
    "created_at": "2026-02-01T10:41:01.566Z"
  },
  {
    "title": "Generalized Spherical Neural Operators: Green's Function Formulation",
    "authors": [
      "Hao Tang",
      "\n      \n      Hao Chen",
      "\n      \n      Chao Li"
    ],
    "abstract": "Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a generalized operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to non-equivariant systems while retaining spectral efficiency and grid invariance. To exploit GSNO, we develop SHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and SHNet consistently outperform state-of-the-art methods. The theoretical and experimental results position GSNO as a principled and generalized framework for spherical operator design and learning, bridging rigorous theory with real-world complexity.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10723",
    "publication_date": "2026-01-26",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942461568,
    "created_at": "2026-02-01T10:41:01.568Z"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "authors": [
      "Gemini Robotics Team",
      "\n      \n      Krzysztof Choromanski",
      "\n      \n      Coline Devin",
      "\n      \n      Yilun Du",
      "\n      \n      Debidatta Dwibedi",
      "\n      \n      Ruiqi Gao",
      "\n      \n      Abhishek Jindal",
      "\n      \n      Thomas Kipf",
      "\n      \n      Sean Kirmani",
      "\n      \n      Isabel Leal",
      "\n      \n      Fangchen Liu",
      "\n      \n      Anirudha Majumdar",
      "\n      \n      Andrew Marmon",
      "\n      \n      Carolina Parada",
      "\n      \n      Yulia Rubanova",
      "\n      \n      Dhruv Shah",
      "\n      \n      Vikas Sindhwani",
      "\n      \n      Jie Tan",
      "\n      \n      Fei Xia",
      "\n      \n      Ted Xiao",
      "\n      \n      Sherry Yang",
      "\n      \n      Wenhao Yu",
      "\n      \n      Allan Zhou"
    ],
    "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10675",
    "publication_date": "2026-01-06",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942461570,
    "created_at": "2026-02-01T10:41:01.570Z"
  },
  {
    "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
    "authors": [
      "Shuhan Tan",
      "\n      \n      Kashyap Chitta",
      "\n      \n      Yuxiao Chen",
      "\n      \n      Ran Tian",
      "\n      \n      Yurong You",
      "\n      \n      Yan Wang",
      "\n      \n      Wenjie Luo",
      "\n      \n      Yulong Cao",
      "\n      \n      Philipp Krahenbuhl",
      "\n      \n      Marco Pavone",
      "\n      \n      Boris Ivanovic"
    ],
    "abstract": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10226",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "id": 1769942461572,
    "created_at": "2026-02-01T10:41:01.572Z"
  },
  {
    "title": "Latent Action World Models for Control with Unlabeled Trajectories",
    "authors": [
      "Marvin Alles",
      "\n      \n      Xingyuan Zhang",
      "\n      \n      Patrick van der Smagt",
      "\n      \n      Philip Becker-Ehmck"
    ],
    "abstract": "Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.10016",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction",
      "Robotics",
      "Representation Learning"
    ],
    "id": 1769942461574,
    "created_at": "2026-02-01T10:41:01.574Z"
  },
  {
    "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
    "authors": [
      "Arjun Parthasarathy",
      "\n      \n      Nimit Kalra",
      "\n      \n      Rohun Agrawal",
      "\n      \n      Yann LeCun",
      "\n      \n      Oumayma Bounou",
      "\n      \n      Pavel Izmailov",
      "\n      \n      Micah Goldblum"
    ],
    "abstract": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.09929",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942461576,
    "created_at": "2026-02-01T10:41:01.576Z"
  },
  {
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "authors": [
      "Hao Lu",
      "\n      \n      Ziyang Liu",
      "\n      \n      Guangfeng Jiang",
      "\n      \n      Yuanfei Luo",
      "\n      \n      Sheng Chen",
      "\n      \n      Yangang Zhang",
      "\n      \n      Ying-Cong Chen"
    ],
    "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.09864",
    "publication_date": "2025-12-10",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Planning"
    ],
    "id": 1769942482526,
    "created_at": "2026-02-01T10:41:22.526Z"
  },
  {
    "title": "Deterministic World Models for Verification of Closed-loop Vision-based Systems",
    "authors": [
      "Yuang Geng",
      "\n      \n      Zhuoyang Zhou",
      "\n      \n      Zhongzheng Zhang",
      "\n      \n      Siyuan Pan",
      "\n      \n      Hoang-Dung Tran",
      "\n      \n      Ivan Ruchkin"
    ],
    "abstract": "Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08991",
    "publication_date": "2025-12-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1769942482529,
    "created_at": "2026-02-01T10:41:22.529Z"
  },
  {
    "title": "Astra: General Interactive World Model with Autoregressive Denoising",
    "authors": [
      "Yixuan Zhu",
      "\n      \n      Jiaqi Feng",
      "\n      \n      Wenzhao Zheng",
      "\n      \n      Yuan Gao",
      "\n      \n      Xin Tao",
      "\n      \n      Pengfei Wan",
      "\n      \n      Jie Zhou",
      "\n      \n      Jiwen Lu"
    ],
    "abstract": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08931",
    "publication_date": "2026-01-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers",
      "Diffusion Models"
    ],
    "id": 1769942482531,
    "created_at": "2026-02-01T10:41:22.531Z"
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "authors": [
      "Yuning Gong",
      "\n      \n      Yifei Liu",
      "\n      \n      Yifan Zhan",
      "\n      \n      Muyao Niu",
      "\n      \n      Xueying Li",
      "\n      \n      Yuanjun Liao",
      "\n      \n      Jiaming Chen",
      "\n      \n      Yuanyuan Gao",
      "\n      \n      Jiaqi Chen",
      "\n      \n      Minming Chen",
      "\n      \n      Li Zhou",
      "\n      \n      Yuning Zhang",
      "\n      \n      Wei Wang",
      "\n      \n      Xiaoqing Hou",
      "\n      \n      Huaxi Huang",
      "\n      \n      Shixiang Tang",
      "\n      \n      Le Ma",
      "\n      \n      Dingwen Zhang",
      "\n      \n      Xue Yang",
      "\n      \n      Junchi Yan",
      "\n      \n      Yanchi Zhang",
      "\n      \n      Yinqiang Zheng",
      "\n      \n      Xiao Sun",
      "\n      \n      Zhihang Zhong"
    ],
    "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08478",
    "publication_date": "2025-12-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942482534,
    "created_at": "2026-02-01T10:41:22.534Z"
  },
  {
    "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems",
    "authors": [
      "Mingwei Li",
      "\n      \n      Xiaoyuan Zhang",
      "\n      \n      Chengwei Yang",
      "\n      \n      Zilong Zheng",
      "\n      \n      Yaodong Yang"
    ],
    "abstract": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08411",
    "publication_date": "2025-12-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942482536,
    "created_at": "2026-02-01T10:41:22.536Z"
  },
  {
    "title": "Learning Robot Manipulation from Audio World Models",
    "authors": [
      "Fan Zhang",
      "\n      \n      Michael Gienger"
    ],
    "abstract": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08405",
    "publication_date": "2025-12-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482538,
    "created_at": "2026-02-01T10:41:22.538Z"
  },
  {
    "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation",
    "authors": [
      "Srijan Dokania",
      "\n      \n      Dharini Raghavan"
    ],
    "abstract": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08271",
    "publication_date": "2025-12-09",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482540,
    "created_at": "2026-02-01T10:41:22.540Z"
  },
  {
    "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions",
    "authors": [
      "Eunice Yiu",
      "\n      \n      Kelsey Allen",
      "\n      \n      Shiry Ginosar",
      "\n      \n      Alison Gopnik"
    ],
    "abstract": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08230",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482542,
    "created_at": "2026-02-01T10:41:22.542Z"
  },
  {
    "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model",
    "authors": [
      "Wenjiang Xu",
      "\n      \n      Cindy Wang",
      "\n      \n      Rui Fang",
      "\n      \n      Mingkang Zhang",
      "\n      \n      Lusong Li",
      "\n      \n      Jing Xu",
      "\n      \n      Jiayuan Gu",
      "\n      \n      Zecui Zeng",
      "\n      \n      Rui Chen"
    ],
    "abstract": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08188",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942482544,
    "created_at": "2026-02-01T10:41:22.544Z"
  },
  {
    "title": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space",
    "authors": [
      "Tianxingjian Ding",
      "\n      \n      Yuanhao Zou",
      "\n      \n      Chen Chen",
      "\n      \n      Mubarak Shah",
      "\n      \n      Yu Tian"
    ],
    "abstract": "Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\\%, and significantly surpasses all other medical-specific large language models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.08029",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Planning",
      "Representation Learning",
      "Diffusion Models"
    ],
    "id": 1769942482546,
    "created_at": "2026-02-01T10:41:22.546Z"
  },
  {
    "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling",
    "authors": [
      "Shaoheng Fang",
      "\n      \n      Hanwen Jiang",
      "\n      \n      Yunpeng Bai",
      "\n      \n      Niloy J. Mitra",
      "\n      \n      Qixing Huang"
    ],
    "abstract": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.07821",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942482548,
    "created_at": "2026-02-01T10:41:22.548Z"
  },
  {
    "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery",
    "authors": [
      "Meng Cao",
      "\n      \n      Xingyu Li",
      "\n      \n      Xue Liu",
      "\n      \n      Ian Reid",
      "\n      \n      Xiaodan Liang"
    ],
    "abstract": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.07733",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482551,
    "created_at": "2026-02-01T10:41:22.551Z"
  },
  {
    "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models",
    "authors": [
      "Chenwei Shi",
      "\n      \n      Xueyu Luan"
    ],
    "abstract": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.07437",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482553,
    "created_at": "2026-02-01T10:41:22.553Z"
  },
  {
    "title": "Unified Camera Positional Encoding for Controlled Video Generation",
    "authors": [
      "Cheng Zhang",
      "\n      \n      Boying Li",
      "\n      \n      Meng Wei",
      "\n      \n      Yan-Pei Cao",
      "\n      \n      Camilo Cruz Gambardella",
      "\n      \n      Dinh Phung",
      "\n      \n      Jianfei Cai"
    ],
    "abstract": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.07237",
    "publication_date": "2025-12-08",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Transformers"
    ],
    "id": 1769942482555,
    "created_at": "2026-02-01T10:41:22.555Z"
  },
  {
    "title": "On Memory: A comparison of memory mechanisms in world models",
    "authors": [
      "Eli J. Laird",
      "\n      \n      Corey Clark"
    ],
    "abstract": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.06983",
    "publication_date": "2025-12-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942482557,
    "created_at": "2026-02-01T10:41:22.557Z"
  },
  {
    "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding",
    "authors": [
      "Yu Yu",
      "\n      \n      Qian Xie",
      "\n      \n      Nairen Cao",
      "\n      \n      Li Jin"
    ],
    "abstract": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.06982",
    "publication_date": "2025-12-11",
    "tags": [
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482560,
    "created_at": "2026-02-01T10:41:22.560Z"
  },
  {
    "title": "Spatial Retrieval Augmented Autonomous Driving",
    "authors": [
      "Xiaosong Jia",
      "\n      \n      Chenhe Zhang",
      "\n      \n      Yule Jiang",
      "\n      \n      Songbur Wong",
      "\n      \n      Zhiyuan Zhang",
      "\n      \n      Chen Chen",
      "\n      \n      Shaofeng Zhang",
      "\n      \n      Xuanhe Zhou",
      "\n      \n      Xue Yang",
      "\n      \n      Junchi Yan",
      "\n      \n      Yu-Gang Jiang"
    ],
    "abstract": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.06865",
    "publication_date": "2025-12-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942482562,
    "created_at": "2026-02-01T10:41:22.562Z"
  },
  {
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "authors": [
      "Ruicheng Zhang",
      "\n      \n      Mingyang Zhang",
      "\n      \n      Jun Zhou",
      "\n      \n      Zhangrui Guo",
      "\n      \n      Xiaofan Liu",
      "\n      \n      Zunnan Xu",
      "\n      \n      Zhizhou Zhong",
      "\n      \n      Puxin Yan",
      "\n      \n      Haocheng Luo",
      "\n      \n      Xiu Li"
    ],
    "abstract": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.06628",
    "publication_date": "2025-12-06",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942482565,
    "created_at": "2026-02-01T10:41:22.565Z"
  },
  {
    "title": "Deep Manifold Part 2: Neural Network Mathematics",
    "authors": [
      "Max Y. Ma",
      "\n      \n      Gen-Hua Shi"
    ],
    "abstract": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.06563",
    "publication_date": "2025-12-06",
    "tags": [],
    "id": 1769942482567,
    "created_at": "2026-02-01T10:41:22.567Z"
  },
  {
    "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models",
    "authors": [
      "Haowen Liu",
      "\n      \n      Shaoxiong Yao",
      "\n      \n      Haonan Chen",
      "\n      \n      Jiawei Gao",
      "\n      \n      Jiayuan Mao",
      "\n      \n      Jia-Bin Huang",
      "\n      \n      Yilun Du"
    ],
    "abstract": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05955",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942482570,
    "created_at": "2026-02-01T10:41:22.570Z"
  },
  {
    "title": "Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech",
    "authors": [
      "Xuanru Zhou",
      "\n      \n      Jiachen Lian",
      "\n      \n      Henry Hong",
      "\n      \n      Xinyi Yang",
      "\n      \n      Gopala Anumanchipalli"
    ],
    "abstract": "Current speech-language models (SLMs) typically use a cascade of speech encoder and large language model, treating speech understanding as a single black box. They analyze the content of speech well but reason weakly about other aspects, especially under sparse supervision. Thus, we argue for explicit reasoning over speech states and actions with modular and transparent decisions. Inspired by cognitive science we adopt a modular perspective and a world model view in which the system learns forward dynamics over latent states. We factorize speech understanding into four modules that communicate through a causal graph, establishing a cognitive state search space. Guided by posterior traces from this space, an instruction-tuned language model produces a concise causal analysis and a user-facing response, enabling counterfactual interventions and interpretability under partial supervision. We present the first graph based modular speech model for explicit reasoning and we will open source the model and data to promote the development of advanced speech understanding.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05933",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942482572,
    "created_at": "2026-02-01T10:41:22.572Z"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "authors": [
      "Zhiting Mei",
      "\n      \n      Tenny Yin",
      "\n      \n      Micah Baker",
      "\n      \n      Ola Shorinwa",
      "\n      \n      Anirudha Majumdar"
    ],
    "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05927",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942482574,
    "created_at": "2026-02-01T10:41:22.574Z"
  },
  {
    "title": "Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling",
    "authors": [
      "Saurav Jha",
      "\n      \n      M. Jehanzeb Mirza",
      "\n      \n      Wei Lin",
      "\n      \n      Shiqi Yang",
      "\n      \n      Sarath Chandar"
    ],
    "abstract": "Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05809",
    "publication_date": "2025-12-05",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482576,
    "created_at": "2026-02-01T10:41:22.576Z"
  },
  {
    "title": "FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability",
    "authors": [
      "Ziheng Guo",
      "\n      \n      Fang Wu",
      "\n      \n      Maoxiong Zhao",
      "\n      \n      Chaoqun Fang",
      "\n      \n      Yang Bu"
    ],
    "abstract": "We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05361",
    "publication_date": "2025-12-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482579,
    "created_at": "2026-02-01T10:41:22.579Z"
  },
  {
    "title": "The Blueprints of Intelligence: A Functional-Topological Foundation for Perception and Representation",
    "authors": [
      "Eduardo Di Santi"
    ],
    "abstract": "Real-world phenomena do not generate arbitrary variability: their signals concentrate on compact, low-variability subsets of functional space, enabling rapid generalization from few examples. A small child can recognize a dog after extremely limited exposure because the perceptual manifold of \"dog\" is compact, structured, and low-dimensional. We formalize this principle through a deterministic functional-topological framework in which the set of valid realizations produced by a physical process forms a compact subset of a Banach space, endowed with stable invariants, a finite Hausdorff radius, and an induced continuous perceptual functional.\n  This geometry provides explicit limits on knowledge, conditions for identifiability, and guarantees for generalization from sparse evidence -- properties fundamental to both natural and artificial intelligence. Across electromechanical, electrochemical, and physiological domains, we show that real-world processes consistently generate compact perceptual manifolds with the same geometric characteristics. Their boundaries can be discovered in a fully self-supervised manner as the empirical radius saturates with increasing sampling, even when the governing equations are unknown.\n  These results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction. It provides a geometric explanation for why biological learners and self-supervised AI systems can generalize from few observations, and establishes compact perceptual manifolds as a fundamental building block for future AI architectures. Finally, this work unifies biological perception and modern self-supervised models under a single geometric principle: both derive their generalization ability from the compactness and invariants of real-world perceptual manifolds.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.05089",
    "publication_date": "2026-01-11",
    "tags": [
      "Representation Learning"
    ],
    "id": 1769942482581,
    "created_at": "2026-02-01T10:41:22.581Z"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "authors": [
      "Pei Yang",
      "\n      \n      Hai Ci",
      "\n      \n      Yiren Song",
      "\n      \n      Mike Zheng Shou"
    ],
    "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04537",
    "publication_date": "2025-12-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482584,
    "created_at": "2026-02-01T10:41:22.584Z"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "authors": [
      "Liuzhou Zhang",
      "\n      \n      Jiarui Ye",
      "\n      \n      Yuanlei Wang",
      "\n      \n      Ming Zhong",
      "\n      \n      Mingju Cao",
      "\n      \n      Wanke Xia",
      "\n      \n      Bowen Zeng",
      "\n      \n      Zeyu Zhang",
      "\n      \n      Hao Tang"
    ],
    "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04515",
    "publication_date": "2025-12-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction"
    ],
    "id": 1769942482586,
    "created_at": "2026-02-01T10:41:22.586Z"
  },
  {
    "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models",
    "authors": [
      "Yu-Wei Zhan",
      "\n      \n      Xin Wang",
      "\n      \n      Pengzhe Mao",
      "\n      \n      Tongtong Feng",
      "\n      \n      Ren Wang",
      "\n      \n      Wenwu Zhu"
    ],
    "abstract": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04513",
    "publication_date": "2025-12-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Representation Learning"
    ],
    "id": 1769942482589,
    "created_at": "2026-02-01T10:41:22.589Z"
  },
  {
    "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving",
    "authors": [
      "Bin Sun",
      "\n      \n      Yaoguang Cao",
      "\n      \n      Yan Wang",
      "\n      \n      Rui Wang",
      "\n      \n      Jiachen Shang",
      "\n      \n      Xiejie Feng",
      "\n      \n      Jiayi Lu",
      "\n      \n      Jia Shi",
      "\n      \n      Shichun Yang",
      "\n      \n      Xiaoyu Yan",
      "\n      \n      Ziying Song"
    ],
    "abstract": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04441",
    "publication_date": "2025-12-07",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Planning"
    ],
    "id": 1769942482591,
    "created_at": "2026-02-01T10:41:22.591Z"
  },
  {
    "title": "Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism",
    "authors": [
      "Tianwei Ni",
      "\n      \n      Esther Derman",
      "\n      \n      Vineet Jain",
      "\n      \n      Vincent Taboga",
      "\n      \n      Siamak Ravanbakhsh",
      "\n      \n      Pierre-Luc Bacon"
    ],
    "abstract": "Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting rollout horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale this principle to realistic tasks and show that long-horizon planning is critical for reducing value overestimation once conservatism is removed. To make this feasible, we introduce key design choices for performing and learning from long-horizon rollouts while controlling compounding errors. These yield our algorithm, NEUBAY, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, NEUBAY generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with rollout horizons of several hundred steps, contrary to dominant practice. Finally, we characterize datasets by quality and coverage, showing when NEUBAY is preferable to conservative methods. Together, we argue NEUBAY lays the foundation for a new practical direction in offline and model-based RL.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04341",
    "publication_date": "2026-01-04",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning"
    ],
    "id": 1769942482593,
    "created_at": "2026-02-01T10:41:22.593Z"
  },
  {
    "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies",
    "authors": [
      "Feeza Khan Khanzada",
      "\n      \n      Jaerock Kwon"
    ],
    "abstract": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04279",
    "publication_date": "2025-12-27",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Video Prediction"
    ],
    "id": 1769942482597,
    "created_at": "2026-02-01T10:41:22.597Z"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "authors": [
      "Yicong Hong",
      "\n      \n      Yiqun Mei",
      "\n      \n      Chongjian Ge",
      "\n      \n      Yiran Xu",
      "\n      \n      Yang Zhou",
      "\n      \n      Sai Bi",
      "\n      \n      Yannick Hold-Geoffroy",
      "\n      \n      Mike Roberts",
      "\n      \n      Matthew Fisher",
      "\n      \n      Eli Shechtman",
      "\n      \n      Kalyan Sunkavalli",
      "\n      \n      Feng Liu",
      "\n      \n      Zhengqi Li",
      "\n      \n      Hao Tan"
    ],
    "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.04040",
    "publication_date": "2025-12-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1769942482599,
    "created_at": "2026-02-01T10:41:22.599Z"
  },
  {
    "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL",
    "authors": [
      "Yinzhou Tang",
      "\n      \n      Yu Shang",
      "\n      \n      Yinuo Chen",
      "\n      \n      Bingwen Wei",
      "\n      \n      Xin Zhang",
      "\n      \n      Shu'ang Yu",
      "\n      \n      Liangzhi Shi",
      "\n      \n      Chao Yu",
      "\n      \n      Chen Gao",
      "\n      \n      Wei Wu",
      "\n      \n      Yong Li"
    ],
    "abstract": "Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03556",
    "publication_date": "2025-12-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482602,
    "created_at": "2026-02-01T10:41:22.602Z"
  },
  {
    "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
    "authors": [
      "Yuhang Huang",
      "\n      \n      Shilong Zou",
      "\n      \n      Jiazhao Zhang",
      "\n      \n      Xinwang Liu",
      "\n      \n      Ruizhen Hu",
      "\n      \n      Kai Xu"
    ],
    "abstract": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03538",
    "publication_date": "2025-12-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482604,
    "created_at": "2026-02-01T10:41:22.604Z"
  },
  {
    "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles",
    "authors": [
      "Haicheng Liao",
      "\n      \n      Huanming Shen",
      "\n      \n      Bonan Wang",
      "\n      \n      Yongkang Li",
      "\n      \n      Yihong Tang",
      "\n      \n      Chengyue Wang",
      "\n      \n      Dingyi Zhuang",
      "\n      \n      Kehua Chen",
      "\n      \n      Hai Yang",
      "\n      \n      Chengzhong Xu",
      "\n      \n      Zhenning Li"
    ],
    "abstract": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03454",
    "publication_date": "2025-12-11",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482607,
    "created_at": "2026-02-01T10:41:22.607Z"
  },
  {
    "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations",
    "authors": [
      "Raul Steinmetz",
      "\n      \n      Fabio Demo Rosa",
      "\n      \n      Victor Augusto Kich",
      "\n      \n      Jair Augusto Bottega",
      "\n      \n      Ricardo Bedin Grando",
      "\n      \n      Daniel Fernando Tello Gamarra"
    ],
    "abstract": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03429",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics"
    ],
    "id": 1769942482610,
    "created_at": "2026-02-01T10:41:22.610Z"
  },
  {
    "title": "Better World Models Can Lead to Better Post-Training Performance",
    "authors": [
      "Prakhar Gupta",
      "\n      \n      Henry Conklin",
      "\n      \n      Sarah-Jane Leslie",
      "\n      \n      Andrew Lee"
    ],
    "abstract": "In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03400",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics",
      "Planning",
      "Transformers"
    ],
    "id": 1769942482612,
    "created_at": "2026-02-01T10:41:22.612Z"
  },
  {
    "title": "Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding",
    "authors": [
      "Duy-Tung Pham",
      "\n      \n      An The Nguyen",
      "\n      \n      Viet-Hoang Tran",
      "\n      \n      Nhan-Phu Chung",
      "\n      \n      Xin T. Tong",
      "\n      \n      Tan M. Nguyen",
      "\n      \n      Thieu N. Vo"
    ],
    "abstract": "This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03058",
    "publication_date": "2025-11-25",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942482615,
    "created_at": "2026-02-01T10:41:22.615Z"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "authors": [
      "Kairun Wen",
      "\n      \n      Yuzhi Huang",
      "\n      \n      Runyu Chen",
      "\n      \n      Hui Zheng",
      "\n      \n      Yunlong Lin",
      "\n      \n      Panwang Pan",
      "\n      \n      Chenxin Li",
      "\n      \n      Wenyan Cong",
      "\n      \n      Jian Zhang",
      "\n      \n      Junbin Lu",
      "\n      \n      Chenguo Lin",
      "\n      \n      Dilin Wang",
      "\n      \n      Zhicheng Yan",
      "\n      \n      Hongyu Xu",
      "\n      \n      Justin Theiss",
      "\n      \n      Yue Huang",
      "\n      \n      Xinghao Ding",
      "\n      \n      Rakesh Ranjan",
      "\n      \n      Zhiwen Fan"
    ],
    "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.03000",
    "publication_date": "2025-12-03",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482617,
    "created_at": "2026-02-01T10:41:22.617Z"
  },
  {
    "title": "U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences",
    "authors": [
      "Xiang Xu",
      "\n      \n      Ao Liang",
      "\n      \n      Youquan Liu",
      "\n      \n      Linfeng Li",
      "\n      \n      Lingdong Kong",
      "\n      \n      Ziwei Liu",
      "\n      \n      Qingshan Liu"
    ],
    "abstract": "Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a \"hard-to-easy\" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02982",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models"
    ],
    "id": 1769942482620,
    "created_at": "2026-02-01T10:41:22.620Z"
  },
  {
    "title": "IC-World: In-Context Generation for Shared World Modeling",
    "authors": [
      "Fan Wu",
      "\n      \n      Jiacheng Wei",
      "\n      \n      Ruibo Li",
      "\n      \n      Yi Xu",
      "\n      \n      Junyou Li",
      "\n      \n      Deheng Ye",
      "\n      \n      Guosheng Lin"
    ],
    "abstract": "Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02793",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482622,
    "created_at": "2026-02-01T10:41:22.622Z"
  },
  {
    "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling",
    "authors": [
      "Yuta Oshima",
      "\n      \n      Yusuke Iwasawa",
      "\n      \n      Masahiro Suzuki",
      "\n      \n      Yutaka Matsuo",
      "\n      \n      Hiroki Furuta"
    ],
    "abstract": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02473",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482625,
    "created_at": "2026-02-01T10:41:22.625Z"
  },
  {
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "authors": [
      "Jianzong Wu",
      "\n      \n      Hao Lian",
      "\n      \n      Dachao Hao",
      "\n      \n      Ye Tian",
      "\n      \n      Qingyu Shi",
      "\n      \n      Biaolong Chen",
      "\n      \n      Hao Jiang",
      "\n      \n      Yunhai Tong"
    ],
    "abstract": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02457",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Video Prediction",
      "Diffusion Models"
    ],
    "id": 1769942482627,
    "created_at": "2026-02-01T10:41:22.627Z"
  },
  {
    "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
    "authors": [
      "Shogo Ohmae",
      "\n      \n      Keiko Ohmae"
    ],
    "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02419",
    "publication_date": "2025-12-02",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Transformers"
    ],
    "id": 1769942482630,
    "created_at": "2026-02-01T10:41:22.630Z"
  },
  {
    "title": "Vehicle Dynamics Embedded World Models for Autonomous Driving",
    "authors": [
      "Huiqian Li",
      "\n      \n      Wei Pan",
      "\n      \n      Haodong Zhang",
      "\n      \n      Jin Huang",
      "\n      \n      Zhihua Zhong"
    ],
    "abstract": "World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02417",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "id": 1769942482633,
    "created_at": "2026-02-01T10:41:22.633Z"
  },
  {
    "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls",
    "authors": [
      "Shubhi Asthana",
      "\n      \n      Bing Zhang",
      "\n      \n      Chad DeLuca",
      "\n      \n      Ruchi Mahindru",
      "\n      \n      Hima Patel"
    ],
    "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.\n  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.\n  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02228",
    "publication_date": "2025-12-01",
    "tags": [],
    "id": 1769942482635,
    "created_at": "2026-02-01T10:41:22.635Z"
  },
  {
    "title": "From monoliths to modules: Decomposing transducers for efficient world modelling",
    "authors": [
      "Alexander Boyd",
      "\n      \n      Franz Nowak",
      "\n      \n      David Hyland",
      "\n      \n      Manuel Baltieri",
      "\n      \n      Fernando E. Rosas"
    ],
    "abstract": "World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02193",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482638,
    "created_at": "2026-02-01T10:41:22.638Z"
  },
  {
    "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
    "authors": [
      "Varun Varma Thozhiyoor",
      "\n      \n      Shivam Tripathi",
      "\n      \n      Venkatesh Babu Radhakrishnan",
      "\n      \n      Anand Bhattad"
    ],
    "abstract": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.02016",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning"
    ],
    "id": 1769942482640,
    "created_at": "2026-02-01T10:41:22.640Z"
  },
  {
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "authors": [
      "Haoyang He",
      "\n      \n      Jay Patrikar",
      "\n      \n      Dong-Ki Kim",
      "\n      \n      Max Smith",
      "\n      \n      Daniel McGann",
      "\n      \n      Ali-akbar Agha-mohammadi",
      "\n      \n      Shayegan Omidshafiei",
      "\n      \n      Sebastian Scherer"
    ],
    "abstract": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.01952",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Generative Models",
      "Robotics",
      "Planning",
      "Representation Learning"
    ],
    "id": 1769942482642,
    "created_at": "2026-02-01T10:41:22.642Z"
  },
  {
    "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
    "authors": [
      "Kentaro Fujii",
      "\n      \n      Shingo Murata"
    ],
    "abstract": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.\n        △ Less",
    "url": "https://arxiv.org/pdf/2512.01924",
    "publication_date": "2025-12-01",
    "tags": [
      "World Models",
      "Model-Based RL",
      "Reinforcement Learning",
      "Robotics"
    ],
    "id": 1769942482645,
    "created_at": "2026-02-01T10:41:22.645Z"
  }
]